{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e08b866-f6c3-4b4c-8185-0e847a2568e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/user/PycharmProjects/reversi-game/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c64fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19fdeba6-0cdc-4fa8-8ebb-b6d5634034d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "import stable_baselines3.common.callbacks as callbacks_module\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy as masked_evaluate_policy\n",
    "\n",
    "# Modify the namespace of EvalCallback directly\n",
    "callbacks_module.evaluate_policy = masked_evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy, MaskableMultiInputActorCriticPolicy\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "\n",
    "from shutil import copyfile # keep track of generations\n",
    "from collections import OrderedDict\n",
    "\n",
    "from gymnasium.spaces import Discrete, Box, Dict, MultiDiscrete\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import gymnasium.spaces as spaces\n",
    "from game_logic import Othello\n",
    "import numpy as np\n",
    "import os, math\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0045e1-7726-4c4e-bef6-1b2f4743547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23086ab8-3197-4112-9877-fcdb52f351ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEED = 19\n",
    "NUM_TIMESTEPS = int(30_000_000)\n",
    "EVAL_FREQ = int(10_000)\n",
    "EVAL_EPISODES = int(100)\n",
    "BEST_THRESHOLD = 0.2 # must achieve a mean score above this to replace prev best self\n",
    "\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "\n",
    "LOGDIR = \"models\"\n",
    "# LOGDIR = \"delete_me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "015cd09b-673e-4145-b7b9-2c22763c0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloEnv(gym.Env):   \n",
    "    def __init__(self):\n",
    "        self.game = Othello()\n",
    "        self.agent_turn = 1\n",
    "        shape = self.game.board.shape  \n",
    "        self.action_mapping = self.am(shape)\n",
    "        self.action_space = Discrete(shape[0] * shape[1])  # sample - [x, y]\n",
    "        self.observation_space = Dict({\n",
    "                                        'board' : Box(0, 2, shape=shape, dtype=int),\n",
    "                                        'chips' : MultiDiscrete([65, 65]),\n",
    "                                        'player': Discrete(2, start=1)\n",
    "                                      })        \n",
    "        self.other_agent = None\n",
    "        # self.reset_othello_gen = self.reset_othello()   \n",
    "        \n",
    "        self.episodes = 0    \n",
    "        # self.global_reward = 0\n",
    "    \n",
    "\n",
    "    def am(self, shape):\n",
    "        x, y = shape\n",
    "        return [(n//x, n%x) for n in range(x * y)]\n",
    "        \n",
    "\n",
    "    # def reset_othello(self):\n",
    "    #     '''resets game to starting position \n",
    "    #        and also changes starting player alternatively'''\n",
    "    #     infinite_player_turn = cycle([1]*10 + [2]*10)\n",
    "    #     while True:\n",
    "    #         game = Othello()\n",
    "    #         model_turn = next(infinite_player_turn)\n",
    "    #         yield game, model_turn\n",
    "    \n",
    "    def change_to_latest_agent(self, agent):\n",
    "        self.other_agent = agent\n",
    "\n",
    "    def get_obs(self):        \n",
    "        return OrderedDict({\n",
    "            'board' : self.game.board,\n",
    "            'chips' : np.array(self.game.chips),\n",
    "            'player': self.game.player_turn\n",
    "        })\n",
    "\n",
    "    def get_chips_diff(self): #  from agent perspective\n",
    "        idx = self.agent_turn - 1  # map [1, 2] to [0, 1]\n",
    "        diff = self.game.chips[idx] - self.game.chips[1 - idx]\n",
    "        return diff\n",
    "        \n",
    "    def check_game_ended(self):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        winner = self.game.get_winner()\n",
    "        \n",
    "        if winner is not None:\n",
    "            self.episodes += 1\n",
    "            if self.episodes % 10 == 0:\n",
    "                print(f'Ep done - {self.episodes}.')\n",
    "                # print(f'global_reward -- {self.global_reward}, -- won: {winner == self.agent_turn}')\n",
    "            \n",
    "            done = True\n",
    "            if winner == self.agent_turn:\n",
    "                # reward = max(abs(self.global_reward)*2, 1000)\n",
    "                reward = 1\n",
    "            elif winner == 3 - self.agent_turn: #  other agent turn/figure\n",
    "                # reward = min(-abs(self.global_reward)*2, -1000)\n",
    "                reward = -1\n",
    "        return reward, done\n",
    "    \n",
    "    def render(self):  # todo \n",
    "        pass\n",
    "\n",
    "    def close(self):  # todo\n",
    "        pass\n",
    "\n",
    "    def other_agent_play_move(self): \n",
    "        obs = self.get_obs()\n",
    "        obs = spaces.flatten(self.observation_space, obs)#  need to flatten observation         \n",
    "        action, _ = self.other_agent.predict(obs,\n",
    "                                             action_masks=self.action_masks(),\n",
    "                                             deterministic=False) \n",
    "        game_action = self.action_mapping[action]\n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "    def step(self, action):\n",
    "        # diff_chips_before = self.get_chips_diff()\n",
    "        \n",
    "        game_action = self.action_mapping[action]  \n",
    "        self.game.play_move(game_action)\n",
    "\n",
    "        # inner agent plays\n",
    "        while self.game.get_winner() is None and self.game.player_turn != self.agent_turn: #  if game hasnt ended do moves if opponent doesnt have one \n",
    "            self.other_agent_play_move()\n",
    "\n",
    "        # diff_chips_after = self.get_chips_diff()\n",
    "        \n",
    "        reward, done = self.check_game_ended()\n",
    "\n",
    "        # turn = self.game.turn\n",
    "        # if turn <= 58: #  not sure... feel like at the end theres no more moves to choose and high reward would be bad            \n",
    "        #     factor = (turn // 10) + 1\n",
    "        #     step_reward = factor * (diff_chips_after - diff_chips_before)\n",
    "        #     self.global_reward += step_reward\n",
    "        #     reward += factor * (diff_chips_after - diff_chips_before)\n",
    "\n",
    "        \n",
    "        info = {}\n",
    "        truncated = False\n",
    "\n",
    "                \n",
    "        # Return step information\n",
    "        return self.get_obs(), reward, done, truncated, info\n",
    "    \n",
    "    def reset(self, *args, **kwargs):\n",
    "        # self.global_reward = 0\n",
    "        self.game = Othello() # self.game, self.agent_turn = next(self.reset_othello_gen)\n",
    "        if self.agent_turn == 2:\n",
    "            self.other_agent_play_move()\n",
    "        return self.get_obs(), None\n",
    "\n",
    "    def action_masks(self):        \n",
    "        valid_moves = self.game.valid_moves()\n",
    "    \n",
    "        mask = np.zeros(self.game.board.shape, dtype=bool)\n",
    "        \n",
    "        # Set True for each index in the set\n",
    "        for index in valid_moves:\n",
    "            mask[index] = True\n",
    "        mask.flatten()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d9237-3a89-4fec-9fbb-9c55d0c5c71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2235363-8bce-49ee-a44a-e63a76b1d75c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfdecf41-ef1c-4f12-bbc7-02d06ce2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPlayCallback(EvalCallback):\n",
    "    # hacked it to only save new version offrom gymnasium.wrappers import FlattenObservation best model if beats prev self by BEST_THRESHOLD score\n",
    "    # after saving model, resets the best score to be BEST_THRESHOLD\n",
    "    def __init__(self, eval_env, *args, **kwargs):\n",
    "        super().__init__(eval_env, *args, **kwargs)\n",
    "        self.best_mean_reward = BEST_THRESHOLD\n",
    "        self.generation = 0        \n",
    "    def _on_step(self) -> bool:\n",
    "        # result = super()._on_step() #  eval needs to be masked, its less efficient \n",
    "        result = super()._on_step()\n",
    "        \n",
    "        if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "            self.generation += 1\n",
    "            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)            \n",
    "            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "            copyfile(source_file, backup_file)\n",
    "            self.best_mean_reward = BEST_THRESHOLD\n",
    "            agent = self.model.load(source_file)\n",
    "                \n",
    "            self.training_env.env_method(\"change_to_latest_agent\", agent)           \n",
    "            self.eval_env.env_method(\"change_to_latest_agent\", agent) #  .env_method(\"method_name\", args1, args2, kwargs1=kwargs1) \n",
    "        return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927a91a-306e-4da3-9056-9755dd527c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2207e630-3911-417d-be7f-2b47f69cde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OthelloEnv()\n",
    "env = Monitor(env=env)\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fd147c-341a-4e53-b705-bc9321831e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8655c3f-2e4f-4f02-8a50-a7b8aaf183fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[128, 64], vf=[64, 64]))\n",
    "# Create the agent\n",
    "#model = PPO(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "981a14a5-ef60-43eb-ad30-da83838d6299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = MaskablePPO(policy=MaskableActorCriticPolicy, \n",
    "                    env=env, \n",
    "                    verbose=1,\n",
    "                    policy_kwargs=policy_kwargs#,\n",
    "                    # learning_rate=1e-5, \n",
    "                    # n_steps=6144\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7cf6e676-7c6f-42e7-a0ed-fd7212f436c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/venv/rl-env/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "starting_model_filepath = LOGDIR + '/start_model'\n",
    "model.save(starting_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d39dc3f-0728-4ac0-bb4c-497201cc7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_model_copy = model.load(starting_model_filepath)\n",
    "env.unwrapped.change_to_latest_agent(start_model_copy)\n",
    "\n",
    "\n",
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "env_eval = FlattenObservation(env_eval)\n",
    "\n",
    "env_eval = DummyVecEnv(env_fns=[lambda: env_eval])\n",
    "env_eval.envs[0].unwrapped.change_to_latest_agent(start_model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec37f2d-4570-4339-83a2-cb07dbe276b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/venv/rl-env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 10.\n",
      "Ep done - 20.\n",
      "Ep done - 30.\n",
      "Ep done - 40.\n",
      "Ep done - 50.\n",
      "Ep done - 60.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.149    |\n",
      "| time/              |          |\n",
      "|    fps             | 282      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Ep done - 70.\n",
      "Ep done - 80.\n",
      "Ep done - 90.\n",
      "Ep done - 100.\n",
      "Ep done - 110.\n",
      "Ep done - 120.\n",
      "Ep done - 130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.14       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 234        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01656268 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.98      |\n",
      "|    explained_variance   | -0.552     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0264    |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0398    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Ep done - 140.\n",
      "Ep done - 150.\n",
      "Ep done - 160.\n",
      "Ep done - 170.\n",
      "Ep done - 180.\n",
      "Ep done - 190.\n",
      "Ep done - 200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015080763 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | -0.114      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0043      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0481     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Ep done - 210.\n",
      "Ep done - 220.\n",
      "Ep done - 230.\n",
      "Ep done - 240.\n",
      "Ep done - 250.\n",
      "Ep done - 260.\n",
      "Ep done - 270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014532169 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.98       |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0503     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Ep done - 280.\n",
      "Ep done - 290.\n",
      "Ep done - 300.\n",
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 10.\n",
      "Ep done - 20.\n",
      "Ep done - 30.\n",
      "Ep done - 40.\n",
      "Ep done - 50.\n",
      "Ep done - 60.\n",
      "Ep done - 70.\n",
      "Ep done - 80.\n",
      "Ep done - 90.\n",
      "Ep done - 100.\n",
      "Eval num_timesteps=10000, episode_reward=0.07 +/- 0.98\n",
      "Episode length: 30.22 +/- 0.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30.2         |\n",
      "|    mean_reward          | 0.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144576235 |\n",
      "|    clip_fraction        | 0.182        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.96        |\n",
      "|    explained_variance   | -0.0694      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0397      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.054       |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.09     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Ep done - 340.\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 30.2         |\n",
      "|    ep_rew_mean          | 0.13         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 169          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136336535 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.99        |\n",
      "|    explained_variance   | 0.00457      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0483      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0499      |\n",
      "|    value_loss           | 0.168        |\n",
      "------------------------------------------\n",
      "Ep done - 410.\n",
      "Ep done - 420.\n",
      "Ep done - 430.\n",
      "Ep done - 440.\n",
      "Ep done - 450.\n",
      "Ep done - 460.\n",
      "Ep done - 470.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.09       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 172        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 83         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01521297 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.93      |\n",
      "|    explained_variance   | -0.00401   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0235     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0546    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "Ep done - 480.\n",
      "Ep done - 490.\n",
      "Ep done - 500.\n",
      "Ep done - 510.\n",
      "Ep done - 520.\n",
      "Ep done - 530.\n",
      "Ep done - 540.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.11     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 173      |\n",
      "|    iterations           | 8        |\n",
      "|    time_elapsed         | 94       |\n",
      "|    total_timesteps      | 16384    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.018646 |\n",
      "|    clip_fraction        | 0.23     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.92    |\n",
      "|    explained_variance   | -0.00584 |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.00454 |\n",
      "|    n_updates            | 70       |\n",
      "|    policy_gradient_loss | -0.0596  |\n",
      "|    value_loss           | 0.191    |\n",
      "--------------------------------------\n",
      "Ep done - 550.\n",
      "Ep done - 560.\n",
      "Ep done - 570.\n",
      "Ep done - 580.\n",
      "Ep done - 590.\n",
      "Ep done - 600.\n",
      "Ep done - 610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 172         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016727801 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.00488     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 620.\n",
      "Ep done - 630.\n",
      "Ep done - 640.\n",
      "Ep done - 650.\n",
      "Ep done - 660.\n",
      "Ep done - 110.\n",
      "Ep done - 120.\n",
      "Ep done - 130.\n",
      "Ep done - 140.\n",
      "Ep done - 150.\n",
      "Ep done - 160.\n",
      "Ep done - 170.\n",
      "Ep done - 180.\n",
      "Ep done - 190.\n",
      "Ep done - 200.\n",
      "Eval num_timesteps=20000, episode_reward=0.13 +/- 0.98\n",
      "Episode length: 30.20 +/- 0.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017519198 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00111    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Ep done - 670.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Ep done - 680.\n",
      "Ep done - 690.\n",
      "Ep done - 700.\n",
      "Ep done - 710.\n",
      "Ep done - 720.\n",
      "Ep done - 730.\n",
      "Ep done - 740.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018024301 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.026       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0234     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0568     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Ep done - 750.\n",
      "Ep done - 760.\n",
      "Ep done - 770.\n",
      "Ep done - 780.\n",
      "Ep done - 790.\n",
      "Ep done - 800.\n",
      "Ep done - 810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018673176 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Ep done - 820.\n",
      "Ep done - 830.\n",
      "Ep done - 840.\n",
      "Ep done - 850.\n",
      "Ep done - 860.\n",
      "Ep done - 870.\n",
      "Ep done - 880.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018672658 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | -0.0246     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0549     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Ep done - 890.\n",
      "Ep done - 900.\n",
      "Ep done - 910.\n",
      "Ep done - 920.\n",
      "Ep done - 930.\n",
      "Ep done - 940.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 179         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020697745 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0415     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 950.\n",
      "Ep done - 960.\n",
      "Ep done - 970.\n",
      "Ep done - 980.\n",
      "Ep done - 990.\n",
      "Ep done - 210.\n",
      "Ep done - 220.\n",
      "Ep done - 230.\n",
      "Ep done - 240.\n",
      "Ep done - 250.\n",
      "Ep done - 260.\n",
      "Ep done - 270.\n",
      "Ep done - 280.\n",
      "Ep done - 290.\n",
      "Ep done - 300.\n",
      "Eval num_timesteps=30000, episode_reward=0.38 +/- 0.90\n",
      "Episode length: 30.21 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019366028 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.81       |\n",
      "|    explained_variance   | -0.0501     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/venv/rl-env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.change_to_latest_agent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.change_to_latest_agent` for environment variables or `env.get_wrapper_attr('change_to_latest_agent')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 1000.\n",
      "Ep done - 1010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.11     |\n",
      "| time/              |          |\n",
      "|    fps             | 140      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Ep done - 1020.\n",
      "Ep done - 1030.\n",
      "Ep done - 1040.\n",
      "Ep done - 1050.\n",
      "Ep done - 1060.\n",
      "Ep done - 1070.\n",
      "Ep done - 1080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | -0.04       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 239         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018902505 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0574     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Ep done - 1090.\n",
      "Ep done - 1100.\n",
      "Ep done - 1110.\n",
      "Ep done - 1120.\n",
      "Ep done - 1130.\n",
      "Ep done - 1140.\n",
      "Ep done - 1150.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | -0.13     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 138       |\n",
      "|    iterations           | 17        |\n",
      "|    time_elapsed         | 251       |\n",
      "|    total_timesteps      | 34816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0192332 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.77     |\n",
      "|    explained_variance   | 0.0733    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0331   |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "Ep done - 1160.\n",
      "Ep done - 1170.\n",
      "Ep done - 1180.\n",
      "Ep done - 1190.\n",
      "Ep done - 1200.\n",
      "Ep done - 1210.\n",
      "Ep done - 1220.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.07      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 140       |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 262       |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0175582 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.78     |\n",
      "|    explained_variance   | 0.104     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00998  |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "Ep done - 1230.\n",
      "Ep done - 1240.\n",
      "Ep done - 1250.\n",
      "Ep done - 1260.\n",
      "Ep done - 1270.\n",
      "Ep done - 1280.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 138       |\n",
      "|    iterations           | 19        |\n",
      "|    time_elapsed         | 281       |\n",
      "|    total_timesteps      | 38912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0193955 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.74     |\n",
      "|    explained_variance   | 0.23      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0308   |\n",
      "|    n_updates            | 180       |\n",
      "|    policy_gradient_loss | -0.0589   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Ep done - 1290.\n",
      "Ep done - 1300.\n",
      "Ep done - 1310.\n",
      "Ep done - 1320.\n",
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 340.\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n",
      "Eval num_timesteps=40000, episode_reward=-0.05 +/- 0.98\n",
      "Episode length: 30.19 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | -0.05       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019718565 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.012       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Ep done - 1330.\n",
      "Ep done - 1340.\n",
      "Ep done - 1350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Ep done - 1360.\n",
      "Ep done - 1370.\n",
      "Ep done - 1380.\n",
      "Ep done - 1390.\n",
      "Ep done - 1400.\n",
      "Ep done - 1410.\n",
      "Ep done - 1420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.15        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 319         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020912474 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Ep done - 1430.\n",
      "Ep done - 1440.\n",
      "Ep done - 1450.\n",
      "Ep done - 1460.\n",
      "Ep done - 1470.\n",
      "Ep done - 1480.\n",
      "Ep done - 1490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.15        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021074891 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0585     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Ep done - 1500.\n",
      "Ep done - 1510.\n",
      "Ep done - 1520.\n",
      "Ep done - 1530.\n",
      "Ep done - 1540.\n",
      "Ep done - 1550.\n",
      "Ep done - 1560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022788329 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | -0.0413     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0555     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 1570.\n",
      "Ep done - 1580.\n",
      "Ep done - 1590.\n",
      "Ep done - 1600.\n",
      "Ep done - 1610.\n",
      "Ep done - 1620.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.1        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 135        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 362        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01992764 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 0.167      |\n",
      "----------------------------------------\n",
      "Ep done - 1630.\n",
      "Ep done - 1640.\n",
      "Ep done - 1650.\n",
      "Ep done - 410.\n",
      "Ep done - 420.\n",
      "Ep done - 430.\n",
      "Ep done - 440.\n",
      "Ep done - 450.\n",
      "Ep done - 460.\n",
      "Ep done - 470.\n",
      "Ep done - 480.\n",
      "Ep done - 490.\n",
      "Ep done - 500.\n",
      "Eval num_timesteps=50000, episode_reward=-0.02 +/- 1.00\n",
      "Episode length: 30.12 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022705177 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 0.0586      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0579     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Ep done - 1660.\n",
      "Ep done - 1670.\n",
      "Ep done - 1680.\n",
      "Ep done - 1690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Ep done - 1700.\n",
      "Ep done - 1710.\n",
      "Ep done - 1720.\n",
      "Ep done - 1730.\n",
      "Ep done - 1740.\n",
      "Ep done - 1750.\n",
      "Ep done - 1760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 395         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021979727 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00629     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0611     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 1770.\n",
      "Ep done - 1780.\n",
      "Ep done - 1790.\n",
      "Ep done - 1800.\n",
      "Ep done - 1810.\n",
      "Ep done - 1820.\n",
      "Ep done - 1830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021254832 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | -0.129      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0384     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0548     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 1840.\n",
      "Ep done - 1850.\n",
      "Ep done - 1860.\n",
      "Ep done - 1870.\n",
      "Ep done - 1880.\n",
      "Ep done - 1890.\n",
      "Ep done - 1900.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.24      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 137       |\n",
      "|    iterations           | 28        |\n",
      "|    time_elapsed         | 416       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0240261 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.59     |\n",
      "|    explained_variance   | 0.236     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00676  |\n",
      "|    n_updates            | 270       |\n",
      "|    policy_gradient_loss | -0.0595   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "Ep done - 1910.\n",
      "Ep done - 1920.\n",
      "Ep done - 1930.\n",
      "Ep done - 1940.\n",
      "Ep done - 1950.\n",
      "Ep done - 1960.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.21       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 426        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02391581 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.56      |\n",
      "|    explained_variance   | 0.0231     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00317   |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0582    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Ep done - 1970.\n",
      "Ep done - 1980.\n",
      "Ep done - 510.\n",
      "Ep done - 520.\n",
      "Ep done - 530.\n",
      "Ep done - 540.\n",
      "Ep done - 550.\n",
      "Ep done - 560.\n",
      "Ep done - 570.\n",
      "Ep done - 580.\n",
      "Ep done - 590.\n",
      "Ep done - 600.\n",
      "Eval num_timesteps=60000, episode_reward=0.30 +/- 0.93\n",
      "Episode length: 30.02 +/- 2.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021849103 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.3\n",
      "SELFPLAY: new best model, bumping up generation to 2\n",
      "Ep done - 1990.\n",
      "Ep done - 2000.\n",
      "Ep done - 2010.\n",
      "Ep done - 2020.\n",
      "Ep done - 2030.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 137      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Ep done - 2040.\n",
      "Ep done - 2050.\n",
      "Ep done - 2060.\n",
      "Ep done - 2070.\n",
      "Ep done - 2080.\n",
      "Ep done - 2090.\n",
      "Ep done - 2100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 455         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023931943 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0414     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0588     |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 2110.\n",
      "Ep done - 2120.\n",
      "Ep done - 2130.\n",
      "Ep done - 2140.\n",
      "Ep done - 2150.\n",
      "Ep done - 2160.\n",
      "Ep done - 2170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023189863 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0866      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0576     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "Ep done - 2180.\n",
      "Ep done - 2190.\n",
      "Ep done - 2200.\n",
      "Ep done - 2210.\n",
      "Ep done - 2220.\n",
      "Ep done - 2230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 474         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022350151 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.0903      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000503    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.058      |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 2240.\n",
      "Ep done - 2250.\n",
      "Ep done - 2260.\n",
      "Ep done - 2270.\n",
      "Ep done - 2280.\n",
      "Ep done - 2290.\n",
      "Ep done - 2300.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025268124 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0605     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 2310.\n",
      "Ep done - 2320.\n",
      "Ep done - 610.\n",
      "Ep done - 620.\n",
      "Ep done - 630.\n",
      "Ep done - 640.\n",
      "Ep done - 650.\n",
      "Ep done - 660.\n",
      "Ep done - 670.\n",
      "Ep done - 680.\n",
      "Ep done - 690.\n",
      "Ep done - 700.\n",
      "Eval num_timesteps=70000, episode_reward=0.21 +/- 0.96\n",
      "Episode length: 30.27 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.21       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02205095 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.179      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0587    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.21\n",
      "SELFPLAY: new best model, bumping up generation to 3\n",
      "Ep done - 2330.\n",
      "Ep done - 2340.\n",
      "Ep done - 2350.\n",
      "Ep done - 2360.\n",
      "Ep done - 2370.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.27     |\n",
      "| time/              |          |\n",
      "|    fps             | 142      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Ep done - 2380.\n",
      "Ep done - 2390.\n",
      "Ep done - 2400.\n",
      "Ep done - 2410.\n",
      "Ep done - 2420.\n",
      "Ep done - 2430.\n",
      "Ep done - 2440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 513         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021787142 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0581     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 2450.\n",
      "Ep done - 2460.\n",
      "Ep done - 2470.\n",
      "Ep done - 2480.\n",
      "Ep done - 2490.\n",
      "Ep done - 2500.\n",
      "Ep done - 2510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 523         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023463655 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Ep done - 2520.\n",
      "Ep done - 2530.\n",
      "Ep done - 2540.\n",
      "Ep done - 2550.\n",
      "Ep done - 2560.\n",
      "Ep done - 2570.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.18       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 145        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 533        |\n",
      "|    total_timesteps      | 77824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02383113 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | -0.109     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 370        |\n",
      "|    policy_gradient_loss | -0.055     |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "Ep done - 2580.\n",
      "Ep done - 2590.\n",
      "Ep done - 2600.\n",
      "Ep done - 2610.\n",
      "Ep done - 2620.\n",
      "Ep done - 2630.\n",
      "Ep done - 2640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 543         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022553885 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0571     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 2650.\n",
      "Ep done - 710.\n",
      "Ep done - 720.\n",
      "Ep done - 730.\n",
      "Ep done - 740.\n",
      "Ep done - 750.\n",
      "Ep done - 760.\n",
      "Ep done - 770.\n",
      "Ep done - 780.\n",
      "Ep done - 790.\n",
      "Ep done - 800.\n",
      "Eval num_timesteps=80000, episode_reward=0.38 +/- 0.91\n",
      "Episode length: 30.30 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025234565 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 4\n",
      "Ep done - 2660.\n",
      "Ep done - 2670.\n",
      "Ep done - 2680.\n",
      "Ep done - 2690.\n",
      "Ep done - 2700.\n",
      "Ep done - 2710.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 563      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Ep done - 2720.\n",
      "Ep done - 2730.\n",
      "Ep done - 2740.\n",
      "Ep done - 2750.\n",
      "Ep done - 2760.\n",
      "Ep done - 2770.\n",
      "Ep done - 2780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 572         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023482952 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0526     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Ep done - 2790.\n",
      "Ep done - 2800.\n",
      "Ep done - 2810.\n",
      "Ep done - 2820.\n",
      "Ep done - 2830.\n",
      "Ep done - 2840.\n",
      "Ep done - 2850.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 582         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024782239 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.0646      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00559     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0558     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Ep done - 2860.\n",
      "Ep done - 2870.\n",
      "Ep done - 2880.\n",
      "Ep done - 2890.\n",
      "Ep done - 2900.\n",
      "Ep done - 2910.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023165531 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0438     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0546     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Ep done - 2920.\n",
      "Ep done - 2930.\n",
      "Ep done - 2940.\n",
      "Ep done - 2950.\n",
      "Ep done - 2960.\n",
      "Ep done - 2970.\n",
      "Ep done - 2980.\n",
      "Ep done - 810.\n",
      "Ep done - 820.\n",
      "Ep done - 830.\n",
      "Ep done - 840.\n",
      "Ep done - 850.\n",
      "Ep done - 860.\n",
      "Ep done - 870.\n",
      "Ep done - 880.\n",
      "Ep done - 890.\n",
      "Ep done - 900.\n",
      "Eval num_timesteps=90000, episode_reward=0.16 +/- 0.96\n",
      "Episode length: 30.19 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024842618 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.0554      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0573     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 147      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 611      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Ep done - 2990.\n",
      "Ep done - 3000.\n",
      "Ep done - 3010.\n",
      "Ep done - 3020.\n",
      "Ep done - 3030.\n",
      "Ep done - 3040.\n",
      "Ep done - 3050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.27       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 621        |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02530316 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.33      |\n",
      "|    explained_variance   | 0.179      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Ep done - 3060.\n",
      "Ep done - 3070.\n",
      "Ep done - 3080.\n",
      "Ep done - 3090.\n",
      "Ep done - 3100.\n",
      "Ep done - 3110.\n",
      "Ep done - 3120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 630         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025297325 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0516     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Ep done - 3130.\n",
      "Ep done - 3140.\n",
      "Ep done - 3150.\n",
      "Ep done - 3160.\n",
      "Ep done - 3170.\n",
      "Ep done - 3180.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 640         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023971708 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0404     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0523     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 3190.\n",
      "Ep done - 3200.\n",
      "Ep done - 3210.\n",
      "Ep done - 3220.\n",
      "Ep done - 3230.\n",
      "Ep done - 3240.\n",
      "Ep done - 3250.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.19      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 151       |\n",
      "|    iterations           | 48        |\n",
      "|    time_elapsed         | 649       |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0247566 |\n",
      "|    clip_fraction        | 0.246     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.24     |\n",
      "|    explained_variance   | 0.134     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.028    |\n",
      "|    n_updates            | 470       |\n",
      "|    policy_gradient_loss | -0.0542   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "Ep done - 3260.\n",
      "Ep done - 3270.\n",
      "Ep done - 3280.\n",
      "Ep done - 3290.\n",
      "Ep done - 3300.\n",
      "Ep done - 3310.\n",
      "Ep done - 910.\n",
      "Ep done - 920.\n",
      "Ep done - 930.\n",
      "Ep done - 940.\n",
      "Ep done - 950.\n",
      "Ep done - 960.\n",
      "Ep done - 970.\n",
      "Ep done - 980.\n",
      "Ep done - 990.\n",
      "Ep done - 1000.\n",
      "Eval num_timesteps=100000, episode_reward=0.15 +/- 0.97\n",
      "Episode length: 30.24 +/- 0.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.15       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02350666 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | -0.0694    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.05      |\n",
      "|    value_loss           | 0.203      |\n",
      "----------------------------------------\n",
      "Ep done - 3320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.4      |\n",
      "| time/              |          |\n",
      "|    fps             | 150      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 668      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Ep done - 3330.\n",
      "Ep done - 3340.\n",
      "Ep done - 3350.\n",
      "Ep done - 3360.\n",
      "Ep done - 3370.\n",
      "Ep done - 3380.\n",
      "Ep done - 3390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 679         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027254054 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0334     |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Ep done - 3400.\n",
      "Ep done - 3410.\n",
      "Ep done - 3420.\n",
      "Ep done - 3430.\n",
      "Ep done - 3440.\n",
      "Ep done - 3450.\n",
      "Ep done - 3460.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.17        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 696         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027207982 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0537     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 3470.\n",
      "Ep done - 3480.\n",
      "Ep done - 3490.\n",
      "Ep done - 3500.\n",
      "Ep done - 3510.\n",
      "Ep done - 3520.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022812834 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Ep done - 3530.\n",
      "Ep done - 3540.\n",
      "Ep done - 3550.\n",
      "Ep done - 3560.\n",
      "Ep done - 3570.\n",
      "Ep done - 3580.\n",
      "Ep done - 3590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025719631 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0517     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 3600.\n",
      "Ep done - 3610.\n",
      "Ep done - 3620.\n",
      "Ep done - 3630.\n",
      "Ep done - 3640.\n",
      "Ep done - 1010.\n",
      "Ep done - 1020.\n",
      "Ep done - 1030.\n",
      "Ep done - 1040.\n",
      "Ep done - 1050.\n",
      "Ep done - 1060.\n",
      "Ep done - 1070.\n",
      "Ep done - 1080.\n",
      "Ep done - 1090.\n",
      "Ep done - 1100.\n",
      "Eval num_timesteps=110000, episode_reward=0.17 +/- 0.98\n",
      "Episode length: 30.18 +/- 0.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.17        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025461935 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0659      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0519     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "Ep done - 3650.\n",
      "Ep done - 3660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 741      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Ep done - 3670.\n",
      "Ep done - 3680.\n",
      "Ep done - 3690.\n",
      "Ep done - 3700.\n",
      "Ep done - 3710.\n",
      "Ep done - 3720.\n",
      "Ep done - 3730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026095148 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 3740.\n",
      "Ep done - 3750.\n",
      "Ep done - 3760.\n",
      "Ep done - 3770.\n",
      "Ep done - 3780.\n",
      "Ep done - 3790.\n",
      "Ep done - 3800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 778         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028326442 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0543     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Ep done - 3810.\n",
      "Ep done - 3820.\n",
      "Ep done - 3830.\n",
      "Ep done - 3840.\n",
      "Ep done - 3850.\n",
      "Ep done - 3860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 793         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026866183 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00747    |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 3870.\n",
      "Ep done - 3880.\n",
      "Ep done - 3890.\n",
      "Ep done - 3900.\n",
      "Ep done - 3910.\n",
      "Ep done - 3920.\n",
      "Ep done - 3930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 805         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027337667 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0484     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Ep done - 3940.\n",
      "Ep done - 3950.\n",
      "Ep done - 3960.\n",
      "Ep done - 3970.\n",
      "Ep done - 1110.\n",
      "Ep done - 1120.\n",
      "Ep done - 1130.\n",
      "Ep done - 1140.\n",
      "Ep done - 1150.\n",
      "Ep done - 1160.\n",
      "Ep done - 1170.\n",
      "Ep done - 1180.\n",
      "Ep done - 1190.\n",
      "Ep done - 1200.\n",
      "Eval num_timesteps=120000, episode_reward=0.35 +/- 0.93\n",
      "Episode length: 30.17 +/- 0.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026110664 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0786      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.35\n",
      "SELFPLAY: new best model, bumping up generation to 5\n",
      "Ep done - 3980.\n",
      "Ep done - 3990.\n",
      "Ep done - 4000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 146      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 825      |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Ep done - 4010.\n",
      "Ep done - 4020.\n",
      "Ep done - 4030.\n",
      "Ep done - 4040.\n",
      "Ep done - 4050.\n",
      "Ep done - 4060.\n",
      "Ep done - 4070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.15        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 839         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027335918 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00571     |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Ep done - 4080.\n",
      "Ep done - 4090.\n",
      "Ep done - 4100.\n",
      "Ep done - 4110.\n",
      "Ep done - 4120.\n",
      "Ep done - 4130.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 856         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026193459 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0512     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Ep done - 4140.\n",
      "Ep done - 4150.\n",
      "Ep done - 4160.\n",
      "Ep done - 4170.\n",
      "Ep done - 4180.\n",
      "Ep done - 4190.\n",
      "Ep done - 4200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 872         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028255535 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00362     |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0513     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Ep done - 4210.\n",
      "Ep done - 4220.\n",
      "Ep done - 4230.\n",
      "Ep done - 4240.\n",
      "Ep done - 4250.\n",
      "Ep done - 4260.\n",
      "Ep done - 4270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 883         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028724827 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 4280.\n",
      "Ep done - 4290.\n",
      "Ep done - 4300.\n",
      "Ep done - 1210.\n",
      "Ep done - 1220.\n",
      "Ep done - 1230.\n",
      "Ep done - 1240.\n",
      "Ep done - 1250.\n",
      "Ep done - 1260.\n",
      "Ep done - 1270.\n",
      "Ep done - 1280.\n",
      "Ep done - 1290.\n",
      "Ep done - 1300.\n",
      "Eval num_timesteps=130000, episode_reward=0.10 +/- 0.98\n",
      "Episode length: 30.16 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026061684 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00976     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0524     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Ep done - 4310.\n",
      "Ep done - 4320.\n",
      "Ep done - 4330.\n",
      "Ep done - 4340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.13     |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 903      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Ep done - 4350.\n",
      "Ep done - 4360.\n",
      "Ep done - 4370.\n",
      "Ep done - 4380.\n",
      "Ep done - 4390.\n",
      "Ep done - 4400.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 912         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024388192 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Ep done - 4410.\n",
      "Ep done - 4420.\n",
      "Ep done - 4430.\n",
      "Ep done - 4440.\n",
      "Ep done - 4450.\n",
      "Ep done - 4460.\n",
      "Ep done - 4470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 924         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026769329 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.0476      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0398     |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 4480.\n",
      "Ep done - 4490.\n",
      "Ep done - 4500.\n",
      "Ep done - 4510.\n",
      "Ep done - 4520.\n",
      "Ep done - 4530.\n",
      "Ep done - 4540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.12        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 934         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025553595 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0482     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n",
      "Ep done - 4550.\n",
      "Ep done - 4560.\n",
      "Ep done - 4570.\n",
      "Ep done - 4580.\n",
      "Ep done - 4590.\n",
      "Ep done - 4600.\n",
      "Ep done - 4610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.17        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 945         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028435208 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000439    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0499     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Ep done - 4620.\n",
      "Ep done - 4630.\n",
      "Ep done - 1310.\n",
      "Ep done - 1320.\n",
      "Ep done - 1330.\n",
      "Ep done - 1340.\n",
      "Ep done - 1350.\n",
      "Ep done - 1360.\n",
      "Ep done - 1370.\n",
      "Ep done - 1380.\n",
      "Ep done - 1390.\n",
      "Ep done - 1400.\n",
      "Eval num_timesteps=140000, episode_reward=0.48 +/- 0.87\n",
      "Episode length: 30.30 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027486324 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.962      |\n",
      "|    explained_variance   | 0.0963      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0121      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 6\n",
      "Ep done - 4640.\n",
      "Ep done - 4650.\n",
      "Ep done - 4660.\n",
      "Ep done - 4670.\n",
      "Ep done - 4680.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.1      |\n",
      "| time/              |          |\n",
      "|    fps             | 146      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 966      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Ep done - 4690.\n",
      "Ep done - 4700.\n",
      "Ep done - 4710.\n",
      "Ep done - 4720.\n",
      "Ep done - 4730.\n",
      "Ep done - 4740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | -0.05      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 146        |\n",
      "|    iterations           | 70         |\n",
      "|    time_elapsed         | 975        |\n",
      "|    total_timesteps      | 143360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02992979 |\n",
      "|    clip_fraction        | 0.236      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.931     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 690        |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Ep done - 4750.\n",
      "Ep done - 4760.\n",
      "Ep done - 4770.\n",
      "Ep done - 4780.\n",
      "Ep done - 4790.\n",
      "Ep done - 4800.\n",
      "Ep done - 4810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 988         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026849441 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.95       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00208     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 4820.\n",
      "Ep done - 4830.\n",
      "Ep done - 4840.\n",
      "Ep done - 4850.\n",
      "Ep done - 4860.\n",
      "Ep done - 4870.\n",
      "Ep done - 4880.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | -0.09       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1011        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030526727 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.914      |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Ep done - 4890.\n",
      "Ep done - 4900.\n",
      "Ep done - 4910.\n",
      "Ep done - 4920.\n",
      "Ep done - 4930.\n",
      "Ep done - 4940.\n",
      "Ep done - 4950.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 1023        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027393341 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.927      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "Ep done - 4960.\n",
      "Ep done - 1410.\n",
      "Ep done - 1420.\n",
      "Ep done - 1430.\n",
      "Ep done - 1440.\n",
      "Ep done - 1450.\n",
      "Ep done - 1460.\n",
      "Ep done - 1470.\n",
      "Ep done - 1480.\n",
      "Ep done - 1490.\n",
      "Ep done - 1500.\n",
      "Eval num_timesteps=150000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 30.17 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026195718 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.033       |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0498     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Ep done - 4970.\n",
      "Ep done - 4980.\n",
      "Ep done - 4990.\n",
      "Ep done - 5000.\n",
      "Ep done - 5010.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.01     |\n",
      "| time/              |          |\n",
      "|    fps             | 142      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 1065     |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Ep done - 5020.\n",
      "Ep done - 5030.\n",
      "Ep done - 5040.\n",
      "Ep done - 5050.\n",
      "Ep done - 5060.\n",
      "Ep done - 5070.\n",
      "Ep done - 5080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | -0.12       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1090        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026242886 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.913      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Ep done - 5090.\n",
      "Ep done - 5100.\n",
      "Ep done - 5110.\n",
      "Ep done - 5120.\n",
      "Ep done - 5130.\n",
      "Ep done - 5140.\n",
      "Ep done - 5150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1100        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029209288 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.865      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0156      |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Ep done - 5160.\n",
      "Ep done - 5170.\n",
      "Ep done - 5180.\n",
      "Ep done - 5190.\n",
      "Ep done - 5200.\n",
      "Ep done - 5210.\n",
      "Ep done - 5220.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1111        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028488964 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.847      |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00106     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Ep done - 5230.\n",
      "Ep done - 5240.\n",
      "Ep done - 5250.\n",
      "Ep done - 5260.\n",
      "Ep done - 5270.\n",
      "Ep done - 5280.\n",
      "Ep done - 5290.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.22       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 142        |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 1122       |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02859402 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.817     |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.04       |\n",
      "|    n_updates            | 770        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "Ep done - 1510.\n",
      "Ep done - 1520.\n",
      "Ep done - 1530.\n",
      "Ep done - 1540.\n",
      "Ep done - 1550.\n",
      "Ep done - 1560.\n",
      "Ep done - 1570.\n",
      "Ep done - 1580.\n",
      "Ep done - 1590.\n",
      "Ep done - 1600.\n",
      "Eval num_timesteps=160000, episode_reward=0.29 +/- 0.93\n",
      "Episode length: 30.25 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028160077 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.844      |\n",
      "|    explained_variance   | 0.0689      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.29\n",
      "SELFPLAY: new best model, bumping up generation to 7\n",
      "Ep done - 5300.\n",
      "Ep done - 5310.\n",
      "Ep done - 5320.\n",
      "Ep done - 5330.\n",
      "Ep done - 5340.\n",
      "Ep done - 5350.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.04    |\n",
      "| time/              |          |\n",
      "|    fps             | 141      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 1144     |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Ep done - 5360.\n",
      "Ep done - 5370.\n",
      "Ep done - 5380.\n",
      "Ep done - 5390.\n",
      "Ep done - 5400.\n",
      "Ep done - 5410.\n",
      "Ep done - 5420.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | -0.03      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 80         |\n",
      "|    time_elapsed         | 1156       |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02411808 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.816     |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 790        |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "Ep done - 5430.\n",
      "Ep done - 5440.\n",
      "Ep done - 5450.\n",
      "Ep done - 5460.\n",
      "Ep done - 5470.\n",
      "Ep done - 5480.\n",
      "Ep done - 5490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | -0.01       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 1171        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030026581 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Ep done - 5500.\n",
      "Ep done - 5510.\n",
      "Ep done - 5520.\n",
      "Ep done - 5530.\n",
      "Ep done - 5540.\n",
      "Ep done - 5550.\n",
      "Ep done - 5560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 1187        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028133191 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.801      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Ep done - 5570.\n",
      "Ep done - 5580.\n",
      "Ep done - 5590.\n",
      "Ep done - 5600.\n",
      "Ep done - 5610.\n",
      "Ep done - 5620.\n",
      "Ep done - 5630.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.12       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 140        |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 1207       |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03133865 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.814     |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0072    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "Ep done - 1610.\n",
      "Ep done - 1620.\n",
      "Ep done - 1630.\n",
      "Ep done - 1640.\n",
      "Ep done - 1650.\n",
      "Ep done - 1660.\n",
      "Ep done - 1670.\n",
      "Ep done - 1680.\n",
      "Ep done - 1690.\n",
      "Ep done - 1700.\n",
      "Eval num_timesteps=170000, episode_reward=0.09 +/- 0.98\n",
      "Episode length: 30.20 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027098145 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.806      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0108      |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "Ep done - 5640.\n",
      "Ep done - 5650.\n",
      "Ep done - 5660.\n",
      "Ep done - 5670.\n",
      "Ep done - 5680.\n",
      "Ep done - 5690.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.14     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 1289     |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Ep done - 5700.\n",
      "Ep done - 5710.\n",
      "Ep done - 5720.\n",
      "Ep done - 5730.\n",
      "Ep done - 5740.\n",
      "Ep done - 5750.\n",
      "Ep done - 5760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 1312        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026257379 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.791      |\n",
      "|    explained_variance   | 0.0768      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Ep done - 5770.\n",
      "Ep done - 5780.\n",
      "Ep done - 5790.\n",
      "Ep done - 5800.\n",
      "Ep done - 5810.\n",
      "Ep done - 5820.\n",
      "Ep done - 5830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 1324        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026725166 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.802      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00586    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Ep done - 5840.\n",
      "Ep done - 5850.\n",
      "Ep done - 5860.\n",
      "Ep done - 5870.\n",
      "Ep done - 5880.\n",
      "Ep done - 5890.\n",
      "Ep done - 5900.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.24        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 1338        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026596742 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.765      |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00544    |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 5910.\n",
      "Ep done - 5920.\n",
      "Ep done - 5930.\n",
      "Ep done - 5940.\n",
      "Ep done - 5950.\n",
      "Ep done - 5960.\n",
      "Ep done - 1710.\n",
      "Ep done - 1720.\n",
      "Ep done - 1730.\n",
      "Ep done - 1740.\n",
      "Ep done - 1750.\n",
      "Ep done - 1760.\n",
      "Ep done - 1770.\n",
      "Ep done - 1780.\n",
      "Ep done - 1790.\n",
      "Ep done - 1800.\n",
      "Eval num_timesteps=180000, episode_reward=0.34 +/- 0.94\n",
      "Episode length: 30.19 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025114851 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.768      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00088    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.34\n",
      "SELFPLAY: new best model, bumping up generation to 8\n",
      "Ep done - 5970.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.06     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 1365     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Ep done - 5980.\n",
      "Ep done - 5990.\n",
      "Ep done - 6000.\n",
      "Ep done - 6010.\n",
      "Ep done - 6020.\n",
      "Ep done - 6030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 1379        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029280748 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Ep done - 6040.\n",
      "Ep done - 6050.\n",
      "Ep done - 6060.\n",
      "Ep done - 6070.\n",
      "Ep done - 6080.\n",
      "Ep done - 6090.\n",
      "Ep done - 6100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 1389        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028190896 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.802      |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00625     |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    value_loss           | 0.223       |\n",
      "-----------------------------------------\n",
      "Ep done - 6110.\n",
      "Ep done - 6120.\n",
      "Ep done - 6130.\n",
      "Ep done - 6140.\n",
      "Ep done - 6150.\n",
      "Ep done - 6160.\n",
      "Ep done - 6170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 1399        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028829575 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.0954      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.201       |\n",
      "-----------------------------------------\n",
      "Ep done - 6180.\n",
      "Ep done - 6190.\n",
      "Ep done - 6200.\n",
      "Ep done - 6210.\n",
      "Ep done - 6220.\n",
      "Ep done - 6230.\n",
      "Ep done - 6240.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.08      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 133       |\n",
      "|    iterations           | 92        |\n",
      "|    time_elapsed         | 1414      |\n",
      "|    total_timesteps      | 188416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0260543 |\n",
      "|    clip_fraction        | 0.217     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.789    |\n",
      "|    explained_variance   | 0.252     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0175   |\n",
      "|    n_updates            | 910       |\n",
      "|    policy_gradient_loss | -0.0469   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "Ep done - 6250.\n",
      "Ep done - 6260.\n",
      "Ep done - 6270.\n",
      "Ep done - 6280.\n",
      "Ep done - 6290.\n",
      "Ep done - 1810.\n",
      "Ep done - 1820.\n",
      "Ep done - 1830.\n",
      "Ep done - 1840.\n",
      "Ep done - 1850.\n",
      "Ep done - 1860.\n",
      "Ep done - 1870.\n",
      "Ep done - 1880.\n",
      "Ep done - 1890.\n",
      "Ep done - 1900.\n",
      "Eval num_timesteps=190000, episode_reward=0.21 +/- 0.97\n",
      "Episode length: 30.18 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.21        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028471243 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.788      |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00758     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.21\n",
      "SELFPLAY: new best model, bumping up generation to 9\n",
      "Ep done - 6300.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | -0.05    |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 1438     |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Ep done - 6310.\n",
      "Ep done - 6320.\n",
      "Ep done - 6330.\n",
      "Ep done - 6340.\n",
      "Ep done - 6350.\n",
      "Ep done - 6360.\n",
      "Ep done - 6370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 1452        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030637803 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00614    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n",
      "Ep done - 6380.\n",
      "Ep done - 6390.\n",
      "Ep done - 6400.\n",
      "Ep done - 6410.\n",
      "Ep done - 6420.\n",
      "Ep done - 6430.\n",
      "Ep done - 6440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1471        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030661073 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00419    |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 6450.\n",
      "Ep done - 6460.\n",
      "Ep done - 6470.\n",
      "Ep done - 6480.\n",
      "Ep done - 6490.\n",
      "Ep done - 6500.\n",
      "Ep done - 6510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1491        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029957514 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0523      |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    value_loss           | 0.22        |\n",
      "-----------------------------------------\n",
      "Ep done - 6520.\n",
      "Ep done - 6530.\n",
      "Ep done - 6540.\n",
      "Ep done - 6550.\n",
      "Ep done - 6560.\n",
      "Ep done - 6570.\n",
      "Ep done - 6580.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 1500        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031338923 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Ep done - 6590.\n",
      "Ep done - 6600.\n",
      "Ep done - 6610.\n",
      "Ep done - 6620.\n",
      "Ep done - 1910.\n",
      "Ep done - 1920.\n",
      "Ep done - 1930.\n",
      "Ep done - 1940.\n",
      "Ep done - 1950.\n",
      "Ep done - 1960.\n",
      "Ep done - 1970.\n",
      "Ep done - 1980.\n",
      "Ep done - 1990.\n",
      "Ep done - 2000.\n",
      "Eval num_timesteps=200000, episode_reward=0.23 +/- 0.97\n",
      "Episode length: 30.14 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034525465 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.734      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.23\n",
      "SELFPLAY: new best model, bumping up generation to 10\n",
      "Ep done - 6630.\n",
      "Ep done - 6640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.25     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 1520     |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Ep done - 6650.\n",
      "Ep done - 6660.\n",
      "Ep done - 6670.\n",
      "Ep done - 6680.\n",
      "Ep done - 6690.\n",
      "Ep done - 6700.\n",
      "Ep done - 6710.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.05        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 1530        |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026140783 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0191      |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Ep done - 6720.\n",
      "Ep done - 6730.\n",
      "Ep done - 6740.\n",
      "Ep done - 6750.\n",
      "Ep done - 6760.\n",
      "Ep done - 6770.\n",
      "Ep done - 6780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.06        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 1540        |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029418247 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.721      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0327      |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 6790.\n",
      "Ep done - 6800.\n",
      "Ep done - 6810.\n",
      "Ep done - 6820.\n",
      "Ep done - 6830.\n",
      "Ep done - 6840.\n",
      "Ep done - 6850.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.19       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 133        |\n",
      "|    iterations           | 101        |\n",
      "|    time_elapsed         | 1549       |\n",
      "|    total_timesteps      | 206848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02942226 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.701     |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00713    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "Ep done - 6860.\n",
      "Ep done - 6870.\n",
      "Ep done - 6880.\n",
      "Ep done - 6890.\n",
      "Ep done - 6900.\n",
      "Ep done - 6910.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.3      |\n",
      "|    ep_rew_mean          | 0.23      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 133       |\n",
      "|    iterations           | 102       |\n",
      "|    time_elapsed         | 1559      |\n",
      "|    total_timesteps      | 208896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0341604 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.7      |\n",
      "|    explained_variance   | 0.229     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0376    |\n",
      "|    n_updates            | 1010      |\n",
      "|    policy_gradient_loss | -0.0408   |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "Ep done - 6920.\n",
      "Ep done - 6930.\n",
      "Ep done - 6940.\n",
      "Ep done - 6950.\n",
      "Ep done - 2010.\n",
      "Ep done - 2020.\n",
      "Ep done - 2030.\n",
      "Ep done - 2040.\n",
      "Ep done - 2050.\n",
      "Ep done - 2060.\n",
      "Ep done - 2070.\n",
      "Ep done - 2080.\n",
      "Ep done - 2090.\n",
      "Ep done - 2100.\n",
      "Eval num_timesteps=210000, episode_reward=0.30 +/- 0.93\n",
      "Episode length: 30.20 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 210000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027185675 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00496     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.3\n",
      "SELFPLAY: new best model, bumping up generation to 11\n",
      "Ep done - 6960.\n",
      "Ep done - 6970.\n",
      "Ep done - 6980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.19     |\n",
      "| time/              |          |\n",
      "|    fps             | 131      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 1600     |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "Ep done - 6990.\n",
      "Ep done - 7000.\n",
      "Ep done - 7010.\n",
      "Ep done - 7020.\n",
      "Ep done - 7030.\n",
      "Ep done - 7040.\n",
      "Ep done - 7050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.1         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 1627        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031510767 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0096      |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Ep done - 7060.\n",
      "Ep done - 7070.\n",
      "Ep done - 7080.\n",
      "Ep done - 7090.\n",
      "Ep done - 7100.\n",
      "Ep done - 7110.\n",
      "Ep done - 7120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 1636        |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030613162 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0292      |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Ep done - 7130.\n",
      "Ep done - 7140.\n",
      "Ep done - 7150.\n",
      "Ep done - 7160.\n",
      "Ep done - 7170.\n",
      "Ep done - 7180.\n",
      "Ep done - 7190.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.25       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 131        |\n",
      "|    iterations           | 106        |\n",
      "|    time_elapsed         | 1646       |\n",
      "|    total_timesteps      | 217088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03193322 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.632     |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "Ep done - 7200.\n",
      "Ep done - 7210.\n",
      "Ep done - 7220.\n",
      "Ep done - 7230.\n",
      "Ep done - 7240.\n",
      "Ep done - 7250.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.16        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 1655        |\n",
      "|    total_timesteps      | 219136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027345784 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00508    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Ep done - 7260.\n",
      "Ep done - 7270.\n",
      "Ep done - 7280.\n",
      "Ep done - 2110.\n",
      "Ep done - 2120.\n",
      "Ep done - 2130.\n",
      "Ep done - 2140.\n",
      "Ep done - 2150.\n",
      "Ep done - 2160.\n",
      "Ep done - 2170.\n",
      "Ep done - 2180.\n",
      "Ep done - 2190.\n",
      "Ep done - 2200.\n",
      "Eval num_timesteps=220000, episode_reward=0.19 +/- 0.98\n",
      "Episode length: 30.20 +/- 0.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.19        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031164316 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00439     |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    value_loss           | 0.194       |\n",
      "-----------------------------------------\n",
      "Ep done - 7290.\n",
      "Ep done - 7300.\n",
      "Ep done - 7310.\n",
      "Ep done - 7320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 132      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 1674     |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Ep done - 7330.\n",
      "Ep done - 7340.\n",
      "Ep done - 7350.\n",
      "Ep done - 7360.\n",
      "Ep done - 7370.\n",
      "Ep done - 7380.\n",
      "Ep done - 7390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 1685        |\n",
      "|    total_timesteps      | 223232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031926934 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00422     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Ep done - 7400.\n",
      "Ep done - 7410.\n",
      "Ep done - 7420.\n",
      "Ep done - 7430.\n",
      "Ep done - 7440.\n",
      "Ep done - 7450.\n",
      "Ep done - 7460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.19       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 132        |\n",
      "|    iterations           | 110        |\n",
      "|    time_elapsed         | 1695       |\n",
      "|    total_timesteps      | 225280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03323596 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.623     |\n",
      "|    explained_variance   | 0.269      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0165     |\n",
      "|    n_updates            | 1090       |\n",
      "|    policy_gradient_loss | -0.0375    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "Ep done - 7470.\n",
      "Ep done - 7480.\n",
      "Ep done - 7490.\n",
      "Ep done - 7500.\n",
      "Ep done - 7510.\n",
      "Ep done - 7520.\n",
      "Ep done - 7530.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.17       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 133        |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 1704       |\n",
      "|    total_timesteps      | 227328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03129824 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.604     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 7540.\n",
      "Ep done - 7550.\n",
      "Ep done - 7560.\n",
      "Ep done - 7570.\n",
      "Ep done - 7580.\n",
      "Ep done - 7590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 1713        |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025999863 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.227       |\n",
      "-----------------------------------------\n",
      "Ep done - 7600.\n",
      "Ep done - 7610.\n",
      "Ep done - 2210.\n",
      "Ep done - 2220.\n",
      "Ep done - 2230.\n",
      "Ep done - 2240.\n",
      "Ep done - 2250.\n",
      "Ep done - 2260.\n",
      "Ep done - 2270.\n",
      "Ep done - 2280.\n",
      "Ep done - 2290.\n",
      "Ep done - 2300.\n",
      "Eval num_timesteps=230000, episode_reward=0.37 +/- 0.90\n",
      "Episode length: 30.33 +/- 0.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027733723 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0377      |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.37\n",
      "SELFPLAY: new best model, bumping up generation to 12\n",
      "Ep done - 7620.\n",
      "Ep done - 7630.\n",
      "Ep done - 7640.\n",
      "Ep done - 7650.\n",
      "Ep done - 7660.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.34     |\n",
      "| time/              |          |\n",
      "|    fps             | 133      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 1739     |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "Ep done - 7670.\n",
      "Ep done - 7680.\n",
      "Ep done - 7690.\n",
      "Ep done - 7700.\n",
      "Ep done - 7710.\n",
      "Ep done - 7720.\n",
      "Ep done - 7730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 1749        |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034189947 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0122      |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Ep done - 7740.\n",
      "Ep done - 7750.\n",
      "Ep done - 7760.\n",
      "Ep done - 7770.\n",
      "Ep done - 7780.\n",
      "Ep done - 7790.\n",
      "Ep done - 7800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 1758        |\n",
      "|    total_timesteps      | 235520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031046908 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Ep done - 7810.\n",
      "Ep done - 7820.\n",
      "Ep done - 7830.\n",
      "Ep done - 7840.\n",
      "Ep done - 7850.\n",
      "Ep done - 7860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 116         |\n",
      "|    time_elapsed         | 1768        |\n",
      "|    total_timesteps      | 237568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027252084 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0334     |\n",
      "|    value_loss           | 0.206       |\n",
      "-----------------------------------------\n",
      "Ep done - 7870.\n",
      "Ep done - 7880.\n",
      "Ep done - 7890.\n",
      "Ep done - 7900.\n",
      "Ep done - 7910.\n",
      "Ep done - 7920.\n",
      "Ep done - 7930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 1777        |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031019244 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Ep done - 7940.\n",
      "Ep done - 2310.\n",
      "Ep done - 2320.\n",
      "Ep done - 2330.\n",
      "Ep done - 2340.\n",
      "Ep done - 2350.\n",
      "Ep done - 2360.\n",
      "Ep done - 2370.\n",
      "Ep done - 2380.\n",
      "Ep done - 2390.\n",
      "Ep done - 2400.\n",
      "Eval num_timesteps=240000, episode_reward=0.32 +/- 0.94\n",
      "Episode length: 30.25 +/- 0.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030765332 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.32\n",
      "SELFPLAY: new best model, bumping up generation to 13\n",
      "Ep done - 7950.\n",
      "Ep done - 7960.\n",
      "Ep done - 7970.\n",
      "Ep done - 7980.\n",
      "Ep done - 7990.\n",
      "Ep done - 8000.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.17     |\n",
      "| time/              |          |\n",
      "|    fps             | 134      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 1796     |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Ep done - 8010.\n",
      "Ep done - 8020.\n",
      "Ep done - 8030.\n",
      "Ep done - 8040.\n",
      "Ep done - 8050.\n",
      "Ep done - 8060.\n",
      "Ep done - 8070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 1805        |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032153536 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0337      |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 8080.\n",
      "Ep done - 8090.\n",
      "Ep done - 8100.\n",
      "Ep done - 8110.\n",
      "Ep done - 8120.\n",
      "Ep done - 8130.\n",
      "Ep done - 8140.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 1815        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027335111 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00815     |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Ep done - 8150.\n",
      "Ep done - 8160.\n",
      "Ep done - 8170.\n",
      "Ep done - 8180.\n",
      "Ep done - 8190.\n",
      "Ep done - 8200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 1824        |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029546827 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00194    |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "Ep done - 8210.\n",
      "Ep done - 8220.\n",
      "Ep done - 8230.\n",
      "Ep done - 8240.\n",
      "Ep done - 8250.\n",
      "Ep done - 8260.\n",
      "Ep done - 8270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 1834        |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029517502 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00753     |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Ep done - 8280.\n",
      "Ep done - 2410.\n",
      "Ep done - 2420.\n",
      "Ep done - 2430.\n",
      "Ep done - 2440.\n",
      "Ep done - 2450.\n",
      "Ep done - 2460.\n",
      "Ep done - 2470.\n",
      "Ep done - 2480.\n",
      "Ep done - 2490.\n",
      "Ep done - 2500.\n",
      "Eval num_timesteps=250000, episode_reward=0.22 +/- 0.97\n",
      "Episode length: 30.22 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030373491 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00346    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.218       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.22\n",
      "SELFPLAY: new best model, bumping up generation to 14\n",
      "Ep done - 8290.\n",
      "Ep done - 8300.\n",
      "Ep done - 8310.\n",
      "Ep done - 8320.\n",
      "Ep done - 8330.\n",
      "Ep done - 8340.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 135      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 1852     |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Ep done - 8350.\n",
      "Ep done - 8360.\n",
      "Ep done - 8370.\n",
      "Ep done - 8380.\n",
      "Ep done - 8390.\n",
      "Ep done - 8400.\n",
      "Ep done - 8410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 136         |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 1862        |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027405191 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00593     |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Ep done - 8420.\n",
      "Ep done - 8430.\n",
      "Ep done - 8440.\n",
      "Ep done - 8450.\n",
      "Ep done - 8460.\n",
      "Ep done - 8470.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.31       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 136        |\n",
      "|    iterations           | 125        |\n",
      "|    time_elapsed         | 1871       |\n",
      "|    total_timesteps      | 256000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03349289 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 8480.\n",
      "Ep done - 8490.\n",
      "Ep done - 8500.\n",
      "Ep done - 8510.\n",
      "Ep done - 8520.\n",
      "Ep done - 8530.\n",
      "Ep done - 8540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.15        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 1880        |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024385396 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Ep done - 8550.\n",
      "Ep done - 8560.\n",
      "Ep done - 8570.\n",
      "Ep done - 8580.\n",
      "Ep done - 8590.\n",
      "Ep done - 8600.\n",
      "Ep done - 8610.\n",
      "Ep done - 2510.\n",
      "Ep done - 2520.\n",
      "Ep done - 2530.\n",
      "Ep done - 2540.\n",
      "Ep done - 2550.\n",
      "Ep done - 2560.\n",
      "Ep done - 2570.\n",
      "Ep done - 2580.\n",
      "Ep done - 2590.\n",
      "Ep done - 2600.\n",
      "Eval num_timesteps=260000, episode_reward=0.40 +/- 0.91\n",
      "Episode length: 30.33 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.4         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030040126 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.4\n",
      "SELFPLAY: new best model, bumping up generation to 15\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 136      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 1899     |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "Ep done - 8620.\n",
      "Ep done - 8630.\n",
      "Ep done - 8640.\n",
      "Ep done - 8650.\n",
      "Ep done - 8660.\n",
      "Ep done - 8670.\n",
      "Ep done - 8680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 137         |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 1909        |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026179533 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Ep done - 8690.\n",
      "Ep done - 8700.\n",
      "Ep done - 8710.\n",
      "Ep done - 8720.\n",
      "Ep done - 8730.\n",
      "Ep done - 8740.\n",
      "Ep done - 8750.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.42      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 137       |\n",
      "|    iterations           | 129       |\n",
      "|    time_elapsed         | 1918      |\n",
      "|    total_timesteps      | 264192    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0330853 |\n",
      "|    clip_fraction        | 0.197     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.573    |\n",
      "|    explained_variance   | 0.118     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0148   |\n",
      "|    n_updates            | 1280      |\n",
      "|    policy_gradient_loss | -0.0372   |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "Ep done - 8760.\n",
      "Ep done - 8770.\n",
      "Ep done - 8780.\n",
      "Ep done - 8790.\n",
      "Ep done - 8800.\n",
      "Ep done - 8810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 1928        |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029418647 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00596     |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "Ep done - 8820.\n",
      "Ep done - 8830.\n",
      "Ep done - 8840.\n",
      "Ep done - 8850.\n",
      "Ep done - 8860.\n",
      "Ep done - 8870.\n",
      "Ep done - 8880.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 131         |\n",
      "|    time_elapsed         | 1937        |\n",
      "|    total_timesteps      | 268288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030103624 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00446     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 8890.\n",
      "Ep done - 8900.\n",
      "Ep done - 8910.\n",
      "Ep done - 8920.\n",
      "Ep done - 8930.\n",
      "Ep done - 8940.\n",
      "Ep done - 2610.\n",
      "Ep done - 2620.\n",
      "Ep done - 2630.\n",
      "Ep done - 2640.\n",
      "Ep done - 2650.\n",
      "Ep done - 2660.\n",
      "Ep done - 2670.\n",
      "Ep done - 2680.\n",
      "Ep done - 2690.\n",
      "Ep done - 2700.\n",
      "Eval num_timesteps=270000, episode_reward=0.36 +/- 0.93\n",
      "Episode length: 30.22 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032545608 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.279       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.36\n",
      "SELFPLAY: new best model, bumping up generation to 16\n",
      "Ep done - 8950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 0.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 138      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 1956     |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Ep done - 8960.\n",
      "Ep done - 8970.\n",
      "Ep done - 8980.\n",
      "Ep done - 8990.\n",
      "Ep done - 9000.\n",
      "Ep done - 9010.\n",
      "Ep done - 9020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 1965        |\n",
      "|    total_timesteps      | 272384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030000227 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "Ep done - 9030.\n",
      "Ep done - 9040.\n",
      "Ep done - 9050.\n",
      "Ep done - 9060.\n",
      "Ep done - 9070.\n",
      "Ep done - 9080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 1975        |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032583915 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0296      |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Ep done - 9090.\n",
      "Ep done - 9100.\n",
      "Ep done - 9110.\n",
      "Ep done - 9120.\n",
      "Ep done - 9130.\n",
      "Ep done - 9140.\n",
      "Ep done - 9150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 135         |\n",
      "|    time_elapsed         | 1984        |\n",
      "|    total_timesteps      | 276480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032880984 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0051     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 9160.\n",
      "Ep done - 9170.\n",
      "Ep done - 9180.\n",
      "Ep done - 9190.\n",
      "Ep done - 9200.\n",
      "Ep done - 9210.\n",
      "Ep done - 9220.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 139         |\n",
      "|    iterations           | 136         |\n",
      "|    time_elapsed         | 1994        |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032934804 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "Ep done - 9230.\n",
      "Ep done - 9240.\n",
      "Ep done - 9250.\n",
      "Ep done - 9260.\n",
      "Ep done - 9270.\n",
      "Ep done - 2710.\n",
      "Ep done - 2720.\n",
      "Ep done - 2730.\n",
      "Ep done - 2740.\n",
      "Ep done - 2750.\n",
      "Ep done - 2760.\n",
      "Ep done - 2770.\n",
      "Ep done - 2780.\n",
      "Ep done - 2790.\n",
      "Ep done - 2800.\n",
      "Eval num_timesteps=280000, episode_reward=0.24 +/- 0.95\n",
      "Episode length: 30.15 +/- 0.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 280000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03107024 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.544     |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0193     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.24\n",
      "SELFPLAY: new best model, bumping up generation to 17\n",
      "Ep done - 9280.\n",
      "Ep done - 9290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 139      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 2013     |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "Ep done - 9300.\n",
      "Ep done - 9310.\n",
      "Ep done - 9320.\n",
      "Ep done - 9330.\n",
      "Ep done - 9340.\n",
      "Ep done - 9350.\n",
      "Ep done - 9360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 138        |\n",
      "|    time_elapsed         | 2022       |\n",
      "|    total_timesteps      | 282624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03647686 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | 0.162      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 1370       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    value_loss           | 0.171      |\n",
      "----------------------------------------\n",
      "Ep done - 9370.\n",
      "Ep done - 9380.\n",
      "Ep done - 9390.\n",
      "Ep done - 9400.\n",
      "Ep done - 9410.\n",
      "Ep done - 9420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 2032        |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031769227 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 9430.\n",
      "Ep done - 9440.\n",
      "Ep done - 9450.\n",
      "Ep done - 9460.\n",
      "Ep done - 9470.\n",
      "Ep done - 9480.\n",
      "Ep done - 9490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.22        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 2041        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028262924 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Ep done - 9500.\n",
      "Ep done - 9510.\n",
      "Ep done - 9520.\n",
      "Ep done - 9530.\n",
      "Ep done - 9540.\n",
      "Ep done - 9550.\n",
      "Ep done - 9560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 141         |\n",
      "|    time_elapsed         | 2051        |\n",
      "|    total_timesteps      | 288768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030467674 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00611     |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Ep done - 9570.\n",
      "Ep done - 9580.\n",
      "Ep done - 9590.\n",
      "Ep done - 9600.\n",
      "Ep done - 2810.\n",
      "Ep done - 2820.\n",
      "Ep done - 2830.\n",
      "Ep done - 2840.\n",
      "Ep done - 2850.\n",
      "Ep done - 2860.\n",
      "Ep done - 2870.\n",
      "Ep done - 2880.\n",
      "Ep done - 2890.\n",
      "Ep done - 2900.\n",
      "Eval num_timesteps=290000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.29 +/- 0.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 290000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03282795 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.544     |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00697    |\n",
      "|    n_updates            | 1410       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 18\n",
      "Ep done - 9610.\n",
      "Ep done - 9620.\n",
      "Ep done - 9630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 140      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 2069     |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "Ep done - 9640.\n",
      "Ep done - 9650.\n",
      "Ep done - 9660.\n",
      "Ep done - 9670.\n",
      "Ep done - 9680.\n",
      "Ep done - 9690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.21        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 2079        |\n",
      "|    total_timesteps      | 292864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033966593 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 9700.\n",
      "Ep done - 9710.\n",
      "Ep done - 9720.\n",
      "Ep done - 9730.\n",
      "Ep done - 9740.\n",
      "Ep done - 9750.\n",
      "Ep done - 9760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 144        |\n",
      "|    time_elapsed         | 2088       |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02708469 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.52      |\n",
      "|    explained_variance   | 0.26       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0446     |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    value_loss           | 0.191      |\n",
      "----------------------------------------\n",
      "Ep done - 9770.\n",
      "Ep done - 9780.\n",
      "Ep done - 9790.\n",
      "Ep done - 9800.\n",
      "Ep done - 9810.\n",
      "Ep done - 9820.\n",
      "Ep done - 9830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 2098        |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028048769 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.0621      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Ep done - 9840.\n",
      "Ep done - 9850.\n",
      "Ep done - 9860.\n",
      "Ep done - 9870.\n",
      "Ep done - 9880.\n",
      "Ep done - 9890.\n",
      "Ep done - 9900.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 2107        |\n",
      "|    total_timesteps      | 299008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027999362 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.0553      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000936    |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 9910.\n",
      "Ep done - 9920.\n",
      "Ep done - 9930.\n",
      "Ep done - 2910.\n",
      "Ep done - 2920.\n",
      "Ep done - 2930.\n",
      "Ep done - 2940.\n",
      "Ep done - 2950.\n",
      "Ep done - 2960.\n",
      "Ep done - 2970.\n",
      "Ep done - 2980.\n",
      "Ep done - 2990.\n",
      "Ep done - 3000.\n",
      "Eval num_timesteps=300000, episode_reward=0.38 +/- 0.91\n",
      "Episode length: 30.29 +/- 0.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 300000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028610766 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00789     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.38\n",
      "SELFPLAY: new best model, bumping up generation to 19\n",
      "Ep done - 9940.\n",
      "Ep done - 9950.\n",
      "Ep done - 9960.\n",
      "Ep done - 9970.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 141      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 2126     |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Ep done - 9980.\n",
      "Ep done - 9990.\n",
      "Ep done - 10000.\n",
      "Ep done - 10010.\n",
      "Ep done - 10020.\n",
      "Ep done - 10030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 141         |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 2135        |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025630783 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 10040.\n",
      "Ep done - 10050.\n",
      "Ep done - 10060.\n",
      "Ep done - 10070.\n",
      "Ep done - 10080.\n",
      "Ep done - 10090.\n",
      "Ep done - 10100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 149         |\n",
      "|    time_elapsed         | 2144        |\n",
      "|    total_timesteps      | 305152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033656295 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0181      |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 10110.\n",
      "Ep done - 10120.\n",
      "Ep done - 10130.\n",
      "Ep done - 10140.\n",
      "Ep done - 10150.\n",
      "Ep done - 10160.\n",
      "Ep done - 10170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 2153        |\n",
      "|    total_timesteps      | 307200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024309091 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 10180.\n",
      "Ep done - 10190.\n",
      "Ep done - 10200.\n",
      "Ep done - 10210.\n",
      "Ep done - 10220.\n",
      "Ep done - 10230.\n",
      "Ep done - 10240.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 2163        |\n",
      "|    total_timesteps      | 309248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029144593 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 10250.\n",
      "Ep done - 10260.\n",
      "Ep done - 3010.\n",
      "Ep done - 3020.\n",
      "Ep done - 3030.\n",
      "Ep done - 3040.\n",
      "Ep done - 3050.\n",
      "Ep done - 3060.\n",
      "Ep done - 3070.\n",
      "Ep done - 3080.\n",
      "Ep done - 3090.\n",
      "Ep done - 3100.\n",
      "Eval num_timesteps=310000, episode_reward=0.47 +/- 0.87\n",
      "Episode length: 30.24 +/- 0.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 310000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02692476 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.495     |\n",
      "|    explained_variance   | 0.213      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0167     |\n",
      "|    n_updates            | 1510       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 20\n",
      "Ep done - 10270.\n",
      "Ep done - 10280.\n",
      "Ep done - 10290.\n",
      "Ep done - 10300.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 142      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 2181     |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Ep done - 10310.\n",
      "Ep done - 10320.\n",
      "Ep done - 10330.\n",
      "Ep done - 10340.\n",
      "Ep done - 10350.\n",
      "Ep done - 10360.\n",
      "Ep done - 10370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 2191        |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036229238 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 10380.\n",
      "Ep done - 10390.\n",
      "Ep done - 10400.\n",
      "Ep done - 10410.\n",
      "Ep done - 10420.\n",
      "Ep done - 10430.\n",
      "Ep done - 10440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 2200        |\n",
      "|    total_timesteps      | 315392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029200837 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00769    |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Ep done - 10450.\n",
      "Ep done - 10460.\n",
      "Ep done - 10470.\n",
      "Ep done - 10480.\n",
      "Ep done - 10490.\n",
      "Ep done - 10500.\n",
      "Ep done - 10510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 2209        |\n",
      "|    total_timesteps      | 317440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037593663 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00636     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 10520.\n",
      "Ep done - 10530.\n",
      "Ep done - 10540.\n",
      "Ep done - 10550.\n",
      "Ep done - 10560.\n",
      "Ep done - 10570.\n",
      "Ep done - 10580.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 2219        |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030138886 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0349      |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Ep done - 10590.\n",
      "Ep done - 3110.\n",
      "Ep done - 3120.\n",
      "Ep done - 3130.\n",
      "Ep done - 3140.\n",
      "Ep done - 3150.\n",
      "Ep done - 3160.\n",
      "Ep done - 3170.\n",
      "Ep done - 3180.\n",
      "Ep done - 3190.\n",
      "Ep done - 3200.\n",
      "Eval num_timesteps=320000, episode_reward=0.34 +/- 0.92\n",
      "Episode length: 30.28 +/- 0.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 320000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02920974 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.491     |\n",
      "|    explained_variance   | 0.273      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00498    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0325    |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.34\n",
      "SELFPLAY: new best model, bumping up generation to 21\n",
      "Ep done - 10600.\n",
      "Ep done - 10610.\n",
      "Ep done - 10620.\n",
      "Ep done - 10630.\n",
      "Ep done - 10640.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 143      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 2238     |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "Ep done - 10650.\n",
      "Ep done - 10660.\n",
      "Ep done - 10670.\n",
      "Ep done - 10680.\n",
      "Ep done - 10690.\n",
      "Ep done - 10700.\n",
      "Ep done - 10710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.32       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 143        |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 2247       |\n",
      "|    total_timesteps      | 323584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02502874 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.509     |\n",
      "|    explained_variance   | 0.0806     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0201     |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    value_loss           | 0.19       |\n",
      "----------------------------------------\n",
      "Ep done - 10720.\n",
      "Ep done - 10730.\n",
      "Ep done - 10740.\n",
      "Ep done - 10750.\n",
      "Ep done - 10760.\n",
      "Ep done - 10770.\n",
      "Ep done - 10780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 159         |\n",
      "|    time_elapsed         | 2256        |\n",
      "|    total_timesteps      | 325632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028168334 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0336      |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 10790.\n",
      "Ep done - 10800.\n",
      "Ep done - 10810.\n",
      "Ep done - 10820.\n",
      "Ep done - 10830.\n",
      "Ep done - 10840.\n",
      "Ep done - 10850.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 2265        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024886655 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00713    |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Ep done - 10860.\n",
      "Ep done - 10870.\n",
      "Ep done - 10880.\n",
      "Ep done - 10890.\n",
      "Ep done - 10900.\n",
      "Ep done - 10910.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 2275        |\n",
      "|    total_timesteps      | 329728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030209493 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0714      |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Ep done - 10920.\n",
      "Ep done - 3210.\n",
      "Ep done - 3220.\n",
      "Ep done - 3230.\n",
      "Ep done - 3240.\n",
      "Ep done - 3250.\n",
      "Ep done - 3260.\n",
      "Ep done - 3270.\n",
      "Ep done - 3280.\n",
      "Ep done - 3290.\n",
      "Ep done - 3300.\n",
      "Eval num_timesteps=330000, episode_reward=0.31 +/- 0.93\n",
      "Episode length: 30.18 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.31        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026711807 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.31\n",
      "SELFPLAY: new best model, bumping up generation to 22\n",
      "Ep done - 10930.\n",
      "Ep done - 10940.\n",
      "Ep done - 10950.\n",
      "Ep done - 10960.\n",
      "Ep done - 10970.\n",
      "Ep done - 10980.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 144      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 2294     |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Ep done - 10990.\n",
      "Ep done - 11000.\n",
      "Ep done - 11010.\n",
      "Ep done - 11020.\n",
      "Ep done - 11030.\n",
      "Ep done - 11040.\n",
      "Ep done - 11050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.2         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 144         |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 2303        |\n",
      "|    total_timesteps      | 333824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027019514 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.165       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00843    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Ep done - 11060.\n",
      "Ep done - 11070.\n",
      "Ep done - 11080.\n",
      "Ep done - 11090.\n",
      "Ep done - 11100.\n",
      "Ep done - 11110.\n",
      "Ep done - 11120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 2313        |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025710117 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00915     |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.209       |\n",
      "-----------------------------------------\n",
      "Ep done - 11130.\n",
      "Ep done - 11140.\n",
      "Ep done - 11150.\n",
      "Ep done - 11160.\n",
      "Ep done - 11170.\n",
      "Ep done - 11180.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 2322        |\n",
      "|    total_timesteps      | 337920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028707687 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00652     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 11190.\n",
      "Ep done - 11200.\n",
      "Ep done - 11210.\n",
      "Ep done - 11220.\n",
      "Ep done - 11230.\n",
      "Ep done - 11240.\n",
      "Ep done - 11250.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 145        |\n",
      "|    iterations           | 166        |\n",
      "|    time_elapsed         | 2332       |\n",
      "|    total_timesteps      | 339968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02725213 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.482     |\n",
      "|    explained_variance   | 0.144      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 1650       |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n",
      "Ep done - 3310.\n",
      "Ep done - 3320.\n",
      "Ep done - 3330.\n",
      "Ep done - 3340.\n",
      "Ep done - 3350.\n",
      "Ep done - 3360.\n",
      "Ep done - 3370.\n",
      "Ep done - 3380.\n",
      "Ep done - 3390.\n",
      "Ep done - 3400.\n",
      "Eval num_timesteps=340000, episode_reward=0.35 +/- 0.92\n",
      "Episode length: 30.21 +/- 0.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027428199 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.35\n",
      "SELFPLAY: new best model, bumping up generation to 23\n",
      "Ep done - 11260.\n",
      "Ep done - 11270.\n",
      "Ep done - 11280.\n",
      "Ep done - 11290.\n",
      "Ep done - 11300.\n",
      "Ep done - 11310.\n",
      "Ep done - 11320.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.24     |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 2351     |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "Ep done - 11330.\n",
      "Ep done - 11340.\n",
      "Ep done - 11350.\n",
      "Ep done - 11360.\n",
      "Ep done - 11370.\n",
      "Ep done - 11380.\n",
      "Ep done - 11390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 168         |\n",
      "|    time_elapsed         | 2360        |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045378447 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.012       |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    value_loss           | 0.196       |\n",
      "-----------------------------------------\n",
      "Ep done - 11400.\n",
      "Ep done - 11410.\n",
      "Ep done - 11420.\n",
      "Ep done - 11430.\n",
      "Ep done - 11440.\n",
      "Ep done - 11450.\n",
      "Ep done - 11460.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 169         |\n",
      "|    time_elapsed         | 2369        |\n",
      "|    total_timesteps      | 346112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028428905 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0495      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Ep done - 11470.\n",
      "Ep done - 11480.\n",
      "Ep done - 11490.\n",
      "Ep done - 11500.\n",
      "Ep done - 11510.\n",
      "Ep done - 11520.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 2379        |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029280521 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "Ep done - 11530.\n",
      "Ep done - 11540.\n",
      "Ep done - 11550.\n",
      "Ep done - 11560.\n",
      "Ep done - 11570.\n",
      "Ep done - 11580.\n",
      "Ep done - 3410.\n",
      "Ep done - 3420.\n",
      "Ep done - 3430.\n",
      "Ep done - 3440.\n",
      "Ep done - 3450.\n",
      "Ep done - 3460.\n",
      "Ep done - 3470.\n",
      "Ep done - 3480.\n",
      "Ep done - 3490.\n",
      "Ep done - 3500.\n",
      "Eval num_timesteps=350000, episode_reward=0.37 +/- 0.91\n",
      "Episode length: 30.27 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028085437 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0448      |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.37\n",
      "SELFPLAY: new best model, bumping up generation to 24\n",
      "Ep done - 11590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.38     |\n",
      "| time/              |          |\n",
      "|    fps             | 145      |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 2400     |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n",
      "Ep done - 11600.\n",
      "Ep done - 11610.\n",
      "Ep done - 11620.\n",
      "Ep done - 11630.\n",
      "Ep done - 11640.\n",
      "Ep done - 11650.\n",
      "Ep done - 11660.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 146        |\n",
      "|    iterations           | 172        |\n",
      "|    time_elapsed         | 2409       |\n",
      "|    total_timesteps      | 352256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03242386 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.538     |\n",
      "|    explained_variance   | 0.142      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00674    |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Ep done - 11670.\n",
      "Ep done - 11680.\n",
      "Ep done - 11690.\n",
      "Ep done - 11700.\n",
      "Ep done - 11710.\n",
      "Ep done - 11720.\n",
      "Ep done - 11730.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 146        |\n",
      "|    iterations           | 173        |\n",
      "|    time_elapsed         | 2418       |\n",
      "|    total_timesteps      | 354304     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03266006 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.555     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00323    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "Ep done - 11740.\n",
      "Ep done - 11750.\n",
      "Ep done - 11760.\n",
      "Ep done - 11770.\n",
      "Ep done - 11780.\n",
      "Ep done - 11790.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 174         |\n",
      "|    time_elapsed         | 2428        |\n",
      "|    total_timesteps      | 356352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027000997 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0744      |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "Ep done - 11800.\n",
      "Ep done - 11810.\n",
      "Ep done - 11820.\n",
      "Ep done - 11830.\n",
      "Ep done - 11840.\n",
      "Ep done - 11850.\n",
      "Ep done - 11860.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 2437        |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030925184 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.0168      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00716     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 0.208       |\n",
      "-----------------------------------------\n",
      "Ep done - 11870.\n",
      "Ep done - 11880.\n",
      "Ep done - 11890.\n",
      "Ep done - 11900.\n",
      "Ep done - 11910.\n",
      "Ep done - 3510.\n",
      "Ep done - 3520.\n",
      "Ep done - 3530.\n",
      "Ep done - 3540.\n",
      "Ep done - 3550.\n",
      "Ep done - 3560.\n",
      "Ep done - 3570.\n",
      "Ep done - 3580.\n",
      "Ep done - 3590.\n",
      "Ep done - 3600.\n",
      "Eval num_timesteps=360000, episode_reward=0.25 +/- 0.96\n",
      "Episode length: 30.21 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 360000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028577236 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00626     |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.25\n",
      "SELFPLAY: new best model, bumping up generation to 25\n",
      "Ep done - 11920.\n",
      "Ep done - 11930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 146      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 2456     |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Ep done - 11940.\n",
      "Ep done - 11950.\n",
      "Ep done - 11960.\n",
      "Ep done - 11970.\n",
      "Ep done - 11980.\n",
      "Ep done - 11990.\n",
      "Ep done - 12000.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 2465        |\n",
      "|    total_timesteps      | 362496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029006835 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 12010.\n",
      "Ep done - 12020.\n",
      "Ep done - 12030.\n",
      "Ep done - 12040.\n",
      "Ep done - 12050.\n",
      "Ep done - 12060.\n",
      "Ep done - 12070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 2475        |\n",
      "|    total_timesteps      | 364544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028077167 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00665     |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 12080.\n",
      "Ep done - 12090.\n",
      "Ep done - 12100.\n",
      "Ep done - 12110.\n",
      "Ep done - 12120.\n",
      "Ep done - 12130.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 179         |\n",
      "|    time_elapsed         | 2484        |\n",
      "|    total_timesteps      | 366592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031967547 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.049       |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 0.226       |\n",
      "-----------------------------------------\n",
      "Ep done - 12140.\n",
      "Ep done - 12150.\n",
      "Ep done - 12160.\n",
      "Ep done - 12170.\n",
      "Ep done - 12180.\n",
      "Ep done - 12190.\n",
      "Ep done - 12200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 2493        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028238604 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.046       |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "Ep done - 12210.\n",
      "Ep done - 12220.\n",
      "Ep done - 12230.\n",
      "Ep done - 12240.\n",
      "Ep done - 12250.\n",
      "Ep done - 3610.\n",
      "Ep done - 3620.\n",
      "Ep done - 3630.\n",
      "Ep done - 3640.\n",
      "Ep done - 3650.\n",
      "Ep done - 3660.\n",
      "Ep done - 3670.\n",
      "Ep done - 3680.\n",
      "Ep done - 3690.\n",
      "Ep done - 3700.\n",
      "Eval num_timesteps=370000, episode_reward=0.45 +/- 0.89\n",
      "Episode length: 30.17 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.45       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 370000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03478021 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.54      |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00687   |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.193      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.45\n",
      "SELFPLAY: new best model, bumping up generation to 26\n",
      "Ep done - 12260.\n",
      "Ep done - 12270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 147      |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 2511     |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "Ep done - 12280.\n",
      "Ep done - 12290.\n",
      "Ep done - 12300.\n",
      "Ep done - 12310.\n",
      "Ep done - 12320.\n",
      "Ep done - 12330.\n",
      "Ep done - 12340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 2521        |\n",
      "|    total_timesteps      | 372736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026740754 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0283      |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | -0.0384     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 12350.\n",
      "Ep done - 12360.\n",
      "Ep done - 12370.\n",
      "Ep done - 12380.\n",
      "Ep done - 12390.\n",
      "Ep done - 12400.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.24       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 183        |\n",
      "|    time_elapsed         | 2530       |\n",
      "|    total_timesteps      | 374784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02798468 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.556     |\n",
      "|    explained_variance   | 0.313      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0277     |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Ep done - 12410.\n",
      "Ep done - 12420.\n",
      "Ep done - 12430.\n",
      "Ep done - 12440.\n",
      "Ep done - 12450.\n",
      "Ep done - 12460.\n",
      "Ep done - 12470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 2539        |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027230544 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.19        |\n",
      "-----------------------------------------\n",
      "Ep done - 12480.\n",
      "Ep done - 12490.\n",
      "Ep done - 12500.\n",
      "Ep done - 12510.\n",
      "Ep done - 12520.\n",
      "Ep done - 12530.\n",
      "Ep done - 12540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.16       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 185        |\n",
      "|    time_elapsed         | 2549       |\n",
      "|    total_timesteps      | 378880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03020607 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 0.33       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.045      |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Ep done - 12550.\n",
      "Ep done - 12560.\n",
      "Ep done - 12570.\n",
      "Ep done - 12580.\n",
      "Ep done - 3710.\n",
      "Ep done - 3720.\n",
      "Ep done - 3730.\n",
      "Ep done - 3740.\n",
      "Ep done - 3750.\n",
      "Ep done - 3760.\n",
      "Ep done - 3770.\n",
      "Ep done - 3780.\n",
      "Ep done - 3790.\n",
      "Ep done - 3800.\n",
      "Eval num_timesteps=380000, episode_reward=0.21 +/- 0.96\n",
      "Episode length: 30.15 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.21        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 380000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027891265 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00608    |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.21\n",
      "SELFPLAY: new best model, bumping up generation to 27\n",
      "Ep done - 12590.\n",
      "Ep done - 12600.\n",
      "Ep done - 12610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.28     |\n",
      "| time/              |          |\n",
      "|    fps             | 148      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 2567     |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Ep done - 12620.\n",
      "Ep done - 12630.\n",
      "Ep done - 12640.\n",
      "Ep done - 12650.\n",
      "Ep done - 12660.\n",
      "Ep done - 12670.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 148         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 2577        |\n",
      "|    total_timesteps      | 382976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027827097 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 12680.\n",
      "Ep done - 12690.\n",
      "Ep done - 12700.\n",
      "Ep done - 12710.\n",
      "Ep done - 12720.\n",
      "Ep done - 12730.\n",
      "Ep done - 12740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.25       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 148        |\n",
      "|    iterations           | 188        |\n",
      "|    time_elapsed         | 2586       |\n",
      "|    total_timesteps      | 385024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03526959 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.574     |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0158     |\n",
      "|    n_updates            | 1870       |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Ep done - 12750.\n",
      "Ep done - 12760.\n",
      "Ep done - 12770.\n",
      "Ep done - 12780.\n",
      "Ep done - 12790.\n",
      "Ep done - 12800.\n",
      "Ep done - 12810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.31        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 2596        |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028723767 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0308      |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Ep done - 12820.\n",
      "Ep done - 12830.\n",
      "Ep done - 12840.\n",
      "Ep done - 12850.\n",
      "Ep done - 12860.\n",
      "Ep done - 12870.\n",
      "Ep done - 12880.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 190        |\n",
      "|    time_elapsed         | 2605       |\n",
      "|    total_timesteps      | 389120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03327217 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.531     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0167     |\n",
      "|    n_updates            | 1890       |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 12890.\n",
      "Ep done - 12900.\n",
      "Ep done - 12910.\n",
      "Ep done - 3810.\n",
      "Ep done - 3820.\n",
      "Ep done - 3830.\n",
      "Ep done - 3840.\n",
      "Ep done - 3850.\n",
      "Ep done - 3860.\n",
      "Ep done - 3870.\n",
      "Ep done - 3880.\n",
      "Ep done - 3890.\n",
      "Ep done - 3900.\n",
      "Eval num_timesteps=390000, episode_reward=0.33 +/- 0.92\n",
      "Episode length: 30.28 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 390000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035332683 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00321    |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.189       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.33\n",
      "SELFPLAY: new best model, bumping up generation to 28\n",
      "Ep done - 12920.\n",
      "Ep done - 12930.\n",
      "Ep done - 12940.\n",
      "Ep done - 12950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 2623     |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "Ep done - 12960.\n",
      "Ep done - 12970.\n",
      "Ep done - 12980.\n",
      "Ep done - 12990.\n",
      "Ep done - 13000.\n",
      "Ep done - 13010.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 192        |\n",
      "|    time_elapsed         | 2632       |\n",
      "|    total_timesteps      | 393216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03078197 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.54      |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 1910       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 13020.\n",
      "Ep done - 13030.\n",
      "Ep done - 13040.\n",
      "Ep done - 13050.\n",
      "Ep done - 13060.\n",
      "Ep done - 13070.\n",
      "Ep done - 13080.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 193        |\n",
      "|    time_elapsed         | 2641       |\n",
      "|    total_timesteps      | 395264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03271904 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.518     |\n",
      "|    explained_variance   | 0.304      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "Ep done - 13090.\n",
      "Ep done - 13100.\n",
      "Ep done - 13110.\n",
      "Ep done - 13120.\n",
      "Ep done - 13130.\n",
      "Ep done - 13140.\n",
      "Ep done - 13150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.3         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 149         |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 2651        |\n",
      "|    total_timesteps      | 397312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031220362 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0283      |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 13160.\n",
      "Ep done - 13170.\n",
      "Ep done - 13180.\n",
      "Ep done - 13190.\n",
      "Ep done - 13200.\n",
      "Ep done - 13210.\n",
      "Ep done - 13220.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.9        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 2660        |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029562915 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00986     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0359     |\n",
      "|    value_loss           | 0.191       |\n",
      "-----------------------------------------\n",
      "Ep done - 13230.\n",
      "Ep done - 13240.\n",
      "Ep done - 3910.\n",
      "Ep done - 3920.\n",
      "Ep done - 3930.\n",
      "Ep done - 3940.\n",
      "Ep done - 3950.\n",
      "Ep done - 3960.\n",
      "Ep done - 3970.\n",
      "Ep done - 3980.\n",
      "Ep done - 3990.\n",
      "Ep done - 4000.\n",
      "Eval num_timesteps=400000, episode_reward=0.34 +/- 0.92\n",
      "Episode length: 30.26 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033409286 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00664    |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.34\n",
      "SELFPLAY: new best model, bumping up generation to 29\n",
      "Ep done - 13250.\n",
      "Ep done - 13260.\n",
      "Ep done - 13270.\n",
      "Ep done - 13280.\n",
      "Ep done - 13290.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 149      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 2679     |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "Ep done - 13300.\n",
      "Ep done - 13310.\n",
      "Ep done - 13320.\n",
      "Ep done - 13330.\n",
      "Ep done - 13340.\n",
      "Ep done - 13350.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 2688       |\n",
      "|    total_timesteps      | 403456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02909583 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.494     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0142     |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "Ep done - 13360.\n",
      "Ep done - 13370.\n",
      "Ep done - 13380.\n",
      "Ep done - 13390.\n",
      "Ep done - 13400.\n",
      "Ep done - 13410.\n",
      "Ep done - 13420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.23        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 2698        |\n",
      "|    total_timesteps      | 405504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032460015 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    value_loss           | 0.192       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 13430.\n",
      "Ep done - 13440.\n",
      "Ep done - 13450.\n",
      "Ep done - 13460.\n",
      "Ep done - 13470.\n",
      "Ep done - 13480.\n",
      "Ep done - 13490.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 199        |\n",
      "|    time_elapsed         | 2707       |\n",
      "|    total_timesteps      | 407552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02726974 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.486     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0547     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Ep done - 13500.\n",
      "Ep done - 13510.\n",
      "Ep done - 13520.\n",
      "Ep done - 13530.\n",
      "Ep done - 13540.\n",
      "Ep done - 13550.\n",
      "Ep done - 13560.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.29       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 200        |\n",
      "|    time_elapsed         | 2716       |\n",
      "|    total_timesteps      | 409600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02953666 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.516     |\n",
      "|    explained_variance   | 0.267      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 1990       |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "Ep done - 13570.\n",
      "Ep done - 4010.\n",
      "Ep done - 4020.\n",
      "Ep done - 4030.\n",
      "Ep done - 4040.\n",
      "Ep done - 4050.\n",
      "Ep done - 4060.\n",
      "Ep done - 4070.\n",
      "Ep done - 4080.\n",
      "Ep done - 4090.\n",
      "Ep done - 4100.\n",
      "Eval num_timesteps=410000, episode_reward=0.44 +/- 0.90\n",
      "Episode length: 30.25 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 410000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03155674 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.499     |\n",
      "|    explained_variance   | 0.345      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0284     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 30\n",
      "Ep done - 13580.\n",
      "Ep done - 13590.\n",
      "Ep done - 13600.\n",
      "Ep done - 13610.\n",
      "Ep done - 13620.\n",
      "Ep done - 13630.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 150      |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 2735     |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Ep done - 13640.\n",
      "Ep done - 13650.\n",
      "Ep done - 13660.\n",
      "Ep done - 13670.\n",
      "Ep done - 13680.\n",
      "Ep done - 13690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 2744        |\n",
      "|    total_timesteps      | 413696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034746684 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Ep done - 13700.\n",
      "Ep done - 13710.\n",
      "Ep done - 13720.\n",
      "Ep done - 13730.\n",
      "Ep done - 13740.\n",
      "Ep done - 13750.\n",
      "Ep done - 13760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.18        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 203         |\n",
      "|    time_elapsed         | 2753        |\n",
      "|    total_timesteps      | 415744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033659205 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00345     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    value_loss           | 0.197       |\n",
      "-----------------------------------------\n",
      "Ep done - 13770.\n",
      "Ep done - 13780.\n",
      "Ep done - 13790.\n",
      "Ep done - 13800.\n",
      "Ep done - 13810.\n",
      "Ep done - 13820.\n",
      "Ep done - 13830.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.21       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 204        |\n",
      "|    time_elapsed         | 2762       |\n",
      "|    total_timesteps      | 417792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03182043 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.465     |\n",
      "|    explained_variance   | 0.203      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 2030       |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "Ep done - 13840.\n",
      "Ep done - 13850.\n",
      "Ep done - 13860.\n",
      "Ep done - 13870.\n",
      "Ep done - 13880.\n",
      "Ep done - 13890.\n",
      "Ep done - 13900.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 205        |\n",
      "|    time_elapsed         | 2772       |\n",
      "|    total_timesteps      | 419840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04338478 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.485     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0481     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "Ep done - 4110.\n",
      "Ep done - 4120.\n",
      "Ep done - 4130.\n",
      "Ep done - 4140.\n",
      "Ep done - 4150.\n",
      "Ep done - 4160.\n",
      "Ep done - 4170.\n",
      "Ep done - 4180.\n",
      "Ep done - 4190.\n",
      "Ep done - 4200.\n",
      "Eval num_timesteps=420000, episode_reward=0.47 +/- 0.88\n",
      "Episode length: 30.23 +/- 0.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 420000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03596125 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.26       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00931   |\n",
      "|    n_updates            | 2050       |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 31\n",
      "Ep done - 13910.\n",
      "Ep done - 13920.\n",
      "Ep done - 13930.\n",
      "Ep done - 13940.\n",
      "Ep done - 13950.\n",
      "Ep done - 13960.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.3      |\n",
      "| time/              |          |\n",
      "|    fps             | 151      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 2790     |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Ep done - 13970.\n",
      "Ep done - 13980.\n",
      "Ep done - 13990.\n",
      "Ep done - 14000.\n",
      "Ep done - 14010.\n",
      "Ep done - 14020.\n",
      "Ep done - 14030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.11        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 151         |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 2800        |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039321594 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Ep done - 14040.\n",
      "Ep done - 14050.\n",
      "Ep done - 14060.\n",
      "Ep done - 14070.\n",
      "Ep done - 14080.\n",
      "Ep done - 14090.\n",
      "Ep done - 14100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | -0.01      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 208        |\n",
      "|    time_elapsed         | 2809       |\n",
      "|    total_timesteps      | 425984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03208056 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.448     |\n",
      "|    explained_variance   | 0.335      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0157     |\n",
      "|    n_updates            | 2070       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "Ep done - 14110.\n",
      "Ep done - 14120.\n",
      "Ep done - 14130.\n",
      "Ep done - 14140.\n",
      "Ep done - 14150.\n",
      "Ep done - 14160.\n",
      "Ep done - 14170.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.3        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 209        |\n",
      "|    time_elapsed         | 2818       |\n",
      "|    total_timesteps      | 428032     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03070966 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.446     |\n",
      "|    explained_variance   | 0.17       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0347     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "Ep done - 14180.\n",
      "Ep done - 14190.\n",
      "Ep done - 14200.\n",
      "Ep done - 14210.\n",
      "Ep done - 14220.\n",
      "Ep done - 14230.\n",
      "Ep done - 4210.\n",
      "Ep done - 4220.\n",
      "Ep done - 4230.\n",
      "Ep done - 4240.\n",
      "Ep done - 4250.\n",
      "Ep done - 4260.\n",
      "Ep done - 4270.\n",
      "Ep done - 4280.\n",
      "Ep done - 4290.\n",
      "Ep done - 4300.\n",
      "Eval num_timesteps=430000, episode_reward=0.37 +/- 0.92\n",
      "Episode length: 30.20 +/- 0.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 430000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031142551 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.0332     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.37\n",
      "SELFPLAY: new best model, bumping up generation to 32\n",
      "Ep done - 14240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 151      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 2837     |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Ep done - 14250.\n",
      "Ep done - 14260.\n",
      "Ep done - 14270.\n",
      "Ep done - 14280.\n",
      "Ep done - 14290.\n",
      "Ep done - 14300.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 151        |\n",
      "|    iterations           | 211        |\n",
      "|    time_elapsed         | 2846       |\n",
      "|    total_timesteps      | 432128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03320412 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.446     |\n",
      "|    explained_variance   | 0.262      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0508     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    value_loss           | 0.193      |\n",
      "----------------------------------------\n",
      "Ep done - 14310.\n",
      "Ep done - 14320.\n",
      "Ep done - 14330.\n",
      "Ep done - 14340.\n",
      "Ep done - 14350.\n",
      "Ep done - 14360.\n",
      "Ep done - 14370.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.33      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 152       |\n",
      "|    iterations           | 212       |\n",
      "|    time_elapsed         | 2856      |\n",
      "|    total_timesteps      | 434176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0351332 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.438    |\n",
      "|    explained_variance   | 0.233     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0163    |\n",
      "|    n_updates            | 2110      |\n",
      "|    policy_gradient_loss | -0.0361   |\n",
      "|    value_loss           | 0.173     |\n",
      "---------------------------------------\n",
      "Ep done - 14380.\n",
      "Ep done - 14390.\n",
      "Ep done - 14400.\n",
      "Ep done - 14410.\n",
      "Ep done - 14420.\n",
      "Ep done - 14430.\n",
      "Ep done - 14440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 213         |\n",
      "|    time_elapsed         | 2865        |\n",
      "|    total_timesteps      | 436224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036650956 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0778      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 14450.\n",
      "Ep done - 14460.\n",
      "Ep done - 14470.\n",
      "Ep done - 14480.\n",
      "Ep done - 14490.\n",
      "Ep done - 14500.\n",
      "Ep done - 14510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 2874        |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028962862 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 14520.\n",
      "Ep done - 14530.\n",
      "Ep done - 14540.\n",
      "Ep done - 14550.\n",
      "Ep done - 14560.\n",
      "Ep done - 4310.\n",
      "Ep done - 4320.\n",
      "Ep done - 4330.\n",
      "Ep done - 4340.\n",
      "Ep done - 4350.\n",
      "Ep done - 4360.\n",
      "Ep done - 4370.\n",
      "Ep done - 4380.\n",
      "Ep done - 4390.\n",
      "Ep done - 4400.\n",
      "Eval num_timesteps=440000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.35 +/- 0.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.4       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 440000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03078529 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.439     |\n",
      "|    explained_variance   | 0.214      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0079    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 33\n",
      "Ep done - 14570.\n",
      "Ep done - 14580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.35     |\n",
      "| time/              |          |\n",
      "|    fps             | 152      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 2893     |\n",
      "|    total_timesteps | 440320   |\n",
      "---------------------------------\n",
      "Ep done - 14590.\n",
      "Ep done - 14600.\n",
      "Ep done - 14610.\n",
      "Ep done - 14620.\n",
      "Ep done - 14630.\n",
      "Ep done - 14640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 2902        |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035638377 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 14650.\n",
      "Ep done - 14660.\n",
      "Ep done - 14670.\n",
      "Ep done - 14680.\n",
      "Ep done - 14690.\n",
      "Ep done - 14700.\n",
      "Ep done - 14710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.33       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 152        |\n",
      "|    iterations           | 217        |\n",
      "|    time_elapsed         | 2911       |\n",
      "|    total_timesteps      | 444416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03051756 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.433     |\n",
      "|    explained_variance   | 0.224      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0299     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    value_loss           | 0.192      |\n",
      "----------------------------------------\n",
      "Ep done - 14720.\n",
      "Ep done - 14730.\n",
      "Ep done - 14740.\n",
      "Ep done - 14750.\n",
      "Ep done - 14760.\n",
      "Ep done - 14770.\n",
      "Ep done - 14780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.29        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 2921        |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039860338 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Ep done - 14790.\n",
      "Ep done - 14800.\n",
      "Ep done - 14810.\n",
      "Ep done - 14820.\n",
      "Ep done - 14830.\n",
      "Ep done - 14840.\n",
      "Ep done - 14850.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 219        |\n",
      "|    time_elapsed         | 2930       |\n",
      "|    total_timesteps      | 448512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03557862 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.456     |\n",
      "|    explained_variance   | 0.271      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.054      |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "Ep done - 14860.\n",
      "Ep done - 14870.\n",
      "Ep done - 14880.\n",
      "Ep done - 14890.\n",
      "Ep done - 14900.\n",
      "Ep done - 4410.\n",
      "Ep done - 4420.\n",
      "Ep done - 4430.\n",
      "Ep done - 4440.\n",
      "Ep done - 4450.\n",
      "Ep done - 4460.\n",
      "Ep done - 4470.\n",
      "Ep done - 4480.\n",
      "Ep done - 4490.\n",
      "Ep done - 4500.\n",
      "Eval num_timesteps=450000, episode_reward=0.41 +/- 0.91\n",
      "Episode length: 30.22 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032477036 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.41\n",
      "SELFPLAY: new best model, bumping up generation to 34\n",
      "Ep done - 14910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 152      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 2949     |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Ep done - 14920.\n",
      "Ep done - 14930.\n",
      "Ep done - 14940.\n",
      "Ep done - 14950.\n",
      "Ep done - 14960.\n",
      "Ep done - 14970.\n",
      "Ep done - 14980.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 2958        |\n",
      "|    total_timesteps      | 452608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029306704 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Ep done - 14990.\n",
      "Ep done - 15000.\n",
      "Ep done - 15010.\n",
      "Ep done - 15020.\n",
      "Ep done - 15030.\n",
      "Ep done - 15040.\n",
      "Ep done - 15050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 2967        |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030226156 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.451      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00453     |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 15060.\n",
      "Ep done - 15070.\n",
      "Ep done - 15080.\n",
      "Ep done - 15090.\n",
      "Ep done - 15100.\n",
      "Ep done - 15110.\n",
      "Ep done - 15120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 223         |\n",
      "|    time_elapsed         | 2976        |\n",
      "|    total_timesteps      | 456704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030168477 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00653    |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0351     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 15130.\n",
      "Ep done - 15140.\n",
      "Ep done - 15150.\n",
      "Ep done - 15160.\n",
      "Ep done - 15170.\n",
      "Ep done - 15180.\n",
      "Ep done - 15190.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 224        |\n",
      "|    time_elapsed         | 2985       |\n",
      "|    total_timesteps      | 458752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03224107 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.441     |\n",
      "|    explained_variance   | 0.113      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 2230       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n",
      "Ep done - 15200.\n",
      "Ep done - 15210.\n",
      "Ep done - 15220.\n",
      "Ep done - 15230.\n",
      "Ep done - 4510.\n",
      "Ep done - 4520.\n",
      "Ep done - 4530.\n",
      "Ep done - 4540.\n",
      "Ep done - 4550.\n",
      "Ep done - 4560.\n",
      "Ep done - 4570.\n",
      "Ep done - 4580.\n",
      "Ep done - 4590.\n",
      "Ep done - 4600.\n",
      "Eval num_timesteps=460000, episode_reward=0.37 +/- 0.92\n",
      "Episode length: 30.18 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.37       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 460000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03145272 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.44      |\n",
      "|    explained_variance   | 0.243      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0282     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.37\n",
      "SELFPLAY: new best model, bumping up generation to 35\n",
      "Ep done - 15240.\n",
      "Ep done - 15250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 3004     |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Ep done - 15260.\n",
      "Ep done - 15270.\n",
      "Ep done - 15280.\n",
      "Ep done - 15290.\n",
      "Ep done - 15300.\n",
      "Ep done - 15310.\n",
      "Ep done - 15320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 3013        |\n",
      "|    total_timesteps      | 462848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031152528 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0255      |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 15330.\n",
      "Ep done - 15340.\n",
      "Ep done - 15350.\n",
      "Ep done - 15360.\n",
      "Ep done - 15370.\n",
      "Ep done - 15380.\n",
      "Ep done - 15390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 3023        |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026006246 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0305     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Ep done - 15400.\n",
      "Ep done - 15410.\n",
      "Ep done - 15420.\n",
      "Ep done - 15430.\n",
      "Ep done - 15440.\n",
      "Ep done - 15450.\n",
      "Ep done - 15460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 228        |\n",
      "|    time_elapsed         | 3032       |\n",
      "|    total_timesteps      | 466944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03668668 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.46      |\n",
      "|    explained_variance   | 0.223      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00559    |\n",
      "|    n_updates            | 2270       |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    value_loss           | 0.202      |\n",
      "----------------------------------------\n",
      "Ep done - 15470.\n",
      "Ep done - 15480.\n",
      "Ep done - 15490.\n",
      "Ep done - 15500.\n",
      "Ep done - 15510.\n",
      "Ep done - 15520.\n",
      "Ep done - 15530.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.26       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 229        |\n",
      "|    time_elapsed         | 3041       |\n",
      "|    total_timesteps      | 468992     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03479203 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.446     |\n",
      "|    explained_variance   | 0.34       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "Ep done - 15540.\n",
      "Ep done - 15550.\n",
      "Ep done - 15560.\n",
      "Ep done - 4610.\n",
      "Ep done - 4620.\n",
      "Ep done - 4630.\n",
      "Ep done - 4640.\n",
      "Ep done - 4650.\n",
      "Ep done - 4660.\n",
      "Ep done - 4670.\n",
      "Ep done - 4680.\n",
      "Ep done - 4690.\n",
      "Ep done - 4700.\n",
      "Eval num_timesteps=470000, episode_reward=0.28 +/- 0.93\n",
      "Episode length: 30.12 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.28        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 470000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032152504 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0428      |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    value_loss           | 0.211       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.28\n",
      "SELFPLAY: new best model, bumping up generation to 36\n",
      "Ep done - 15570.\n",
      "Ep done - 15580.\n",
      "Ep done - 15590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.31     |\n",
      "| time/              |          |\n",
      "|    fps             | 153      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 3060     |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Ep done - 15600.\n",
      "Ep done - 15610.\n",
      "Ep done - 15620.\n",
      "Ep done - 15630.\n",
      "Ep done - 15640.\n",
      "Ep done - 15650.\n",
      "Ep done - 15660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 3069        |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028639298 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0341     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "Ep done - 15670.\n",
      "Ep done - 15680.\n",
      "Ep done - 15690.\n",
      "Ep done - 15700.\n",
      "Ep done - 15710.\n",
      "Ep done - 15720.\n",
      "Ep done - 15730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 3078        |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031163681 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00954     |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 15740.\n",
      "Ep done - 15750.\n",
      "Ep done - 15760.\n",
      "Ep done - 15770.\n",
      "Ep done - 15780.\n",
      "Ep done - 15790.\n",
      "Ep done - 15800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 154         |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 3088        |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028831802 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00504     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 15810.\n",
      "Ep done - 15820.\n",
      "Ep done - 15830.\n",
      "Ep done - 15840.\n",
      "Ep done - 15850.\n",
      "Ep done - 15860.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 234        |\n",
      "|    time_elapsed         | 3097       |\n",
      "|    total_timesteps      | 479232     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03057297 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.48      |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 2330       |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Ep done - 15870.\n",
      "Ep done - 15880.\n",
      "Ep done - 15890.\n",
      "Ep done - 4710.\n",
      "Ep done - 4720.\n",
      "Ep done - 4730.\n",
      "Ep done - 4740.\n",
      "Ep done - 4750.\n",
      "Ep done - 4760.\n",
      "Ep done - 4770.\n",
      "Ep done - 4780.\n",
      "Ep done - 4790.\n",
      "Ep done - 4800.\n",
      "Eval num_timesteps=480000, episode_reward=0.36 +/- 0.92\n",
      "Episode length: 30.08 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.36       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 480000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02802231 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | 0.253      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00293    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.36\n",
      "SELFPLAY: new best model, bumping up generation to 37\n",
      "Ep done - 15900.\n",
      "Ep done - 15910.\n",
      "Ep done - 15920.\n",
      "Ep done - 15930.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.42     |\n",
      "| time/              |          |\n",
      "|    fps             | 154      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 3115     |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Ep done - 15940.\n",
      "Ep done - 15950.\n",
      "Ep done - 15960.\n",
      "Ep done - 15970.\n",
      "Ep done - 15980.\n",
      "Ep done - 15990.\n",
      "Ep done - 16000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 236        |\n",
      "|    time_elapsed         | 3124       |\n",
      "|    total_timesteps      | 483328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03941874 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.254      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 2350       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Ep done - 16010.\n",
      "Ep done - 16020.\n",
      "Ep done - 16030.\n",
      "Ep done - 16040.\n",
      "Ep done - 16050.\n",
      "Ep done - 16060.\n",
      "Ep done - 16070.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.42      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 154       |\n",
      "|    iterations           | 237       |\n",
      "|    time_elapsed         | 3134      |\n",
      "|    total_timesteps      | 485376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0317704 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.464    |\n",
      "|    explained_variance   | 0.114     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0166    |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -0.0347   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Ep done - 16080.\n",
      "Ep done - 16090.\n",
      "Ep done - 16100.\n",
      "Ep done - 16110.\n",
      "Ep done - 16120.\n",
      "Ep done - 16130.\n",
      "Ep done - 16140.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 3143        |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029776664 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 16150.\n",
      "Ep done - 16160.\n",
      "Ep done - 16170.\n",
      "Ep done - 16180.\n",
      "Ep done - 16190.\n",
      "Ep done - 16200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 3152        |\n",
      "|    total_timesteps      | 489472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029371463 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "Ep done - 16210.\n",
      "Ep done - 16220.\n",
      "Ep done - 4810.\n",
      "Ep done - 4820.\n",
      "Ep done - 4830.\n",
      "Ep done - 4840.\n",
      "Ep done - 4850.\n",
      "Ep done - 4860.\n",
      "Ep done - 4870.\n",
      "Ep done - 4880.\n",
      "Ep done - 4890.\n",
      "Ep done - 4900.\n",
      "Eval num_timesteps=490000, episode_reward=0.44 +/- 0.88\n",
      "Episode length: 30.15 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 490000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03587226 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.455     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0112     |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 38\n",
      "Ep done - 16230.\n",
      "Ep done - 16240.\n",
      "Ep done - 16250.\n",
      "Ep done - 16260.\n",
      "Ep done - 16270.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 154      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 3171     |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Ep done - 16280.\n",
      "Ep done - 16290.\n",
      "Ep done - 16300.\n",
      "Ep done - 16310.\n",
      "Ep done - 16320.\n",
      "Ep done - 16330.\n",
      "Ep done - 16340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 3180        |\n",
      "|    total_timesteps      | 493568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029427323 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 16350.\n",
      "Ep done - 16360.\n",
      "Ep done - 16370.\n",
      "Ep done - 16380.\n",
      "Ep done - 16390.\n",
      "Ep done - 16400.\n",
      "Ep done - 16410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 3189        |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033737183 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0198      |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Ep done - 16420.\n",
      "Ep done - 16430.\n",
      "Ep done - 16440.\n",
      "Ep done - 16450.\n",
      "Ep done - 16460.\n",
      "Ep done - 16470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 3199        |\n",
      "|    total_timesteps      | 497664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035282496 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.0689      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00894    |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0303     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 16480.\n",
      "Ep done - 16490.\n",
      "Ep done - 16500.\n",
      "Ep done - 16510.\n",
      "Ep done - 16520.\n",
      "Ep done - 16530.\n",
      "Ep done - 16540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 3208       |\n",
      "|    total_timesteps      | 499712     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03436216 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.419     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Ep done - 16550.\n",
      "Ep done - 4910.\n",
      "Ep done - 4920.\n",
      "Ep done - 4930.\n",
      "Ep done - 4940.\n",
      "Ep done - 4950.\n",
      "Ep done - 4960.\n",
      "Ep done - 4970.\n",
      "Ep done - 4980.\n",
      "Ep done - 4990.\n",
      "Ep done - 5000.\n",
      "Eval num_timesteps=500000, episode_reward=0.50 +/- 0.84\n",
      "Episode length: 30.17 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033504046 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 39\n",
      "Ep done - 16560.\n",
      "Ep done - 16570.\n",
      "Ep done - 16580.\n",
      "Ep done - 16590.\n",
      "Ep done - 16600.\n",
      "Ep done - 16610.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 3226     |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "Ep done - 16620.\n",
      "Ep done - 16630.\n",
      "Ep done - 16640.\n",
      "Ep done - 16650.\n",
      "Ep done - 16660.\n",
      "Ep done - 16670.\n",
      "Ep done - 16680.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 3235        |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036805235 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 16690.\n",
      "Ep done - 16700.\n",
      "Ep done - 16710.\n",
      "Ep done - 16720.\n",
      "Ep done - 16730.\n",
      "Ep done - 16740.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 3244        |\n",
      "|    total_timesteps      | 505856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034850243 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0331      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Ep done - 16750.\n",
      "Ep done - 16760.\n",
      "Ep done - 16770.\n",
      "Ep done - 16780.\n",
      "Ep done - 16790.\n",
      "Ep done - 16800.\n",
      "Ep done - 16810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 3254        |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035851773 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Ep done - 16820.\n",
      "Ep done - 16830.\n",
      "Ep done - 16840.\n",
      "Ep done - 16850.\n",
      "Ep done - 16860.\n",
      "Ep done - 16870.\n",
      "Ep done - 16880.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 249         |\n",
      "|    time_elapsed         | 3265        |\n",
      "|    total_timesteps      | 509952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028946381 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00951    |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 0.0969      |\n",
      "-----------------------------------------\n",
      "Ep done - 5010.\n",
      "Ep done - 5020.\n",
      "Ep done - 5030.\n",
      "Ep done - 5040.\n",
      "Ep done - 5050.\n",
      "Ep done - 5060.\n",
      "Ep done - 5070.\n",
      "Ep done - 5080.\n",
      "Ep done - 5090.\n",
      "Ep done - 5100.\n",
      "Eval num_timesteps=510000, episode_reward=0.68 +/- 0.69\n",
      "Episode length: 30.09 +/- 0.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 510000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03963881 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00725    |\n",
      "|    n_updates            | 2490       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    value_loss           | 0.149      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 40\n",
      "Ep done - 16890.\n",
      "Ep done - 16900.\n",
      "Ep done - 16910.\n",
      "Ep done - 16920.\n",
      "Ep done - 16930.\n",
      "Ep done - 16940.\n",
      "Ep done - 16950.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 155      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 3283     |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Ep done - 16960.\n",
      "Ep done - 16970.\n",
      "Ep done - 16980.\n",
      "Ep done - 16990.\n",
      "Ep done - 17000.\n",
      "Ep done - 17010.\n",
      "Ep done - 17020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 3292        |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035798624 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0323     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "Ep done - 17030.\n",
      "Ep done - 17040.\n",
      "Ep done - 17050.\n",
      "Ep done - 17060.\n",
      "Ep done - 17070.\n",
      "Ep done - 17080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 252         |\n",
      "|    time_elapsed         | 3301        |\n",
      "|    total_timesteps      | 516096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028365863 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 17090.\n",
      "Ep done - 17100.\n",
      "Ep done - 17110.\n",
      "Ep done - 17120.\n",
      "Ep done - 17130.\n",
      "Ep done - 17140.\n",
      "Ep done - 17150.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 156        |\n",
      "|    iterations           | 253        |\n",
      "|    time_elapsed         | 3311       |\n",
      "|    total_timesteps      | 518144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03454088 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.379     |\n",
      "|    explained_variance   | 0.0935     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00941    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 17160.\n",
      "Ep done - 17170.\n",
      "Ep done - 17180.\n",
      "Ep done - 17190.\n",
      "Ep done - 17200.\n",
      "Ep done - 17210.\n",
      "Ep done - 5110.\n",
      "Ep done - 5120.\n",
      "Ep done - 5130.\n",
      "Ep done - 5140.\n",
      "Ep done - 5150.\n",
      "Ep done - 5160.\n",
      "Ep done - 5170.\n",
      "Ep done - 5180.\n",
      "Ep done - 5190.\n",
      "Ep done - 5200.\n",
      "Eval num_timesteps=520000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.28 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 520000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029928576 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00441     |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.0853      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 41\n",
      "Ep done - 17220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 3329     |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Ep done - 17230.\n",
      "Ep done - 17240.\n",
      "Ep done - 17250.\n",
      "Ep done - 17260.\n",
      "Ep done - 17270.\n",
      "Ep done - 17280.\n",
      "Ep done - 17290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 3338        |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040780142 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0172      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Ep done - 17300.\n",
      "Ep done - 17310.\n",
      "Ep done - 17320.\n",
      "Ep done - 17330.\n",
      "Ep done - 17340.\n",
      "Ep done - 17350.\n",
      "Ep done - 17360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 3347        |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034862045 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    value_loss           | 0.0908      |\n",
      "-----------------------------------------\n",
      "Ep done - 17370.\n",
      "Ep done - 17380.\n",
      "Ep done - 17390.\n",
      "Ep done - 17400.\n",
      "Ep done - 17410.\n",
      "Ep done - 17420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 257         |\n",
      "|    time_elapsed         | 3356        |\n",
      "|    total_timesteps      | 526336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031635195 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0015     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.103       |\n",
      "-----------------------------------------\n",
      "Ep done - 17430.\n",
      "Ep done - 17440.\n",
      "Ep done - 17450.\n",
      "Ep done - 17460.\n",
      "Ep done - 17470.\n",
      "Ep done - 17480.\n",
      "Ep done - 17490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 258         |\n",
      "|    time_elapsed         | 3365        |\n",
      "|    total_timesteps      | 528384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035097376 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.0725      |\n",
      "-----------------------------------------\n",
      "Ep done - 17500.\n",
      "Ep done - 17510.\n",
      "Ep done - 17520.\n",
      "Ep done - 17530.\n",
      "Ep done - 17540.\n",
      "Ep done - 5210.\n",
      "Ep done - 5220.\n",
      "Ep done - 5230.\n",
      "Ep done - 5240.\n",
      "Ep done - 5250.\n",
      "Ep done - 5260.\n",
      "Ep done - 5270.\n",
      "Ep done - 5280.\n",
      "Ep done - 5290.\n",
      "Ep done - 5300.\n",
      "Eval num_timesteps=530000, episode_reward=0.76 +/- 0.62\n",
      "Episode length: 30.21 +/- 0.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030209864 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.0584      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 42\n",
      "Ep done - 17550.\n",
      "Ep done - 17560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.87     |\n",
      "| time/              |          |\n",
      "|    fps             | 156      |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 3384     |\n",
      "|    total_timesteps | 530432   |\n",
      "---------------------------------\n",
      "Ep done - 17570.\n",
      "Ep done - 17580.\n",
      "Ep done - 17590.\n",
      "Ep done - 17600.\n",
      "Ep done - 17610.\n",
      "Ep done - 17620.\n",
      "Ep done - 17630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 156         |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 3393        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029201604 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | -0.249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00442    |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "Ep done - 17640.\n",
      "Ep done - 17650.\n",
      "Ep done - 17660.\n",
      "Ep done - 17670.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 17680.\n",
      "Ep done - 17690.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 261        |\n",
      "|    time_elapsed         | 3403       |\n",
      "|    total_timesteps      | 534528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03239207 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.389     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.0558     |\n",
      "----------------------------------------\n",
      "Ep done - 17700.\n",
      "Ep done - 17710.\n",
      "Ep done - 17720.\n",
      "Ep done - 17730.\n",
      "Ep done - 17740.\n",
      "Ep done - 17750.\n",
      "Ep done - 17760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 262        |\n",
      "|    time_elapsed         | 3412       |\n",
      "|    total_timesteps      | 536576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03447684 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.383     |\n",
      "|    explained_variance   | 0.29       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0236     |\n",
      "|    n_updates            | 2610       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.0914     |\n",
      "----------------------------------------\n",
      "Ep done - 17770.\n",
      "Ep done - 17780.\n",
      "Ep done - 17790.\n",
      "Ep done - 17800.\n",
      "Ep done - 17810.\n",
      "Ep done - 17820.\n",
      "Ep done - 17830.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 263        |\n",
      "|    time_elapsed         | 3421       |\n",
      "|    total_timesteps      | 538624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02810727 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.374     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0092    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 0.0702     |\n",
      "----------------------------------------\n",
      "Ep done - 17840.\n",
      "Ep done - 17850.\n",
      "Ep done - 17860.\n",
      "Ep done - 17870.\n",
      "Ep done - 5310.\n",
      "Ep done - 5320.\n",
      "Ep done - 5330.\n",
      "Ep done - 5340.\n",
      "Ep done - 5350.\n",
      "Ep done - 5360.\n",
      "Ep done - 5370.\n",
      "Ep done - 5380.\n",
      "Ep done - 5390.\n",
      "Ep done - 5400.\n",
      "Eval num_timesteps=540000, episode_reward=0.70 +/- 0.71\n",
      "Episode length: 30.33 +/- 0.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 540000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031228447 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.0737      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 43\n",
      "Ep done - 17880.\n",
      "Ep done - 17890.\n",
      "Ep done - 17900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 3439     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Ep done - 17910.\n",
      "Ep done - 17920.\n",
      "Ep done - 17930.\n",
      "Ep done - 17940.\n",
      "Ep done - 17950.\n",
      "Ep done - 17960.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 3449        |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039229874 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 0.0582      |\n",
      "-----------------------------------------\n",
      "Ep done - 17970.\n",
      "Ep done - 17980.\n",
      "Ep done - 17990.\n",
      "Ep done - 18000.\n",
      "Ep done - 18010.\n",
      "Ep done - 18020.\n",
      "Ep done - 18030.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 266        |\n",
      "|    time_elapsed         | 3458       |\n",
      "|    total_timesteps      | 544768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03240133 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.394     |\n",
      "|    explained_variance   | 0.437      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0213     |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 18040.\n",
      "Ep done - 18050.\n",
      "Ep done - 18060.\n",
      "Ep done - 18070.\n",
      "Ep done - 18080.\n",
      "Ep done - 18090.\n",
      "Ep done - 18100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 157        |\n",
      "|    iterations           | 267        |\n",
      "|    time_elapsed         | 3467       |\n",
      "|    total_timesteps      | 546816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04037878 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.387     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    value_loss           | 0.0968     |\n",
      "----------------------------------------\n",
      "Ep done - 18110.\n",
      "Ep done - 18120.\n",
      "Ep done - 18130.\n",
      "Ep done - 18140.\n",
      "Ep done - 18150.\n",
      "Ep done - 18160.\n",
      "Ep done - 18170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 268         |\n",
      "|    time_elapsed         | 3476        |\n",
      "|    total_timesteps      | 548864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037587367 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 0.0723      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 18180.\n",
      "Ep done - 18190.\n",
      "Ep done - 18200.\n",
      "Ep done - 18210.\n",
      "Ep done - 5410.\n",
      "Ep done - 5420.\n",
      "Ep done - 5430.\n",
      "Ep done - 5440.\n",
      "Ep done - 5450.\n",
      "Ep done - 5460.\n",
      "Ep done - 5470.\n",
      "Ep done - 5480.\n",
      "Ep done - 5490.\n",
      "Ep done - 5500.\n",
      "Eval num_timesteps=550000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.22 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 550000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03591458 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.373     |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.65\n",
      "SELFPLAY: new best model, bumping up generation to 44\n",
      "Ep done - 18220.\n",
      "Ep done - 18230.\n",
      "Ep done - 18240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 3495     |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Ep done - 18250.\n",
      "Ep done - 18260.\n",
      "Ep done - 18270.\n",
      "Ep done - 18280.\n",
      "Ep done - 18290.\n",
      "Ep done - 18300.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 3504        |\n",
      "|    total_timesteps      | 552960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039184228 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.0953      |\n",
      "-----------------------------------------\n",
      "Ep done - 18310.\n",
      "Ep done - 18320.\n",
      "Ep done - 18330.\n",
      "Ep done - 18340.\n",
      "Ep done - 18350.\n",
      "Ep done - 18360.\n",
      "Ep done - 18370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 157         |\n",
      "|    iterations           | 271         |\n",
      "|    time_elapsed         | 3513        |\n",
      "|    total_timesteps      | 555008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040366262 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 18380.\n",
      "Ep done - 18390.\n",
      "Ep done - 18400.\n",
      "Ep done - 18410.\n",
      "Ep done - 18420.\n",
      "Ep done - 18430.\n",
      "Ep done - 18440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 3522        |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037545413 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "Ep done - 18450.\n",
      "Ep done - 18460.\n",
      "Ep done - 18470.\n",
      "Ep done - 18480.\n",
      "Ep done - 18490.\n",
      "Ep done - 18500.\n",
      "Ep done - 18510.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 273        |\n",
      "|    time_elapsed         | 3531       |\n",
      "|    total_timesteps      | 559104     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04759322 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.371     |\n",
      "|    explained_variance   | 0.328      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "Ep done - 18520.\n",
      "Ep done - 18530.\n",
      "Ep done - 18540.\n",
      "Ep done - 5510.\n",
      "Ep done - 5520.\n",
      "Ep done - 5530.\n",
      "Ep done - 5540.\n",
      "Ep done - 5550.\n",
      "Ep done - 5560.\n",
      "Ep done - 5570.\n",
      "Ep done - 5580.\n",
      "Ep done - 5590.\n",
      "Ep done - 5600.\n",
      "Eval num_timesteps=560000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.23 +/- 0.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 560000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03629961 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.368     |\n",
      "|    explained_variance   | 0.465      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 2730       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.65\n",
      "SELFPLAY: new best model, bumping up generation to 45\n",
      "Ep done - 18550.\n",
      "Ep done - 18560.\n",
      "Ep done - 18570.\n",
      "Ep done - 18580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.65     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 3550     |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Ep done - 18590.\n",
      "Ep done - 18600.\n",
      "Ep done - 18610.\n",
      "Ep done - 18620.\n",
      "Ep done - 18630.\n",
      "Ep done - 18640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 3559        |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043360487 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0142     |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.0961      |\n",
      "-----------------------------------------\n",
      "Ep done - 18650.\n",
      "Ep done - 18660.\n",
      "Ep done - 18670.\n",
      "Ep done - 18680.\n",
      "Ep done - 18690.\n",
      "Ep done - 18700.\n",
      "Ep done - 18710.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 276         |\n",
      "|    time_elapsed         | 3568        |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039779715 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    value_loss           | 0.0944      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 18720.\n",
      "Ep done - 18730.\n",
      "Ep done - 18740.\n",
      "Ep done - 18750.\n",
      "Ep done - 18760.\n",
      "Ep done - 18770.\n",
      "Ep done - 18780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 277         |\n",
      "|    time_elapsed         | 3577        |\n",
      "|    total_timesteps      | 567296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032685764 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.357      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00193     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    value_loss           | 0.0791      |\n",
      "-----------------------------------------\n",
      "Ep done - 18790.\n",
      "Ep done - 18800.\n",
      "Ep done - 18810.\n",
      "Ep done - 18820.\n",
      "Ep done - 18830.\n",
      "Ep done - 18840.\n",
      "Ep done - 18850.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 278        |\n",
      "|    time_elapsed         | 3586       |\n",
      "|    total_timesteps      | 569344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03804052 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.349     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 2770       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.0836     |\n",
      "----------------------------------------\n",
      "Ep done - 18860.\n",
      "Ep done - 18870.\n",
      "Ep done - 5610.\n",
      "Ep done - 5620.\n",
      "Ep done - 5630.\n",
      "Ep done - 5640.\n",
      "Ep done - 5650.\n",
      "Ep done - 5660.\n",
      "Ep done - 5670.\n",
      "Ep done - 5680.\n",
      "Ep done - 5690.\n",
      "Ep done - 5700.\n",
      "Eval num_timesteps=570000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 30.16 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 570000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03493006 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.353     |\n",
      "|    explained_variance   | 0.474      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0216    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.59\n",
      "SELFPLAY: new best model, bumping up generation to 46\n",
      "Ep done - 18880.\n",
      "Ep done - 18890.\n",
      "Ep done - 18900.\n",
      "Ep done - 18910.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 3604     |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Ep done - 18920.\n",
      "Ep done - 18930.\n",
      "Ep done - 18940.\n",
      "Ep done - 18950.\n",
      "Ep done - 18960.\n",
      "Ep done - 18970.\n",
      "Ep done - 18980.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 280        |\n",
      "|    time_elapsed         | 3614       |\n",
      "|    total_timesteps      | 573440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03530073 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.36      |\n",
      "|    explained_variance   | 0.368      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00809    |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.0902     |\n",
      "----------------------------------------\n",
      "Ep done - 18990.\n",
      "Ep done - 19000.\n",
      "Ep done - 19010.\n",
      "Ep done - 19020.\n",
      "Ep done - 19030.\n",
      "Ep done - 19040.\n",
      "Ep done - 19050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 158        |\n",
      "|    iterations           | 281        |\n",
      "|    time_elapsed         | 3623       |\n",
      "|    total_timesteps      | 575488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03469403 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 19060.\n",
      "Ep done - 19070.\n",
      "Ep done - 19080.\n",
      "Ep done - 19090.\n",
      "Ep done - 19100.\n",
      "Ep done - 19110.\n",
      "Ep done - 19120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 3632        |\n",
      "|    total_timesteps      | 577536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027266767 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 19130.\n",
      "Ep done - 19140.\n",
      "Ep done - 19150.\n",
      "Ep done - 19160.\n",
      "Ep done - 19170.\n",
      "Ep done - 19180.\n",
      "Ep done - 19190.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.7       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 159       |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 3641      |\n",
      "|    total_timesteps      | 579584    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0477902 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.361    |\n",
      "|    explained_variance   | 0.392     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0236    |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | -0.0283   |\n",
      "|    value_loss           | 0.0813    |\n",
      "---------------------------------------\n",
      "Ep done - 19200.\n",
      "Ep done - 5710.\n",
      "Ep done - 5720.\n",
      "Ep done - 5730.\n",
      "Ep done - 5740.\n",
      "Ep done - 5750.\n",
      "Ep done - 5760.\n",
      "Ep done - 5770.\n",
      "Ep done - 5780.\n",
      "Ep done - 5790.\n",
      "Ep done - 5800.\n",
      "Eval num_timesteps=580000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.31 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048305646 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 0.0988      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 47\n",
      "Ep done - 19210.\n",
      "Ep done - 19220.\n",
      "Ep done - 19230.\n",
      "Ep done - 19240.\n",
      "Ep done - 19250.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 3660     |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Ep done - 19260.\n",
      "Ep done - 19270.\n",
      "Ep done - 19280.\n",
      "Ep done - 19290.\n",
      "Ep done - 19300.\n",
      "Ep done - 19310.\n",
      "Ep done - 19320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 285         |\n",
      "|    time_elapsed         | 3669        |\n",
      "|    total_timesteps      | 583680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039100397 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | 0.57        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    value_loss           | 0.0874      |\n",
      "-----------------------------------------\n",
      "Ep done - 19330.\n",
      "Ep done - 19340.\n",
      "Ep done - 19350.\n",
      "Ep done - 19360.\n",
      "Ep done - 19370.\n",
      "Ep done - 19380.\n",
      "Ep done - 19390.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 286        |\n",
      "|    time_elapsed         | 3678       |\n",
      "|    total_timesteps      | 585728     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03324843 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.337     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0039     |\n",
      "|    n_updates            | 2850       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Ep done - 19400.\n",
      "Ep done - 19410.\n",
      "Ep done - 19420.\n",
      "Ep done - 19430.\n",
      "Ep done - 19440.\n",
      "Ep done - 19450.\n",
      "Ep done - 19460.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 287         |\n",
      "|    time_elapsed         | 3688        |\n",
      "|    total_timesteps      | 587776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042998794 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.323      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.0992      |\n",
      "-----------------------------------------\n",
      "Ep done - 19470.\n",
      "Ep done - 19480.\n",
      "Ep done - 19490.\n",
      "Ep done - 19500.\n",
      "Ep done - 19510.\n",
      "Ep done - 19520.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 288        |\n",
      "|    time_elapsed         | 3697       |\n",
      "|    total_timesteps      | 589824     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04501415 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.328     |\n",
      "|    explained_variance   | 0.551      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 2870       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    value_loss           | 0.0718     |\n",
      "----------------------------------------\n",
      "Ep done - 19530.\n",
      "Ep done - 5810.\n",
      "Ep done - 5820.\n",
      "Ep done - 5830.\n",
      "Ep done - 5840.\n",
      "Ep done - 5850.\n",
      "Ep done - 5860.\n",
      "Ep done - 5870.\n",
      "Ep done - 5880.\n",
      "Ep done - 5890.\n",
      "Ep done - 5900.\n",
      "Eval num_timesteps=590000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 30.27 +/- 0.49\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.3     |\n",
      "|    mean_reward          | 0.62     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 590000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.034277 |\n",
      "|    clip_fraction        | 0.16     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.343   |\n",
      "|    explained_variance   | 0.478    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0164  |\n",
      "|    n_updates            | 2880     |\n",
      "|    policy_gradient_loss | -0.0247  |\n",
      "|    value_loss           | 0.0801   |\n",
      "--------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 48\n",
      "Ep done - 19540.\n",
      "Ep done - 19550.\n",
      "Ep done - 19560.\n",
      "Ep done - 19570.\n",
      "Ep done - 19580.\n",
      "Ep done - 19590.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.74     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 3715     |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "Ep done - 19600.\n",
      "Ep done - 19610.\n",
      "Ep done - 19620.\n",
      "Ep done - 19630.\n",
      "Ep done - 19640.\n",
      "Ep done - 19650.\n",
      "Ep done - 19660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 3724        |\n",
      "|    total_timesteps      | 593920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030143682 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00225    |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 19670.\n",
      "Ep done - 19680.\n",
      "Ep done - 19690.\n",
      "Ep done - 19700.\n",
      "Ep done - 19710.\n",
      "Ep done - 19720.\n",
      "Ep done - 19730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 3733        |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037642214 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.355      |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 19740.\n",
      "Ep done - 19750.\n",
      "Ep done - 19760.\n",
      "Ep done - 19770.\n",
      "Ep done - 19780.\n",
      "Ep done - 19790.\n",
      "Ep done - 19800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 159         |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 3743        |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033572752 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0287     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 19810.\n",
      "Ep done - 19820.\n",
      "Ep done - 19830.\n",
      "Ep done - 19840.\n",
      "Ep done - 19850.\n",
      "Ep done - 19860.\n",
      "Ep done - 5910.\n",
      "Ep done - 5920.\n",
      "Ep done - 5930.\n",
      "Ep done - 5940.\n",
      "Ep done - 5950.\n",
      "Ep done - 5960.\n",
      "Ep done - 5970.\n",
      "Ep done - 5980.\n",
      "Ep done - 5990.\n",
      "Ep done - 6000.\n",
      "Eval num_timesteps=600000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.28 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.3        |\n",
      "|    mean_reward          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044609495 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.325      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0364      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0311     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.49\n",
      "SELFPLAY: new best model, bumping up generation to 49\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 3762     |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "Ep done - 19870.\n",
      "Ep done - 19880.\n",
      "Ep done - 19890.\n",
      "Ep done - 19900.\n",
      "Ep done - 19910.\n",
      "Ep done - 19920.\n",
      "Ep done - 19930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.57       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 294        |\n",
      "|    time_elapsed         | 3771       |\n",
      "|    total_timesteps      | 602112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04179779 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.37       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00292   |\n",
      "|    n_updates            | 2930       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Ep done - 19940.\n",
      "Ep done - 19950.\n",
      "Ep done - 19960.\n",
      "Ep done - 19970.\n",
      "Ep done - 19980.\n",
      "Ep done - 19990.\n",
      "Ep done - 20000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.57       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 295        |\n",
      "|    time_elapsed         | 3780       |\n",
      "|    total_timesteps      | 604160     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03168325 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 20010.\n",
      "Ep done - 20020.\n",
      "Ep done - 20030.\n",
      "Ep done - 20040.\n",
      "Ep done - 20050.\n",
      "Ep done - 20060.\n",
      "Ep done - 20070.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 159        |\n",
      "|    iterations           | 296        |\n",
      "|    time_elapsed         | 3789       |\n",
      "|    total_timesteps      | 606208     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04163842 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.319     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 2950       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Ep done - 20080.\n",
      "Ep done - 20090.\n",
      "Ep done - 20100.\n",
      "Ep done - 20110.\n",
      "Ep done - 20120.\n",
      "Ep done - 20130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 297        |\n",
      "|    time_elapsed         | 3798       |\n",
      "|    total_timesteps      | 608256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05259666 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.329     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "Ep done - 20140.\n",
      "Ep done - 20150.\n",
      "Ep done - 20160.\n",
      "Ep done - 20170.\n",
      "Ep done - 20180.\n",
      "Ep done - 20190.\n",
      "Ep done - 6010.\n",
      "Ep done - 6020.\n",
      "Ep done - 6030.\n",
      "Ep done - 6040.\n",
      "Ep done - 6050.\n",
      "Ep done - 6060.\n",
      "Ep done - 6070.\n",
      "Ep done - 6080.\n",
      "Ep done - 6090.\n",
      "Ep done - 6100.\n",
      "Eval num_timesteps=610000, episode_reward=0.42 +/- 0.90\n",
      "Episode length: 30.16 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040196516 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 0.118       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.42\n",
      "SELFPLAY: new best model, bumping up generation to 50\n",
      "Ep done - 20200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.52     |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 3816     |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Ep done - 20210.\n",
      "Ep done - 20220.\n",
      "Ep done - 20230.\n",
      "Ep done - 20240.\n",
      "Ep done - 20250.\n",
      "Ep done - 20260.\n",
      "Ep done - 20270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 3825        |\n",
      "|    total_timesteps      | 612352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043309458 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 20280.\n",
      "Ep done - 20290.\n",
      "Ep done - 20300.\n",
      "Ep done - 20310.\n",
      "Ep done - 20320.\n",
      "Ep done - 20330.\n",
      "Ep done - 20340.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 300        |\n",
      "|    time_elapsed         | 3835       |\n",
      "|    total_timesteps      | 614400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03668261 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 2990       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 20350.\n",
      "Ep done - 20360.\n",
      "Ep done - 20370.\n",
      "Ep done - 20380.\n",
      "Ep done - 20390.\n",
      "Ep done - 20400.\n",
      "Ep done - 20410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 160        |\n",
      "|    iterations           | 301        |\n",
      "|    time_elapsed         | 3844       |\n",
      "|    total_timesteps      | 616448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03403146 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.294     |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00674    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Ep done - 20420.\n",
      "Ep done - 20430.\n",
      "Ep done - 20440.\n",
      "Ep done - 20450.\n",
      "Ep done - 20460.\n",
      "Ep done - 20470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 3853        |\n",
      "|    total_timesteps      | 618496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037598863 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0536      |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 20480.\n",
      "Ep done - 20490.\n",
      "Ep done - 20500.\n",
      "Ep done - 20510.\n",
      "Ep done - 20520.\n",
      "Ep done - 6110.\n",
      "Ep done - 6120.\n",
      "Ep done - 6130.\n",
      "Ep done - 6140.\n",
      "Ep done - 6150.\n",
      "Ep done - 6160.\n",
      "Ep done - 6170.\n",
      "Ep done - 6180.\n",
      "Ep done - 6190.\n",
      "Ep done - 6200.\n",
      "Eval num_timesteps=620000, episode_reward=0.47 +/- 0.87\n",
      "Episode length: 30.25 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 620000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04175386 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.325     |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0454     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 51\n",
      "Ep done - 20530.\n",
      "Ep done - 20540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 3872     |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n",
      "Ep done - 20550.\n",
      "Ep done - 20560.\n",
      "Ep done - 20570.\n",
      "Ep done - 20580.\n",
      "Ep done - 20590.\n",
      "Ep done - 20600.\n",
      "Ep done - 20610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 3881        |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033433735 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0137      |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "Ep done - 20620.\n",
      "Ep done - 20630.\n",
      "Ep done - 20640.\n",
      "Ep done - 20650.\n",
      "Ep done - 20660.\n",
      "Ep done - 20670.\n",
      "Ep done - 20680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.33        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 3890        |\n",
      "|    total_timesteps      | 624640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048142135 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0514      |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 20690.\n",
      "Ep done - 20700.\n",
      "Ep done - 20710.\n",
      "Ep done - 20720.\n",
      "Ep done - 20730.\n",
      "Ep done - 20740.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 306         |\n",
      "|    time_elapsed         | 3899        |\n",
      "|    total_timesteps      | 626688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049989685 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00859     |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 20750.\n",
      "Ep done - 20760.\n",
      "Ep done - 20770.\n",
      "Ep done - 20780.\n",
      "Ep done - 20790.\n",
      "Ep done - 20800.\n",
      "Ep done - 20810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 3908        |\n",
      "|    total_timesteps      | 628736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040880047 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0081     |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 20820.\n",
      "Ep done - 20830.\n",
      "Ep done - 20840.\n",
      "Ep done - 20850.\n",
      "Ep done - 6210.\n",
      "Ep done - 6220.\n",
      "Ep done - 6230.\n",
      "Ep done - 6240.\n",
      "Ep done - 6250.\n",
      "Ep done - 6260.\n",
      "Ep done - 6270.\n",
      "Ep done - 6280.\n",
      "Ep done - 6290.\n",
      "Ep done - 6300.\n",
      "Eval num_timesteps=630000, episode_reward=0.50 +/- 0.87\n",
      "Episode length: 30.20 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 630000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046350118 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 52\n",
      "Ep done - 20860.\n",
      "Ep done - 20870.\n",
      "Ep done - 20880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 3927     |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Ep done - 20890.\n",
      "Ep done - 20900.\n",
      "Ep done - 20910.\n",
      "Ep done - 20920.\n",
      "Ep done - 20930.\n",
      "Ep done - 20940.\n",
      "Ep done - 20950.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 309         |\n",
      "|    time_elapsed         | 3936        |\n",
      "|    total_timesteps      | 632832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037381064 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 20960.\n",
      "Ep done - 20970.\n",
      "Ep done - 20980.\n",
      "Ep done - 20990.\n",
      "Ep done - 21000.\n",
      "Ep done - 21010.\n",
      "Ep done - 21020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 160         |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 3945        |\n",
      "|    total_timesteps      | 634880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035384685 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00441     |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Ep done - 21030.\n",
      "Ep done - 21040.\n",
      "Ep done - 21050.\n",
      "Ep done - 21060.\n",
      "Ep done - 21070.\n",
      "Ep done - 21080.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.59      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 161       |\n",
      "|    iterations           | 311       |\n",
      "|    time_elapsed         | 3954      |\n",
      "|    total_timesteps      | 636928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0422196 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.3      |\n",
      "|    explained_variance   | 0.271     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0219    |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.0246   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "Ep done - 21090.\n",
      "Ep done - 21100.\n",
      "Ep done - 21110.\n",
      "Ep done - 21120.\n",
      "Ep done - 21130.\n",
      "Ep done - 21140.\n",
      "Ep done - 21150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 312         |\n",
      "|    time_elapsed         | 3963        |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039725684 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 21160.\n",
      "Ep done - 21170.\n",
      "Ep done - 21180.\n",
      "Ep done - 21190.\n",
      "Ep done - 6310.\n",
      "Ep done - 6320.\n",
      "Ep done - 6330.\n",
      "Ep done - 6340.\n",
      "Ep done - 6350.\n",
      "Ep done - 6360.\n",
      "Ep done - 6370.\n",
      "Ep done - 6380.\n",
      "Ep done - 6390.\n",
      "Ep done - 6400.\n",
      "Eval num_timesteps=640000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.17 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049880527 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00983     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 53\n",
      "Ep done - 21200.\n",
      "Ep done - 21210.\n",
      "Ep done - 21220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 3982     |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Ep done - 21230.\n",
      "Ep done - 21240.\n",
      "Ep done - 21250.\n",
      "Ep done - 21260.\n",
      "Ep done - 21270.\n",
      "Ep done - 21280.\n",
      "Ep done - 21290.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 314        |\n",
      "|    time_elapsed         | 3991       |\n",
      "|    total_timesteps      | 643072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03478316 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.307     |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 3130       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 21300.\n",
      "Ep done - 21310.\n",
      "Ep done - 21320.\n",
      "Ep done - 21330.\n",
      "Ep done - 21340.\n",
      "Ep done - 21350.\n",
      "Ep done - 21360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 315        |\n",
      "|    time_elapsed         | 4000       |\n",
      "|    total_timesteps      | 645120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04085458 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.309     |\n",
      "|    explained_variance   | 0.303      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0165     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Ep done - 21370.\n",
      "Ep done - 21380.\n",
      "Ep done - 21390.\n",
      "Ep done - 21400.\n",
      "Ep done - 21410.\n",
      "Ep done - 21420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 4009        |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040228833 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 21430.\n",
      "Ep done - 21440.\n",
      "Ep done - 21450.\n",
      "Ep done - 21460.\n",
      "Ep done - 21470.\n",
      "Ep done - 21480.\n",
      "Ep done - 21490.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 317         |\n",
      "|    time_elapsed         | 4019        |\n",
      "|    total_timesteps      | 649216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044958454 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0311      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Ep done - 21500.\n",
      "Ep done - 21510.\n",
      "Ep done - 21520.\n",
      "Ep done - 6410.\n",
      "Ep done - 6420.\n",
      "Ep done - 6430.\n",
      "Ep done - 6440.\n",
      "Ep done - 6450.\n",
      "Ep done - 6460.\n",
      "Ep done - 6470.\n",
      "Ep done - 6480.\n",
      "Ep done - 6490.\n",
      "Ep done - 6500.\n",
      "Eval num_timesteps=650000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.14 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035596795 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.294      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 54\n",
      "Ep done - 21530.\n",
      "Ep done - 21540.\n",
      "Ep done - 21550.\n",
      "Ep done - 21560.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 4037     |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Ep done - 21570.\n",
      "Ep done - 21580.\n",
      "Ep done - 21590.\n",
      "Ep done - 21600.\n",
      "Ep done - 21610.\n",
      "Ep done - 21620.\n",
      "Ep done - 21630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 319         |\n",
      "|    time_elapsed         | 4046        |\n",
      "|    total_timesteps      | 653312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041661162 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 21640.\n",
      "Ep done - 21650.\n",
      "Ep done - 21660.\n",
      "Ep done - 21670.\n",
      "Ep done - 21680.\n",
      "Ep done - 21690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 4055        |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038634695 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Ep done - 21700.\n",
      "Ep done - 21710.\n",
      "Ep done - 21720.\n",
      "Ep done - 21730.\n",
      "Ep done - 21740.\n",
      "Ep done - 21750.\n",
      "Ep done - 21760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 4064        |\n",
      "|    total_timesteps      | 657408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033893593 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0322      |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Ep done - 21770.\n",
      "Ep done - 21780.\n",
      "Ep done - 21790.\n",
      "Ep done - 21800.\n",
      "Ep done - 21810.\n",
      "Ep done - 21820.\n",
      "Ep done - 21830.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 4073        |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043831665 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00303     |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0997      |\n",
      "-----------------------------------------\n",
      "Ep done - 21840.\n",
      "Ep done - 21850.\n",
      "Ep done - 6510.\n",
      "Ep done - 6520.\n",
      "Ep done - 6530.\n",
      "Ep done - 6540.\n",
      "Ep done - 6550.\n",
      "Ep done - 6560.\n",
      "Ep done - 6570.\n",
      "Ep done - 6580.\n",
      "Ep done - 6590.\n",
      "Ep done - 6600.\n",
      "Eval num_timesteps=660000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.22 +/- 0.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 660000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03685048 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.312     |\n",
      "|    explained_variance   | 0.497      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0033    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 55\n",
      "Ep done - 21860.\n",
      "Ep done - 21870.\n",
      "Ep done - 21880.\n",
      "Ep done - 21890.\n",
      "Ep done - 21900.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 4092     |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Ep done - 21910.\n",
      "Ep done - 21920.\n",
      "Ep done - 21930.\n",
      "Ep done - 21940.\n",
      "Ep done - 21950.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 21960.\n",
      "Ep done - 21970.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 324        |\n",
      "|    time_elapsed         | 4101       |\n",
      "|    total_timesteps      | 663552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03811264 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.3       |\n",
      "|    explained_variance   | 0.298      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0053    |\n",
      "|    n_updates            | 3230       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Ep done - 21980.\n",
      "Ep done - 21990.\n",
      "Ep done - 22000.\n",
      "Ep done - 22010.\n",
      "Ep done - 22020.\n",
      "Ep done - 22030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 161         |\n",
      "|    iterations           | 325         |\n",
      "|    time_elapsed         | 4110        |\n",
      "|    total_timesteps      | 665600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036279533 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00954    |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Ep done - 22040.\n",
      "Ep done - 22050.\n",
      "Ep done - 22060.\n",
      "Ep done - 22070.\n",
      "Ep done - 22080.\n",
      "Ep done - 22090.\n",
      "Ep done - 22100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 326        |\n",
      "|    time_elapsed         | 4119       |\n",
      "|    total_timesteps      | 667648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04980818 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.318     |\n",
      "|    explained_variance   | 0.404      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Ep done - 22110.\n",
      "Ep done - 22120.\n",
      "Ep done - 22130.\n",
      "Ep done - 22140.\n",
      "Ep done - 22150.\n",
      "Ep done - 22160.\n",
      "Ep done - 22170.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 327         |\n",
      "|    time_elapsed         | 4128        |\n",
      "|    total_timesteps      | 669696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053845845 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.303      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00316    |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 22180.\n",
      "Ep done - 6610.\n",
      "Ep done - 6620.\n",
      "Ep done - 6630.\n",
      "Ep done - 6640.\n",
      "Ep done - 6650.\n",
      "Ep done - 6660.\n",
      "Ep done - 6670.\n",
      "Ep done - 6680.\n",
      "Ep done - 6690.\n",
      "Ep done - 6700.\n",
      "Eval num_timesteps=670000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.21 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 670000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03674811 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.284     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 3270       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.74\n",
      "SELFPLAY: new best model, bumping up generation to 56\n",
      "Ep done - 22190.\n",
      "Ep done - 22200.\n",
      "Ep done - 22210.\n",
      "Ep done - 22220.\n",
      "Ep done - 22230.\n",
      "Ep done - 22240.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 161      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 4147     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Ep done - 22250.\n",
      "Ep done - 22260.\n",
      "Ep done - 22270.\n",
      "Ep done - 22280.\n",
      "Ep done - 22290.\n",
      "Ep done - 22300.\n",
      "Ep done - 22310.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 329         |\n",
      "|    time_elapsed         | 4156        |\n",
      "|    total_timesteps      | 673792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036582306 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00739     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 22320.\n",
      "Ep done - 22330.\n",
      "Ep done - 22340.\n",
      "Ep done - 22350.\n",
      "Ep done - 22360.\n",
      "Ep done - 22370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 330        |\n",
      "|    time_elapsed         | 4165       |\n",
      "|    total_timesteps      | 675840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05431831 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.265     |\n",
      "|    explained_variance   | 0.36       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0387     |\n",
      "|    n_updates            | 3290       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Ep done - 22380.\n",
      "Ep done - 22390.\n",
      "Ep done - 22400.\n",
      "Ep done - 22410.\n",
      "Ep done - 22420.\n",
      "Ep done - 22430.\n",
      "Ep done - 22440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 331         |\n",
      "|    time_elapsed         | 4174        |\n",
      "|    total_timesteps      | 677888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048333377 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.273      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.0601      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 22450.\n",
      "Ep done - 22460.\n",
      "Ep done - 22470.\n",
      "Ep done - 22480.\n",
      "Ep done - 22490.\n",
      "Ep done - 22500.\n",
      "Ep done - 22510.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 4184        |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041124206 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 6710.\n",
      "Ep done - 6720.\n",
      "Ep done - 6730.\n",
      "Ep done - 6740.\n",
      "Ep done - 6750.\n",
      "Ep done - 6760.\n",
      "Ep done - 6770.\n",
      "Ep done - 6780.\n",
      "Ep done - 6790.\n",
      "Ep done - 6800.\n",
      "Eval num_timesteps=680000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.21 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 680000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03638556 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00717    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    value_loss           | 0.097      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 57\n",
      "Ep done - 22520.\n",
      "Ep done - 22530.\n",
      "Ep done - 22540.\n",
      "Ep done - 22550.\n",
      "Ep done - 22560.\n",
      "Ep done - 22570.\n",
      "Ep done - 22580.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 4202     |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Ep done - 22590.\n",
      "Ep done - 22600.\n",
      "Ep done - 22610.\n",
      "Ep done - 22620.\n",
      "Ep done - 22630.\n",
      "Ep done - 22640.\n",
      "Ep done - 22650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 4211        |\n",
      "|    total_timesteps      | 684032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039804175 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 22660.\n",
      "Ep done - 22670.\n",
      "Ep done - 22680.\n",
      "Ep done - 22690.\n",
      "Ep done - 22700.\n",
      "Ep done - 22710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 335        |\n",
      "|    time_elapsed         | 4220       |\n",
      "|    total_timesteps      | 686080     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04603356 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.186      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0402     |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Ep done - 22720.\n",
      "Ep done - 22730.\n",
      "Ep done - 22740.\n",
      "Ep done - 22750.\n",
      "Ep done - 22760.\n",
      "Ep done - 22770.\n",
      "Ep done - 22780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 4229        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038624663 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 22790.\n",
      "Ep done - 22800.\n",
      "Ep done - 22810.\n",
      "Ep done - 22820.\n",
      "Ep done - 22830.\n",
      "Ep done - 22840.\n",
      "Ep done - 6810.\n",
      "Ep done - 6820.\n",
      "Ep done - 6830.\n",
      "Ep done - 6840.\n",
      "Ep done - 6850.\n",
      "Ep done - 6860.\n",
      "Ep done - 6870.\n",
      "Ep done - 6880.\n",
      "Ep done - 6890.\n",
      "Ep done - 6900.\n",
      "Eval num_timesteps=690000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 30.18 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 690000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03430425 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.254     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0094     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.64\n",
      "SELFPLAY: new best model, bumping up generation to 58\n",
      "Ep done - 22850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 4248     |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "Ep done - 22860.\n",
      "Ep done - 22870.\n",
      "Ep done - 22880.\n",
      "Ep done - 22890.\n",
      "Ep done - 22900.\n",
      "Ep done - 22910.\n",
      "Ep done - 22920.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 162        |\n",
      "|    iterations           | 338        |\n",
      "|    time_elapsed         | 4257       |\n",
      "|    total_timesteps      | 692224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04322489 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.292      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 3370       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Ep done - 22930.\n",
      "Ep done - 22940.\n",
      "Ep done - 22950.\n",
      "Ep done - 22960.\n",
      "Ep done - 22970.\n",
      "Ep done - 22980.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 339         |\n",
      "|    time_elapsed         | 4266        |\n",
      "|    total_timesteps      | 694272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053199325 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0317      |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0823      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 22990.\n",
      "Ep done - 23000.\n",
      "Ep done - 23010.\n",
      "Ep done - 23020.\n",
      "Ep done - 23030.\n",
      "Ep done - 23040.\n",
      "Ep done - 23050.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 162       |\n",
      "|    iterations           | 340       |\n",
      "|    time_elapsed         | 4276      |\n",
      "|    total_timesteps      | 696320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0529887 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.278    |\n",
      "|    explained_variance   | 0.263     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0262    |\n",
      "|    n_updates            | 3390      |\n",
      "|    policy_gradient_loss | -0.026    |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Ep done - 23060.\n",
      "Ep done - 23070.\n",
      "Ep done - 23080.\n",
      "Ep done - 23090.\n",
      "Ep done - 23100.\n",
      "Ep done - 23110.\n",
      "Ep done - 23120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 341         |\n",
      "|    time_elapsed         | 4285        |\n",
      "|    total_timesteps      | 698368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041927338 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "Ep done - 23130.\n",
      "Ep done - 23140.\n",
      "Ep done - 23150.\n",
      "Ep done - 23160.\n",
      "Ep done - 23170.\n",
      "Ep done - 6910.\n",
      "Ep done - 6920.\n",
      "Ep done - 6930.\n",
      "Ep done - 6940.\n",
      "Ep done - 6950.\n",
      "Ep done - 6960.\n",
      "Ep done - 6970.\n",
      "Ep done - 6980.\n",
      "Ep done - 6990.\n",
      "Ep done - 7000.\n",
      "Eval num_timesteps=700000, episode_reward=0.66 +/- 0.72\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033991836 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00254    |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 59\n",
      "Ep done - 23180.\n",
      "Ep done - 23190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.81     |\n",
      "| time/              |          |\n",
      "|    fps             | 162      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 4303     |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Ep done - 23200.\n",
      "Ep done - 23210.\n",
      "Ep done - 23220.\n",
      "Ep done - 23230.\n",
      "Ep done - 23240.\n",
      "Ep done - 23250.\n",
      "Ep done - 23260.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 162         |\n",
      "|    iterations           | 343         |\n",
      "|    time_elapsed         | 4312        |\n",
      "|    total_timesteps      | 702464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043208882 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00143     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.0664      |\n",
      "-----------------------------------------\n",
      "Ep done - 23270.\n",
      "Ep done - 23280.\n",
      "Ep done - 23290.\n",
      "Ep done - 23300.\n",
      "Ep done - 23310.\n",
      "Ep done - 23320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 4321        |\n",
      "|    total_timesteps      | 704512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030756341 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00598     |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 0.0966      |\n",
      "-----------------------------------------\n",
      "Ep done - 23330.\n",
      "Ep done - 23340.\n",
      "Ep done - 23350.\n",
      "Ep done - 23360.\n",
      "Ep done - 23370.\n",
      "Ep done - 23380.\n",
      "Ep done - 23390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 4331        |\n",
      "|    total_timesteps      | 706560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036867652 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0864      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 23400.\n",
      "Ep done - 23410.\n",
      "Ep done - 23420.\n",
      "Ep done - 23430.\n",
      "Ep done - 23440.\n",
      "Ep done - 23450.\n",
      "Ep done - 23460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 346        |\n",
      "|    time_elapsed         | 4340       |\n",
      "|    total_timesteps      | 708608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03295829 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.369      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0209     |\n",
      "|    n_updates            | 3450       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 23470.\n",
      "Ep done - 23480.\n",
      "Ep done - 23490.\n",
      "Ep done - 23500.\n",
      "Ep done - 23510.\n",
      "Ep done - 7010.\n",
      "Ep done - 7020.\n",
      "Ep done - 7030.\n",
      "Ep done - 7040.\n",
      "Ep done - 7050.\n",
      "Ep done - 7060.\n",
      "Ep done - 7070.\n",
      "Ep done - 7080.\n",
      "Ep done - 7090.\n",
      "Ep done - 7100.\n",
      "Eval num_timesteps=710000, episode_reward=0.69 +/- 0.72\n",
      "Episode length: 30.19 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034872223 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.69\n",
      "SELFPLAY: new best model, bumping up generation to 60\n",
      "Ep done - 23520.\n",
      "Ep done - 23530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.59     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 4358     |\n",
      "|    total_timesteps | 710656   |\n",
      "---------------------------------\n",
      "Ep done - 23540.\n",
      "Ep done - 23550.\n",
      "Ep done - 23560.\n",
      "Ep done - 23570.\n",
      "Ep done - 23580.\n",
      "Ep done - 23590.\n",
      "Ep done - 23600.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 348        |\n",
      "|    time_elapsed         | 4367       |\n",
      "|    total_timesteps      | 712704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03607657 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.281     |\n",
      "|    explained_variance   | 0.413      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 3470       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "Ep done - 23610.\n",
      "Ep done - 23620.\n",
      "Ep done - 23630.\n",
      "Ep done - 23640.\n",
      "Ep done - 23650.\n",
      "Ep done - 23660.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 4377        |\n",
      "|    total_timesteps      | 714752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042406023 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.001      |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.0887      |\n",
      "-----------------------------------------\n",
      "Ep done - 23670.\n",
      "Ep done - 23680.\n",
      "Ep done - 23690.\n",
      "Ep done - 23700.\n",
      "Ep done - 23710.\n",
      "Ep done - 23720.\n",
      "Ep done - 23730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 4386        |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038179085 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0439      |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 23740.\n",
      "Ep done - 23750.\n",
      "Ep done - 23760.\n",
      "Ep done - 23770.\n",
      "Ep done - 23780.\n",
      "Ep done - 23790.\n",
      "Ep done - 23800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 4395        |\n",
      "|    total_timesteps      | 718848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040271416 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00062     |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 23810.\n",
      "Ep done - 23820.\n",
      "Ep done - 23830.\n",
      "Ep done - 23840.\n",
      "Ep done - 7110.\n",
      "Ep done - 7120.\n",
      "Ep done - 7130.\n",
      "Ep done - 7140.\n",
      "Ep done - 7150.\n",
      "Ep done - 7160.\n",
      "Ep done - 7170.\n",
      "Ep done - 7180.\n",
      "Ep done - 7190.\n",
      "Ep done - 7200.\n",
      "Eval num_timesteps=720000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 30.25 +/- 0.57\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 720000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03715581 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.421      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00642    |\n",
      "|    n_updates            | 3510       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    value_loss           | 0.0977     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.75\n",
      "SELFPLAY: new best model, bumping up generation to 61\n",
      "Ep done - 23850.\n",
      "Ep done - 23860.\n",
      "Ep done - 23870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 4414     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Ep done - 23880.\n",
      "Ep done - 23890.\n",
      "Ep done - 23900.\n",
      "Ep done - 23910.\n",
      "Ep done - 23920.\n",
      "Ep done - 23930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 4423        |\n",
      "|    total_timesteps      | 722944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034397826 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00918     |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 0.0856      |\n",
      "-----------------------------------------\n",
      "Ep done - 23940.\n",
      "Ep done - 23950.\n",
      "Ep done - 23960.\n",
      "Ep done - 23970.\n",
      "Ep done - 23980.\n",
      "Ep done - 23990.\n",
      "Ep done - 24000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.45      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 163       |\n",
      "|    iterations           | 354       |\n",
      "|    time_elapsed         | 4432      |\n",
      "|    total_timesteps      | 724992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0417161 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.305    |\n",
      "|    explained_variance   | 0.45      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00117  |\n",
      "|    n_updates            | 3530      |\n",
      "|    policy_gradient_loss | -0.027    |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Ep done - 24010.\n",
      "Ep done - 24020.\n",
      "Ep done - 24030.\n",
      "Ep done - 24040.\n",
      "Ep done - 24050.\n",
      "Ep done - 24060.\n",
      "Ep done - 24070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 355         |\n",
      "|    time_elapsed         | 4441        |\n",
      "|    total_timesteps      | 727040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043461114 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 24080.\n",
      "Ep done - 24090.\n",
      "Ep done - 24100.\n",
      "Ep done - 24110.\n",
      "Ep done - 24120.\n",
      "Ep done - 24130.\n",
      "Ep done - 24140.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 4450        |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037916128 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 24150.\n",
      "Ep done - 24160.\n",
      "Ep done - 24170.\n",
      "Ep done - 7210.\n",
      "Ep done - 7220.\n",
      "Ep done - 7230.\n",
      "Ep done - 7240.\n",
      "Ep done - 7250.\n",
      "Ep done - 7260.\n",
      "Ep done - 7270.\n",
      "Ep done - 7280.\n",
      "Ep done - 7290.\n",
      "Ep done - 7300.\n",
      "Eval num_timesteps=730000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.06 +/- 0.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041322704 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.098       |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 62\n",
      "Ep done - 24180.\n",
      "Ep done - 24190.\n",
      "Ep done - 24200.\n",
      "Ep done - 24210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.68     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Ep done - 24220.\n",
      "Ep done - 24230.\n",
      "Ep done - 24240.\n",
      "Ep done - 24250.\n",
      "Ep done - 24260.\n",
      "Ep done - 24270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 358         |\n",
      "|    time_elapsed         | 4478        |\n",
      "|    total_timesteps      | 733184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034216154 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00226    |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.0703      |\n",
      "-----------------------------------------\n",
      "Ep done - 24280.\n",
      "Ep done - 24290.\n",
      "Ep done - 24300.\n",
      "Ep done - 24310.\n",
      "Ep done - 24320.\n",
      "Ep done - 24330.\n",
      "Ep done - 24340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 359         |\n",
      "|    time_elapsed         | 4487        |\n",
      "|    total_timesteps      | 735232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035479628 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0652      |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.0981      |\n",
      "-----------------------------------------\n",
      "Ep done - 24350.\n",
      "Ep done - 24360.\n",
      "Ep done - 24370.\n",
      "Ep done - 24380.\n",
      "Ep done - 24390.\n",
      "Ep done - 24400.\n",
      "Ep done - 24410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 163        |\n",
      "|    iterations           | 360        |\n",
      "|    time_elapsed         | 4496       |\n",
      "|    total_timesteps      | 737280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04084664 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.29       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00323    |\n",
      "|    n_updates            | 3590       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Ep done - 24420.\n",
      "Ep done - 24430.\n",
      "Ep done - 24440.\n",
      "Ep done - 24450.\n",
      "Ep done - 24460.\n",
      "Ep done - 24470.\n",
      "Ep done - 24480.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 361         |\n",
      "|    time_elapsed         | 4506        |\n",
      "|    total_timesteps      | 739328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047817506 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Ep done - 24490.\n",
      "Ep done - 24500.\n",
      "Ep done - 7310.\n",
      "Ep done - 7320.\n",
      "Ep done - 7330.\n",
      "Ep done - 7340.\n",
      "Ep done - 7350.\n",
      "Ep done - 7360.\n",
      "Ep done - 7370.\n",
      "Ep done - 7380.\n",
      "Ep done - 7390.\n",
      "Ep done - 7400.\n",
      "Eval num_timesteps=740000, episode_reward=0.79 +/- 0.59\n",
      "Episode length: 30.22 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 740000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03978927 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.275     |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 3610       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.0527     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 63\n",
      "Ep done - 24510.\n",
      "Ep done - 24520.\n",
      "Ep done - 24530.\n",
      "Ep done - 24540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 163      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 4524     |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Ep done - 24550.\n",
      "Ep done - 24560.\n",
      "Ep done - 24570.\n",
      "Ep done - 24580.\n",
      "Ep done - 24590.\n",
      "Ep done - 24600.\n",
      "Ep done - 24610.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 163         |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 4533        |\n",
      "|    total_timesteps      | 743424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041021675 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Ep done - 24620.\n",
      "Ep done - 24630.\n",
      "Ep done - 24640.\n",
      "Ep done - 24650.\n",
      "Ep done - 24660.\n",
      "Ep done - 24670.\n",
      "Ep done - 24680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 364         |\n",
      "|    time_elapsed         | 4542        |\n",
      "|    total_timesteps      | 745472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043341406 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.296      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | -0.0267     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 24690.\n",
      "Ep done - 24700.\n",
      "Ep done - 24710.\n",
      "Ep done - 24720.\n",
      "Ep done - 24730.\n",
      "Ep done - 24740.\n",
      "Ep done - 24750.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 4551        |\n",
      "|    total_timesteps      | 747520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040164605 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.314      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Ep done - 24760.\n",
      "Ep done - 24770.\n",
      "Ep done - 24780.\n",
      "Ep done - 24790.\n",
      "Ep done - 24800.\n",
      "Ep done - 24810.\n",
      "Ep done - 24820.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 4560        |\n",
      "|    total_timesteps      | 749568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030899674 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.306      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00655     |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 24830.\n",
      "Ep done - 7410.\n",
      "Ep done - 7420.\n",
      "Ep done - 7430.\n",
      "Ep done - 7440.\n",
      "Ep done - 7450.\n",
      "Ep done - 7460.\n",
      "Ep done - 7470.\n",
      "Ep done - 7480.\n",
      "Ep done - 7490.\n",
      "Ep done - 7500.\n",
      "Eval num_timesteps=750000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.24 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 750000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03604187 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.304     |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.0908     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.73\n",
      "SELFPLAY: new best model, bumping up generation to 64\n",
      "Ep done - 24840.\n",
      "Ep done - 24850.\n",
      "Ep done - 24860.\n",
      "Ep done - 24870.\n",
      "Ep done - 24880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 4579     |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Ep done - 24890.\n",
      "Ep done - 24900.\n",
      "Ep done - 24910.\n",
      "Ep done - 24920.\n",
      "Ep done - 24930.\n",
      "Ep done - 24940.\n",
      "Ep done - 24950.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 4588        |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035654403 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "Ep done - 24960.\n",
      "Ep done - 24970.\n",
      "Ep done - 24980.\n",
      "Ep done - 24990.\n",
      "Ep done - 25000.\n",
      "Ep done - 25010.\n",
      "Ep done - 25020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 369         |\n",
      "|    time_elapsed         | 4597        |\n",
      "|    total_timesteps      | 755712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039278314 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.0892      |\n",
      "-----------------------------------------\n",
      "Ep done - 25030.\n",
      "Ep done - 25040.\n",
      "Ep done - 25050.\n",
      "Ep done - 25060.\n",
      "Ep done - 25070.\n",
      "Ep done - 25080.\n",
      "Ep done - 25090.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 370         |\n",
      "|    time_elapsed         | 4607        |\n",
      "|    total_timesteps      | 757760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033049703 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.297      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.022       |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 25100.\n",
      "Ep done - 25110.\n",
      "Ep done - 25120.\n",
      "Ep done - 25130.\n",
      "Ep done - 25140.\n",
      "Ep done - 25150.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 4616        |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045327045 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 25160.\n",
      "Ep done - 7510.\n",
      "Ep done - 7520.\n",
      "Ep done - 7530.\n",
      "Ep done - 7540.\n",
      "Ep done - 7550.\n",
      "Ep done - 7560.\n",
      "Ep done - 7570.\n",
      "Ep done - 7580.\n",
      "Ep done - 7590.\n",
      "Ep done - 7600.\n",
      "Eval num_timesteps=760000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.16 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 760000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04543481 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.301     |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0214     |\n",
      "|    n_updates            | 3710       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    value_loss           | 0.0829     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 65\n",
      "Ep done - 25170.\n",
      "Ep done - 25180.\n",
      "Ep done - 25190.\n",
      "Ep done - 25200.\n",
      "Ep done - 25210.\n",
      "Ep done - 25220.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 4634     |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Ep done - 25230.\n",
      "Ep done - 25240.\n",
      "Ep done - 25250.\n",
      "Ep done - 25260.\n",
      "Ep done - 25270.\n",
      "Ep done - 25280.\n",
      "Ep done - 25290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 4644        |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037409727 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.461       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 25300.\n",
      "Ep done - 25310.\n",
      "Ep done - 25320.\n",
      "Ep done - 25330.\n",
      "Ep done - 25340.\n",
      "Ep done - 25350.\n",
      "Ep done - 25360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 4653        |\n",
      "|    total_timesteps      | 765952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041028574 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.387       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0121      |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 25370.\n",
      "Ep done - 25380.\n",
      "Ep done - 25390.\n",
      "Ep done - 25400.\n",
      "Ep done - 25410.\n",
      "Ep done - 25420.\n",
      "Ep done - 25430.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 4662        |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044086322 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Ep done - 25440.\n",
      "Ep done - 25450.\n",
      "Ep done - 25460.\n",
      "Ep done - 25470.\n",
      "Ep done - 25480.\n",
      "Ep done - 25490.\n",
      "Ep done - 7610.\n",
      "Ep done - 7620.\n",
      "Ep done - 7630.\n",
      "Ep done - 7640.\n",
      "Ep done - 7650.\n",
      "Ep done - 7660.\n",
      "Ep done - 7670.\n",
      "Ep done - 7680.\n",
      "Ep done - 7690.\n",
      "Ep done - 7700.\n",
      "Eval num_timesteps=770000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.15 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039108127 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0692      |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 66\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Ep done - 25500.\n",
      "Ep done - 25510.\n",
      "Ep done - 25520.\n",
      "Ep done - 25530.\n",
      "Ep done - 25540.\n",
      "Ep done - 25550.\n",
      "Ep done - 25560.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 4689        |\n",
      "|    total_timesteps      | 772096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049055643 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.0271     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Ep done - 25570.\n",
      "Ep done - 25580.\n",
      "Ep done - 25590.\n",
      "Ep done - 25600.\n",
      "Ep done - 25610.\n",
      "Ep done - 25620.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 25630.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 378        |\n",
      "|    time_elapsed         | 4699       |\n",
      "|    total_timesteps      | 774144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05117338 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.284     |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00643   |\n",
      "|    n_updates            | 3770       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Ep done - 25640.\n",
      "Ep done - 25650.\n",
      "Ep done - 25660.\n",
      "Ep done - 25670.\n",
      "Ep done - 25680.\n",
      "Ep done - 25690.\n",
      "Ep done - 25700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 379         |\n",
      "|    time_elapsed         | 4708        |\n",
      "|    total_timesteps      | 776192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044220738 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00728    |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Ep done - 25710.\n",
      "Ep done - 25720.\n",
      "Ep done - 25730.\n",
      "Ep done - 25740.\n",
      "Ep done - 25750.\n",
      "Ep done - 25760.\n",
      "Ep done - 25770.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.42       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 380        |\n",
      "|    time_elapsed         | 4717       |\n",
      "|    total_timesteps      | 778240     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04610355 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.305     |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0428     |\n",
      "|    n_updates            | 3790       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Ep done - 25780.\n",
      "Ep done - 25790.\n",
      "Ep done - 25800.\n",
      "Ep done - 25810.\n",
      "Ep done - 25820.\n",
      "Ep done - 7710.\n",
      "Ep done - 7720.\n",
      "Ep done - 7730.\n",
      "Ep done - 7740.\n",
      "Ep done - 7750.\n",
      "Ep done - 7760.\n",
      "Ep done - 7770.\n",
      "Ep done - 7780.\n",
      "Ep done - 7790.\n",
      "Ep done - 7800.\n",
      "Eval num_timesteps=780000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 30.23 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041373268 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.288      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.52\n",
      "SELFPLAY: new best model, bumping up generation to 67\n",
      "Ep done - 25830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 4735     |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Ep done - 25840.\n",
      "Ep done - 25850.\n",
      "Ep done - 25860.\n",
      "Ep done - 25870.\n",
      "Ep done - 25880.\n",
      "Ep done - 25890.\n",
      "Ep done - 25900.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 4745        |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037173532 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.045       |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "Ep done - 25910.\n",
      "Ep done - 25920.\n",
      "Ep done - 25930.\n",
      "Ep done - 25940.\n",
      "Ep done - 25950.\n",
      "Ep done - 25960.\n",
      "Ep done - 25970.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 383         |\n",
      "|    time_elapsed         | 4754        |\n",
      "|    total_timesteps      | 784384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054253396 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.1         |\n",
      "-----------------------------------------\n",
      "Ep done - 25980.\n",
      "Ep done - 25990.\n",
      "Ep done - 26000.\n",
      "Ep done - 26010.\n",
      "Ep done - 26020.\n",
      "Ep done - 26030.\n",
      "Ep done - 26040.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 384         |\n",
      "|    time_elapsed         | 4763        |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047210276 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00412     |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 26050.\n",
      "Ep done - 26060.\n",
      "Ep done - 26070.\n",
      "Ep done - 26080.\n",
      "Ep done - 26090.\n",
      "Ep done - 26100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 4772        |\n",
      "|    total_timesteps      | 788480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038226776 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.394       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00913     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 26110.\n",
      "Ep done - 26120.\n",
      "Ep done - 26130.\n",
      "Ep done - 26140.\n",
      "Ep done - 26150.\n",
      "Ep done - 7810.\n",
      "Ep done - 7820.\n",
      "Ep done - 7830.\n",
      "Ep done - 7840.\n",
      "Ep done - 7850.\n",
      "Ep done - 7860.\n",
      "Ep done - 7870.\n",
      "Ep done - 7880.\n",
      "Ep done - 7890.\n",
      "Ep done - 7900.\n",
      "Eval num_timesteps=790000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.19 +/- 0.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 790000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04302481 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0273     |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 68\n",
      "Ep done - 26160.\n",
      "Ep done - 26170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.73     |\n",
      "| time/              |          |\n",
      "|    fps             | 164      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 4791     |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Ep done - 26180.\n",
      "Ep done - 26190.\n",
      "Ep done - 26200.\n",
      "Ep done - 26210.\n",
      "Ep done - 26220.\n",
      "Ep done - 26230.\n",
      "Ep done - 26240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 387        |\n",
      "|    time_elapsed         | 4800       |\n",
      "|    total_timesteps      | 792576     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05178717 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00752   |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.0944     |\n",
      "----------------------------------------\n",
      "Ep done - 26250.\n",
      "Ep done - 26260.\n",
      "Ep done - 26270.\n",
      "Ep done - 26280.\n",
      "Ep done - 26290.\n",
      "Ep done - 26300.\n",
      "Ep done - 26310.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 388        |\n",
      "|    time_elapsed         | 4809       |\n",
      "|    total_timesteps      | 794624     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04040174 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | 0.433      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00758    |\n",
      "|    n_updates            | 3870       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 26320.\n",
      "Ep done - 26330.\n",
      "Ep done - 26340.\n",
      "Ep done - 26350.\n",
      "Ep done - 26360.\n",
      "Ep done - 26370.\n",
      "Ep done - 26380.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 389         |\n",
      "|    time_elapsed         | 4819        |\n",
      "|    total_timesteps      | 796672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038781267 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0352      |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.0955      |\n",
      "-----------------------------------------\n",
      "Ep done - 26390.\n",
      "Ep done - 26400.\n",
      "Ep done - 26410.\n",
      "Ep done - 26420.\n",
      "Ep done - 26430.\n",
      "Ep done - 26440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 390         |\n",
      "|    time_elapsed         | 4828        |\n",
      "|    total_timesteps      | 798720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040669207 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 26450.\n",
      "Ep done - 26460.\n",
      "Ep done - 26470.\n",
      "Ep done - 26480.\n",
      "Ep done - 26490.\n",
      "Ep done - 7910.\n",
      "Ep done - 7920.\n",
      "Ep done - 7930.\n",
      "Ep done - 7940.\n",
      "Ep done - 7950.\n",
      "Ep done - 7960.\n",
      "Ep done - 7970.\n",
      "Ep done - 7980.\n",
      "Ep done - 7990.\n",
      "Ep done - 8000.\n",
      "Eval num_timesteps=800000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.14 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 800000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044370867 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0363      |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 69\n",
      "Ep done - 26500.\n",
      "Ep done - 26510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 4846     |\n",
      "|    total_timesteps | 800768   |\n",
      "---------------------------------\n",
      "Ep done - 26520.\n",
      "Ep done - 26530.\n",
      "Ep done - 26540.\n",
      "Ep done - 26550.\n",
      "Ep done - 26560.\n",
      "Ep done - 26570.\n",
      "Ep done - 26580.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 392         |\n",
      "|    time_elapsed         | 4855        |\n",
      "|    total_timesteps      | 802816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043309633 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00973     |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 26590.\n",
      "Ep done - 26600.\n",
      "Ep done - 26610.\n",
      "Ep done - 26620.\n",
      "Ep done - 26630.\n",
      "Ep done - 26640.\n",
      "Ep done - 26650.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 4865        |\n",
      "|    total_timesteps      | 804864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041192144 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 26660.\n",
      "Ep done - 26670.\n",
      "Ep done - 26680.\n",
      "Ep done - 26690.\n",
      "Ep done - 26700.\n",
      "Ep done - 26710.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 394         |\n",
      "|    time_elapsed         | 4874        |\n",
      "|    total_timesteps      | 806912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040266983 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0278      |\n",
      "|    n_updates            | 3930        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 26720.\n",
      "Ep done - 26730.\n",
      "Ep done - 26740.\n",
      "Ep done - 26750.\n",
      "Ep done - 26760.\n",
      "Ep done - 26770.\n",
      "Ep done - 26780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 395         |\n",
      "|    time_elapsed         | 4884        |\n",
      "|    total_timesteps      | 808960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041681416 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "Ep done - 26790.\n",
      "Ep done - 26800.\n",
      "Ep done - 26810.\n",
      "Ep done - 26820.\n",
      "Ep done - 8010.\n",
      "Ep done - 8020.\n",
      "Ep done - 8030.\n",
      "Ep done - 8040.\n",
      "Ep done - 8050.\n",
      "Ep done - 8060.\n",
      "Ep done - 8070.\n",
      "Ep done - 8080.\n",
      "Ep done - 8090.\n",
      "Ep done - 8100.\n",
      "Eval num_timesteps=810000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.08 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 810000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04001738 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.435      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 3950       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 70\n",
      "Ep done - 26830.\n",
      "Ep done - 26840.\n",
      "Ep done - 26850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 4902     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Ep done - 26860.\n",
      "Ep done - 26870.\n",
      "Ep done - 26880.\n",
      "Ep done - 26890.\n",
      "Ep done - 26900.\n",
      "Ep done - 26910.\n",
      "Ep done - 26920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 397         |\n",
      "|    time_elapsed         | 4911        |\n",
      "|    total_timesteps      | 813056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032377914 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 26930.\n",
      "Ep done - 26940.\n",
      "Ep done - 26950.\n",
      "Ep done - 26960.\n",
      "Ep done - 26970.\n",
      "Ep done - 26980.\n",
      "Ep done - 26990.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 4920        |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041726857 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0569      |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "Ep done - 27000.\n",
      "Ep done - 27010.\n",
      "Ep done - 27020.\n",
      "Ep done - 27030.\n",
      "Ep done - 27040.\n",
      "Ep done - 27050.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 399        |\n",
      "|    time_elapsed         | 4930       |\n",
      "|    total_timesteps      | 817152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03988068 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.192      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 0.0866     |\n",
      "----------------------------------------\n",
      "Ep done - 27060.\n",
      "Ep done - 27070.\n",
      "Ep done - 27080.\n",
      "Ep done - 27090.\n",
      "Ep done - 27100.\n",
      "Ep done - 27110.\n",
      "Ep done - 27120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 165         |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 4939        |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040207613 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 27130.\n",
      "Ep done - 27140.\n",
      "Ep done - 27150.\n",
      "Ep done - 8110.\n",
      "Ep done - 8120.\n",
      "Ep done - 8130.\n",
      "Ep done - 8140.\n",
      "Ep done - 8150.\n",
      "Ep done - 8160.\n",
      "Ep done - 8170.\n",
      "Ep done - 8180.\n",
      "Ep done - 8190.\n",
      "Ep done - 8200.\n",
      "Eval num_timesteps=820000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 30.22 +/- 0.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040978573 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.62\n",
      "SELFPLAY: new best model, bumping up generation to 71\n",
      "Ep done - 27160.\n",
      "Ep done - 27170.\n",
      "Ep done - 27180.\n",
      "Ep done - 27190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 4958     |\n",
      "|    total_timesteps | 821248   |\n",
      "---------------------------------\n",
      "Ep done - 27200.\n",
      "Ep done - 27210.\n",
      "Ep done - 27220.\n",
      "Ep done - 27230.\n",
      "Ep done - 27240.\n",
      "Ep done - 27250.\n",
      "Ep done - 27260.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 402        |\n",
      "|    time_elapsed         | 4967       |\n",
      "|    total_timesteps      | 823296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03806655 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 4010       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Ep done - 27270.\n",
      "Ep done - 27280.\n",
      "Ep done - 27290.\n",
      "Ep done - 27300.\n",
      "Ep done - 27310.\n",
      "Ep done - 27320.\n",
      "Ep done - 27330.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 403        |\n",
      "|    time_elapsed         | 4976       |\n",
      "|    total_timesteps      | 825344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04557497 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.247     |\n",
      "|    explained_variance   | 0.364      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0124     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 27340.\n",
      "Ep done - 27350.\n",
      "Ep done - 27360.\n",
      "Ep done - 27370.\n",
      "Ep done - 27380.\n",
      "Ep done - 27390.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 404        |\n",
      "|    time_elapsed         | 4986       |\n",
      "|    total_timesteps      | 827392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03469927 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.234     |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00498   |\n",
      "|    n_updates            | 4030       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 27400.\n",
      "Ep done - 27410.\n",
      "Ep done - 27420.\n",
      "Ep done - 27430.\n",
      "Ep done - 27440.\n",
      "Ep done - 27450.\n",
      "Ep done - 27460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 405        |\n",
      "|    time_elapsed         | 4995       |\n",
      "|    total_timesteps      | 829440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04118479 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.245     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 27470.\n",
      "Ep done - 27480.\n",
      "Ep done - 8210.\n",
      "Ep done - 8220.\n",
      "Ep done - 8230.\n",
      "Ep done - 8240.\n",
      "Ep done - 8250.\n",
      "Ep done - 8260.\n",
      "Ep done - 8270.\n",
      "Ep done - 8280.\n",
      "Ep done - 8290.\n",
      "Ep done - 8300.\n",
      "Eval num_timesteps=830000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.20 +/- 0.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039532688 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00189    |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.105       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 72\n",
      "Ep done - 27490.\n",
      "Ep done - 27500.\n",
      "Ep done - 27510.\n",
      "Ep done - 27520.\n",
      "Ep done - 27530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 165      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 5014     |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Ep done - 27540.\n",
      "Ep done - 27550.\n",
      "Ep done - 27560.\n",
      "Ep done - 27570.\n",
      "Ep done - 27580.\n",
      "Ep done - 27590.\n",
      "Ep done - 27600.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 165        |\n",
      "|    iterations           | 407        |\n",
      "|    time_elapsed         | 5023       |\n",
      "|    total_timesteps      | 833536     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03985147 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.236     |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0547     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 27610.\n",
      "Ep done - 27620.\n",
      "Ep done - 27630.\n",
      "Ep done - 27640.\n",
      "Ep done - 27650.\n",
      "Ep done - 27660.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 408        |\n",
      "|    time_elapsed         | 5032       |\n",
      "|    total_timesteps      | 835584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04159336 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.014      |\n",
      "|    n_updates            | 4070       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 27670.\n",
      "Ep done - 27680.\n",
      "Ep done - 27690.\n",
      "Ep done - 27700.\n",
      "Ep done - 27710.\n",
      "Ep done - 27720.\n",
      "Ep done - 27730.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 409        |\n",
      "|    time_elapsed         | 5041       |\n",
      "|    total_timesteps      | 837632     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04047399 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.0849     |\n",
      "----------------------------------------\n",
      "Ep done - 27740.\n",
      "Ep done - 27750.\n",
      "Ep done - 27760.\n",
      "Ep done - 27770.\n",
      "Ep done - 27780.\n",
      "Ep done - 27790.\n",
      "Ep done - 27800.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 410         |\n",
      "|    time_elapsed         | 5051        |\n",
      "|    total_timesteps      | 839680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043069396 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00683    |\n",
      "|    n_updates            | 4090        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.085       |\n",
      "-----------------------------------------\n",
      "Ep done - 27810.\n",
      "Ep done - 8310.\n",
      "Ep done - 8320.\n",
      "Ep done - 8330.\n",
      "Ep done - 8340.\n",
      "Ep done - 8350.\n",
      "Ep done - 8360.\n",
      "Ep done - 8370.\n",
      "Ep done - 8380.\n",
      "Ep done - 8390.\n",
      "Ep done - 8400.\n",
      "Eval num_timesteps=840000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.21 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 840000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045856565 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.106       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.72\n",
      "SELFPLAY: new best model, bumping up generation to 73\n",
      "Ep done - 27820.\n",
      "Ep done - 27830.\n",
      "Ep done - 27840.\n",
      "Ep done - 27850.\n",
      "Ep done - 27860.\n",
      "Ep done - 27870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 5070     |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Ep done - 27880.\n",
      "Ep done - 27890.\n",
      "Ep done - 27900.\n",
      "Ep done - 27910.\n",
      "Ep done - 27920.\n",
      "Ep done - 27930.\n",
      "Ep done - 27940.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 5079        |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044109784 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.242      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0066      |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 27950.\n",
      "Ep done - 27960.\n",
      "Ep done - 27970.\n",
      "Ep done - 27980.\n",
      "Ep done - 27990.\n",
      "Ep done - 28000.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.55      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 166       |\n",
      "|    iterations           | 413       |\n",
      "|    time_elapsed         | 5088      |\n",
      "|    total_timesteps      | 845824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0439839 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.244    |\n",
      "|    explained_variance   | 0.421     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0195    |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -0.0229   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Ep done - 28010.\n",
      "Ep done - 28020.\n",
      "Ep done - 28030.\n",
      "Ep done - 28040.\n",
      "Ep done - 28050.\n",
      "Ep done - 28060.\n",
      "Ep done - 28070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 414         |\n",
      "|    time_elapsed         | 5098        |\n",
      "|    total_timesteps      | 847872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045566168 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0578      |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 28080.\n",
      "Ep done - 28090.\n",
      "Ep done - 28100.\n",
      "Ep done - 28110.\n",
      "Ep done - 28120.\n",
      "Ep done - 28130.\n",
      "Ep done - 28140.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.45       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 415        |\n",
      "|    time_elapsed         | 5107       |\n",
      "|    total_timesteps      | 849920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03962121 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00671   |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "Ep done - 8410.\n",
      "Ep done - 8420.\n",
      "Ep done - 8430.\n",
      "Ep done - 8440.\n",
      "Ep done - 8450.\n",
      "Ep done - 8460.\n",
      "Ep done - 8470.\n",
      "Ep done - 8480.\n",
      "Ep done - 8490.\n",
      "Ep done - 8500.\n",
      "Eval num_timesteps=850000, episode_reward=0.71 +/- 0.70\n",
      "Episode length: 30.20 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 850000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03723972 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0434     |\n",
      "|    n_updates            | 4150       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.71\n",
      "SELFPLAY: new best model, bumping up generation to 74\n",
      "Ep done - 28150.\n",
      "Ep done - 28160.\n",
      "Ep done - 28170.\n",
      "Ep done - 28180.\n",
      "Ep done - 28190.\n",
      "Ep done - 28200.\n",
      "Ep done - 28210.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 5126     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Ep done - 28220.\n",
      "Ep done - 28230.\n",
      "Ep done - 28240.\n",
      "Ep done - 28250.\n",
      "Ep done - 28260.\n",
      "Ep done - 28270.\n",
      "Ep done - 28280.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 5135        |\n",
      "|    total_timesteps      | 854016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049795844 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 28290.\n",
      "Ep done - 28300.\n",
      "Ep done - 28310.\n",
      "Ep done - 28320.\n",
      "Ep done - 28330.\n",
      "Ep done - 28340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 5144        |\n",
      "|    total_timesteps      | 856064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054085612 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 28350.\n",
      "Ep done - 28360.\n",
      "Ep done - 28370.\n",
      "Ep done - 28380.\n",
      "Ep done - 28390.\n",
      "Ep done - 28400.\n",
      "Ep done - 28410.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 419        |\n",
      "|    time_elapsed         | 5153       |\n",
      "|    total_timesteps      | 858112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03861413 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.209     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 28420.\n",
      "Ep done - 28430.\n",
      "Ep done - 28440.\n",
      "Ep done - 28450.\n",
      "Ep done - 28460.\n",
      "Ep done - 28470.\n",
      "Ep done - 8510.\n",
      "Ep done - 8520.\n",
      "Ep done - 8530.\n",
      "Ep done - 8540.\n",
      "Ep done - 8550.\n",
      "Ep done - 8560.\n",
      "Ep done - 8570.\n",
      "Ep done - 8580.\n",
      "Ep done - 8590.\n",
      "Ep done - 8600.\n",
      "Eval num_timesteps=860000, episode_reward=0.61 +/- 0.79\n",
      "Episode length: 30.25 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047041457 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 75\n",
      "Ep done - 28480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 5172     |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Ep done - 28490.\n",
      "Ep done - 28500.\n",
      "Ep done - 28510.\n",
      "Ep done - 28520.\n",
      "Ep done - 28530.\n",
      "Ep done - 28540.\n",
      "Ep done - 28550.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 421         |\n",
      "|    time_elapsed         | 5182        |\n",
      "|    total_timesteps      | 862208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038588893 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.401       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.153       |\n",
      "-----------------------------------------\n",
      "Ep done - 28560.\n",
      "Ep done - 28570.\n",
      "Ep done - 28580.\n",
      "Ep done - 28590.\n",
      "Ep done - 28600.\n",
      "Ep done - 28610.\n",
      "Ep done - 28620.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 5191        |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039061025 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0376      |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 28630.\n",
      "Ep done - 28640.\n",
      "Ep done - 28650.\n",
      "Ep done - 28660.\n",
      "Ep done - 28670.\n",
      "Ep done - 28680.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 423         |\n",
      "|    time_elapsed         | 5200        |\n",
      "|    total_timesteps      | 866304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044100247 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0148      |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 28690.\n",
      "Ep done - 28700.\n",
      "Ep done - 28710.\n",
      "Ep done - 28720.\n",
      "Ep done - 28730.\n",
      "Ep done - 28740.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 28750.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.71       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 424        |\n",
      "|    time_elapsed         | 5209       |\n",
      "|    total_timesteps      | 868352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04622655 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.22      |\n",
      "|    explained_variance   | 0.209      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00651   |\n",
      "|    n_updates            | 4230       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 28760.\n",
      "Ep done - 28770.\n",
      "Ep done - 28780.\n",
      "Ep done - 28790.\n",
      "Ep done - 28800.\n",
      "Ep done - 28810.\n",
      "Ep done - 8610.\n",
      "Ep done - 8620.\n",
      "Ep done - 8630.\n",
      "Ep done - 8640.\n",
      "Ep done - 8650.\n",
      "Ep done - 8660.\n",
      "Ep done - 8670.\n",
      "Ep done - 8680.\n",
      "Ep done - 8690.\n",
      "Ep done - 8700.\n",
      "Eval num_timesteps=870000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.24 +/- 0.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 870000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04074339 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.221     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0565     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.63\n",
      "SELFPLAY: new best model, bumping up generation to 76\n",
      "Ep done - 28820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 5228     |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n",
      "Ep done - 28830.\n",
      "Ep done - 28840.\n",
      "Ep done - 28850.\n",
      "Ep done - 28860.\n",
      "Ep done - 28870.\n",
      "Ep done - 28880.\n",
      "Ep done - 28890.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 426         |\n",
      "|    time_elapsed         | 5237        |\n",
      "|    total_timesteps      | 872448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039481863 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 4250        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.0939      |\n",
      "-----------------------------------------\n",
      "Ep done - 28900.\n",
      "Ep done - 28910.\n",
      "Ep done - 28920.\n",
      "Ep done - 28930.\n",
      "Ep done - 28940.\n",
      "Ep done - 28950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 427        |\n",
      "|    time_elapsed         | 5247       |\n",
      "|    total_timesteps      | 874496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04343249 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.207     |\n",
      "|    explained_variance   | 0.443      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0617     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "Ep done - 28960.\n",
      "Ep done - 28970.\n",
      "Ep done - 28980.\n",
      "Ep done - 28990.\n",
      "Ep done - 29000.\n",
      "Ep done - 29010.\n",
      "Ep done - 29020.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 428        |\n",
      "|    time_elapsed         | 5256       |\n",
      "|    total_timesteps      | 876544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04471367 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.212     |\n",
      "|    explained_variance   | 0.509      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0029     |\n",
      "|    n_updates            | 4270       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    value_loss           | 0.0947     |\n",
      "----------------------------------------\n",
      "Ep done - 29030.\n",
      "Ep done - 29040.\n",
      "Ep done - 29050.\n",
      "Ep done - 29060.\n",
      "Ep done - 29070.\n",
      "Ep done - 29080.\n",
      "Ep done - 29090.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 429        |\n",
      "|    time_elapsed         | 5265       |\n",
      "|    total_timesteps      | 878592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04288338 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.206     |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0575     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "Ep done - 29100.\n",
      "Ep done - 29110.\n",
      "Ep done - 29120.\n",
      "Ep done - 29130.\n",
      "Ep done - 29140.\n",
      "Ep done - 8710.\n",
      "Ep done - 8720.\n",
      "Ep done - 8730.\n",
      "Ep done - 8740.\n",
      "Ep done - 8750.\n",
      "Ep done - 8760.\n",
      "Ep done - 8770.\n",
      "Ep done - 8780.\n",
      "Ep done - 8790.\n",
      "Ep done - 8800.\n",
      "Eval num_timesteps=880000, episode_reward=0.48 +/- 0.88\n",
      "Episode length: 30.08 +/- 0.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 880000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05353629 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.184     |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 4290       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 77\n",
      "Ep done - 29150.\n",
      "Ep done - 29160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 0.45     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 5284     |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Ep done - 29170.\n",
      "Ep done - 29180.\n",
      "Ep done - 29190.\n",
      "Ep done - 29200.\n",
      "Ep done - 29210.\n",
      "Ep done - 29220.\n",
      "Ep done - 29230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.1        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 431         |\n",
      "|    time_elapsed         | 5293        |\n",
      "|    total_timesteps      | 882688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055607487 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 4300        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 29240.\n",
      "Ep done - 29250.\n",
      "Ep done - 29260.\n",
      "Ep done - 29270.\n",
      "Ep done - 29280.\n",
      "Ep done - 29290.\n",
      "Ep done - 29300.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.6        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 432         |\n",
      "|    time_elapsed         | 5302        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037188195 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 4310        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "Ep done - 29310.\n",
      "Ep done - 29320.\n",
      "Ep done - 29330.\n",
      "Ep done - 29340.\n",
      "Ep done - 29350.\n",
      "Ep done - 29360.\n",
      "Ep done - 29370.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 28.8       |\n",
      "|    ep_rew_mean          | 0.35       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 433        |\n",
      "|    time_elapsed         | 5312       |\n",
      "|    total_timesteps      | 886784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04685136 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0387     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "Ep done - 29380.\n",
      "Ep done - 29390.\n",
      "Ep done - 29400.\n",
      "Ep done - 29410.\n",
      "Ep done - 29420.\n",
      "Ep done - 29430.\n",
      "Ep done - 29440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 28.7        |\n",
      "|    ep_rew_mean          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 434         |\n",
      "|    time_elapsed         | 5321        |\n",
      "|    total_timesteps      | 888832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037771262 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0609      |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 29450.\n",
      "Ep done - 29460.\n",
      "Ep done - 29470.\n",
      "Ep done - 29480.\n",
      "Ep done - 8810.\n",
      "Ep done - 8820.\n",
      "Ep done - 8830.\n",
      "Ep done - 8840.\n",
      "Ep done - 8850.\n",
      "Ep done - 8860.\n",
      "Ep done - 8870.\n",
      "Ep done - 8880.\n",
      "Ep done - 8890.\n",
      "Ep done - 8900.\n",
      "Eval num_timesteps=890000, episode_reward=0.49 +/- 0.87\n",
      "Episode length: 29.42 +/- 3.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.4       |\n",
      "|    mean_reward          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 890000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05774898 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.202     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.49\n",
      "SELFPLAY: new best model, bumping up generation to 78\n",
      "Ep done - 29490.\n",
      "Ep done - 29500.\n",
      "Ep done - 29510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.6     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 166      |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 5340     |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Ep done - 29520.\n",
      "Ep done - 29530.\n",
      "Ep done - 29540.\n",
      "Ep done - 29550.\n",
      "Ep done - 29560.\n",
      "Ep done - 29570.\n",
      "Ep done - 29580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 166        |\n",
      "|    iterations           | 436        |\n",
      "|    time_elapsed         | 5349       |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05291777 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.222     |\n",
      "|    explained_variance   | 0.343      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00793   |\n",
      "|    n_updates            | 4350       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "Ep done - 29590.\n",
      "Ep done - 29600.\n",
      "Ep done - 29610.\n",
      "Ep done - 29620.\n",
      "Ep done - 29630.\n",
      "Ep done - 29640.\n",
      "Ep done - 29650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 437         |\n",
      "|    time_elapsed         | 5358        |\n",
      "|    total_timesteps      | 894976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033090267 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 29660.\n",
      "Ep done - 29670.\n",
      "Ep done - 29680.\n",
      "Ep done - 29690.\n",
      "Ep done - 29700.\n",
      "Ep done - 29710.\n",
      "Ep done - 29720.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.5       |\n",
      "|    ep_rew_mean          | 0.4        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 438        |\n",
      "|    time_elapsed         | 5367       |\n",
      "|    total_timesteps      | 897024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03390434 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.471      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 4370       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Ep done - 29730.\n",
      "Ep done - 29740.\n",
      "Ep done - 29750.\n",
      "Ep done - 29760.\n",
      "Ep done - 29770.\n",
      "Ep done - 29780.\n",
      "Ep done - 29790.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.4        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 5377        |\n",
      "|    total_timesteps      | 899072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045801118 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.209      |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0464      |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 29800.\n",
      "Ep done - 29810.\n",
      "Ep done - 29820.\n",
      "Ep done - 8910.\n",
      "Ep done - 8920.\n",
      "Ep done - 8930.\n",
      "Ep done - 8940.\n",
      "Ep done - 8950.\n",
      "Ep done - 8960.\n",
      "Ep done - 8970.\n",
      "Ep done - 8980.\n",
      "Ep done - 8990.\n",
      "Ep done - 9000.\n",
      "Eval num_timesteps=900000, episode_reward=0.57 +/- 0.82\n",
      "Episode length: 28.61 +/- 5.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28.6        |\n",
      "|    mean_reward          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044796735 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.214      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0423      |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.57\n",
      "SELFPLAY: new best model, bumping up generation to 79\n",
      "Ep done - 29830.\n",
      "Ep done - 29840.\n",
      "Ep done - 29850.\n",
      "Ep done - 29860.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 5395     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Ep done - 29870.\n",
      "Ep done - 29880.\n",
      "Ep done - 29890.\n",
      "Ep done - 29900.\n",
      "Ep done - 29910.\n",
      "Ep done - 29920.\n",
      "Ep done - 29930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 441        |\n",
      "|    time_elapsed         | 5404       |\n",
      "|    total_timesteps      | 903168     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05211205 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.212     |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00763   |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 0.116      |\n",
      "----------------------------------------\n",
      "Ep done - 29940.\n",
      "Ep done - 29950.\n",
      "Ep done - 29960.\n",
      "Ep done - 29970.\n",
      "Ep done - 29980.\n",
      "Ep done - 29990.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 5413        |\n",
      "|    total_timesteps      | 905216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047195964 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | 0.312       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Ep done - 30000.\n",
      "Ep done - 30010.\n",
      "Ep done - 30020.\n",
      "Ep done - 30030.\n",
      "Ep done - 30040.\n",
      "Ep done - 30050.\n",
      "Ep done - 30060.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 443         |\n",
      "|    time_elapsed         | 5423        |\n",
      "|    total_timesteps      | 907264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058931828 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.468       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 30070.\n",
      "Ep done - 30080.\n",
      "Ep done - 30090.\n",
      "Ep done - 30100.\n",
      "Ep done - 30110.\n",
      "Ep done - 30120.\n",
      "Ep done - 30130.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.29       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 444        |\n",
      "|    time_elapsed         | 5432       |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04344323 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.246     |\n",
      "|    explained_variance   | 0.519      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00732   |\n",
      "|    n_updates            | 4430       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "Ep done - 30140.\n",
      "Ep done - 30150.\n",
      "Ep done - 9010.\n",
      "Ep done - 9020.\n",
      "Ep done - 9030.\n",
      "Ep done - 9040.\n",
      "Ep done - 9050.\n",
      "Ep done - 9060.\n",
      "Ep done - 9070.\n",
      "Ep done - 9080.\n",
      "Ep done - 9090.\n",
      "Ep done - 9100.\n",
      "Eval num_timesteps=910000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.18 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 910000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04347971 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.217     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0685     |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 80\n",
      "Ep done - 30160.\n",
      "Ep done - 30170.\n",
      "Ep done - 30180.\n",
      "Ep done - 30190.\n",
      "Ep done - 30200.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 5451     |\n",
      "|    total_timesteps | 911360   |\n",
      "---------------------------------\n",
      "Ep done - 30210.\n",
      "Ep done - 30220.\n",
      "Ep done - 30230.\n",
      "Ep done - 30240.\n",
      "Ep done - 30250.\n",
      "Ep done - 30260.\n",
      "Ep done - 30270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 446         |\n",
      "|    time_elapsed         | 5460        |\n",
      "|    total_timesteps      | 913408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037328564 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00244    |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Ep done - 30280.\n",
      "Ep done - 30290.\n",
      "Ep done - 30300.\n",
      "Ep done - 30310.\n",
      "Ep done - 30320.\n",
      "Ep done - 30330.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.13       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 447        |\n",
      "|    time_elapsed         | 5470       |\n",
      "|    total_timesteps      | 915456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04533232 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.246     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 30340.\n",
      "Ep done - 30350.\n",
      "Ep done - 30360.\n",
      "Ep done - 30370.\n",
      "Ep done - 30380.\n",
      "Ep done - 30390.\n",
      "Ep done - 30400.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 448        |\n",
      "|    time_elapsed         | 5479       |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03981182 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.255     |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 4470       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Ep done - 30410.\n",
      "Ep done - 30420.\n",
      "Ep done - 30430.\n",
      "Ep done - 30440.\n",
      "Ep done - 30450.\n",
      "Ep done - 30460.\n",
      "Ep done - 30470.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.35        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 449         |\n",
      "|    time_elapsed         | 5488        |\n",
      "|    total_timesteps      | 919552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043232292 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.154       |\n",
      "-----------------------------------------\n",
      "Ep done - 30480.\n",
      "Ep done - 30490.\n",
      "Ep done - 9110.\n",
      "Ep done - 9120.\n",
      "Ep done - 9130.\n",
      "Ep done - 9140.\n",
      "Ep done - 9150.\n",
      "Ep done - 9160.\n",
      "Ep done - 9170.\n",
      "Ep done - 9180.\n",
      "Ep done - 9190.\n",
      "Ep done - 9200.\n",
      "Eval num_timesteps=920000, episode_reward=0.26 +/- 0.96\n",
      "Episode length: 30.18 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.26        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044422932 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 0.2         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.26\n",
      "SELFPLAY: new best model, bumping up generation to 81\n",
      "Ep done - 30500.\n",
      "Ep done - 30510.\n",
      "Ep done - 30520.\n",
      "Ep done - 30530.\n",
      "Ep done - 30540.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.41     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 5507     |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Ep done - 30550.\n",
      "Ep done - 30560.\n",
      "Ep done - 30570.\n",
      "Ep done - 30580.\n",
      "Ep done - 30590.\n",
      "Ep done - 30600.\n",
      "Ep done - 30610.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.36       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 451        |\n",
      "|    time_elapsed         | 5516       |\n",
      "|    total_timesteps      | 923648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05373078 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.229     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "Ep done - 30620.\n",
      "Ep done - 30630.\n",
      "Ep done - 30640.\n",
      "Ep done - 30650.\n",
      "Ep done - 30660.\n",
      "Ep done - 30670.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 452        |\n",
      "|    time_elapsed         | 5525       |\n",
      "|    total_timesteps      | 925696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05445145 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.233     |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0285     |\n",
      "|    n_updates            | 4510       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.184      |\n",
      "----------------------------------------\n",
      "Ep done - 30680.\n",
      "Ep done - 30690.\n",
      "Ep done - 30700.\n",
      "Ep done - 30710.\n",
      "Ep done - 30720.\n",
      "Ep done - 30730.\n",
      "Ep done - 30740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.8       |\n",
      "|    ep_rew_mean          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 453        |\n",
      "|    time_elapsed         | 5535       |\n",
      "|    total_timesteps      | 927744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04490687 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Ep done - 30750.\n",
      "Ep done - 30760.\n",
      "Ep done - 30770.\n",
      "Ep done - 30780.\n",
      "Ep done - 30790.\n",
      "Ep done - 30800.\n",
      "Ep done - 30810.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 454         |\n",
      "|    time_elapsed         | 5544        |\n",
      "|    total_timesteps      | 929792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050227266 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0547      |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "Ep done - 30820.\n",
      "Ep done - 9210.\n",
      "Ep done - 9220.\n",
      "Ep done - 9230.\n",
      "Ep done - 9240.\n",
      "Ep done - 9250.\n",
      "Ep done - 9260.\n",
      "Ep done - 9270.\n",
      "Ep done - 9280.\n",
      "Ep done - 9290.\n",
      "Ep done - 9300.\n",
      "Eval num_timesteps=930000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 29.95 +/- 2.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.9        |\n",
      "|    mean_reward          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044989277 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    value_loss           | 0.205       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.39\n",
      "SELFPLAY: new best model, bumping up generation to 82\n",
      "Ep done - 30830.\n",
      "Ep done - 30840.\n",
      "Ep done - 30850.\n",
      "Ep done - 30860.\n",
      "Ep done - 30870.\n",
      "Ep done - 30880.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 5562     |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Ep done - 30890.\n",
      "Ep done - 30900.\n",
      "Ep done - 30910.\n",
      "Ep done - 30920.\n",
      "Ep done - 30930.\n",
      "Ep done - 30940.\n",
      "Ep done - 30950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.6       |\n",
      "|    ep_rew_mean          | 0.34       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 456        |\n",
      "|    time_elapsed         | 5572       |\n",
      "|    total_timesteps      | 933888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04567308 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Ep done - 30960.\n",
      "Ep done - 30970.\n",
      "Ep done - 30980.\n",
      "Ep done - 30990.\n",
      "Ep done - 31000.\n",
      "Ep done - 31010.\n",
      "Ep done - 31020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 29.8        |\n",
      "|    ep_rew_mean          | 0.37        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 457         |\n",
      "|    time_elapsed         | 5581        |\n",
      "|    total_timesteps      | 935936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044257864 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.24       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 0.195       |\n",
      "-----------------------------------------\n",
      "Ep done - 31030.\n",
      "Ep done - 31040.\n",
      "Ep done - 31050.\n",
      "Ep done - 31060.\n",
      "Ep done - 31070.\n",
      "Ep done - 31080.\n",
      "Ep done - 31090.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.38        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 5590        |\n",
      "|    total_timesteps      | 937984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038006626 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 31100.\n",
      "Ep done - 31110.\n",
      "Ep done - 31120.\n",
      "Ep done - 31130.\n",
      "Ep done - 31140.\n",
      "Ep done - 31150.\n",
      "Ep done - 9310.\n",
      "Ep done - 9320.\n",
      "Ep done - 9330.\n",
      "Ep done - 9340.\n",
      "Ep done - 9350.\n",
      "Ep done - 9360.\n",
      "Ep done - 9370.\n",
      "Ep done - 9380.\n",
      "Ep done - 9390.\n",
      "Ep done - 9400.\n",
      "Eval num_timesteps=940000, episode_reward=0.43 +/- 0.90\n",
      "Episode length: 30.16 +/- 0.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039132167 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0205      |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.175       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.43\n",
      "SELFPLAY: new best model, bumping up generation to 83\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.39     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 5609     |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Ep done - 31160.\n",
      "Ep done - 31170.\n",
      "Ep done - 31180.\n",
      "Ep done - 31190.\n",
      "Ep done - 31200.\n",
      "Ep done - 31210.\n",
      "Ep done - 31220.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 5618        |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042460565 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.0264     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 31230.\n",
      "Ep done - 31240.\n",
      "Ep done - 31250.\n",
      "Ep done - 31260.\n",
      "Ep done - 31270.\n",
      "Ep done - 31280.\n",
      "Ep done - 31290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 461         |\n",
      "|    time_elapsed         | 5627        |\n",
      "|    total_timesteps      | 944128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039197117 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Ep done - 31300.\n",
      "Ep done - 31310.\n",
      "Ep done - 31320.\n",
      "Ep done - 31330.\n",
      "Ep done - 31340.\n",
      "Ep done - 31350.\n",
      "Ep done - 31360.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 462         |\n",
      "|    time_elapsed         | 5636        |\n",
      "|    total_timesteps      | 946176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052901983 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0343      |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.177       |\n",
      "-----------------------------------------\n",
      "Ep done - 31370.\n",
      "Ep done - 31380.\n",
      "Ep done - 31390.\n",
      "Ep done - 31400.\n",
      "Ep done - 31410.\n",
      "Ep done - 31420.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 31430.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 463        |\n",
      "|    time_elapsed         | 5646       |\n",
      "|    total_timesteps      | 948224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03817923 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0538     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "Ep done - 31440.\n",
      "Ep done - 31450.\n",
      "Ep done - 31460.\n",
      "Ep done - 31470.\n",
      "Ep done - 31480.\n",
      "Ep done - 9410.\n",
      "Ep done - 9420.\n",
      "Ep done - 9430.\n",
      "Ep done - 9440.\n",
      "Ep done - 9450.\n",
      "Ep done - 9460.\n",
      "Ep done - 9470.\n",
      "Ep done - 9480.\n",
      "Ep done - 9490.\n",
      "Ep done - 9500.\n",
      "Eval num_timesteps=950000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.15 +/- 0.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.44       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 950000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04305911 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.326      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0135    |\n",
      "|    n_updates            | 4630       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 84\n",
      "Ep done - 31490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 5664     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Ep done - 31500.\n",
      "Ep done - 31510.\n",
      "Ep done - 31520.\n",
      "Ep done - 31530.\n",
      "Ep done - 31540.\n",
      "Ep done - 31550.\n",
      "Ep done - 31560.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 465        |\n",
      "|    time_elapsed         | 5674       |\n",
      "|    total_timesteps      | 952320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03075062 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.247     |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0619     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "Ep done - 31570.\n",
      "Ep done - 31580.\n",
      "Ep done - 31590.\n",
      "Ep done - 31600.\n",
      "Ep done - 31610.\n",
      "Ep done - 31620.\n",
      "Ep done - 31630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.57        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 167         |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 5683        |\n",
      "|    total_timesteps      | 954368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051397655 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.264      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0173      |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.165       |\n",
      "-----------------------------------------\n",
      "Ep done - 31640.\n",
      "Ep done - 31650.\n",
      "Ep done - 31660.\n",
      "Ep done - 31670.\n",
      "Ep done - 31680.\n",
      "Ep done - 31690.\n",
      "Ep done - 31700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 467         |\n",
      "|    time_elapsed         | 5692        |\n",
      "|    total_timesteps      | 956416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051111907 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.356       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "Ep done - 31710.\n",
      "Ep done - 31720.\n",
      "Ep done - 31730.\n",
      "Ep done - 31740.\n",
      "Ep done - 31750.\n",
      "Ep done - 31760.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.47       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 468        |\n",
      "|    time_elapsed         | 5701       |\n",
      "|    total_timesteps      | 958464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05088955 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.254     |\n",
      "|    explained_variance   | 0.393      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000478   |\n",
      "|    n_updates            | 4670       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Ep done - 31770.\n",
      "Ep done - 31780.\n",
      "Ep done - 31790.\n",
      "Ep done - 31800.\n",
      "Ep done - 31810.\n",
      "Ep done - 31820.\n",
      "Ep done - 9510.\n",
      "Ep done - 9520.\n",
      "Ep done - 9530.\n",
      "Ep done - 9540.\n",
      "Ep done - 9550.\n",
      "Ep done - 9560.\n",
      "Ep done - 9570.\n",
      "Ep done - 9580.\n",
      "Ep done - 9590.\n",
      "Ep done - 9600.\n",
      "Eval num_timesteps=960000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.23 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034435693 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0964      |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.53\n",
      "SELFPLAY: new best model, bumping up generation to 85\n",
      "Ep done - 31830.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 0.43     |\n",
      "| time/              |          |\n",
      "|    fps             | 167      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 5720     |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "Ep done - 31840.\n",
      "Ep done - 31850.\n",
      "Ep done - 31860.\n",
      "Ep done - 31870.\n",
      "Ep done - 31880.\n",
      "Ep done - 31890.\n",
      "Ep done - 31900.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.4       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 167        |\n",
      "|    iterations           | 470        |\n",
      "|    time_elapsed         | 5729       |\n",
      "|    total_timesteps      | 962560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03837693 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.251     |\n",
      "|    explained_variance   | 0.342      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0531     |\n",
      "|    n_updates            | 4690       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 31910.\n",
      "Ep done - 31920.\n",
      "Ep done - 31930.\n",
      "Ep done - 31940.\n",
      "Ep done - 31950.\n",
      "Ep done - 31960.\n",
      "Ep done - 31970.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 471        |\n",
      "|    time_elapsed         | 5739       |\n",
      "|    total_timesteps      | 964608     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04042045 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Ep done - 31980.\n",
      "Ep done - 31990.\n",
      "Ep done - 32000.\n",
      "Ep done - 32010.\n",
      "Ep done - 32020.\n",
      "Ep done - 32030.\n",
      "Ep done - 32040.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 472        |\n",
      "|    time_elapsed         | 5748       |\n",
      "|    total_timesteps      | 966656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03465868 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.312      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.038      |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 32050.\n",
      "Ep done - 32060.\n",
      "Ep done - 32070.\n",
      "Ep done - 32080.\n",
      "Ep done - 32090.\n",
      "Ep done - 32100.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.49       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 473        |\n",
      "|    time_elapsed         | 5757       |\n",
      "|    total_timesteps      | 968704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04198975 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.013      |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Ep done - 32110.\n",
      "Ep done - 32120.\n",
      "Ep done - 32130.\n",
      "Ep done - 32140.\n",
      "Ep done - 32150.\n",
      "Ep done - 9610.\n",
      "Ep done - 9620.\n",
      "Ep done - 9630.\n",
      "Ep done - 9640.\n",
      "Ep done - 9650.\n",
      "Ep done - 9660.\n",
      "Ep done - 9670.\n",
      "Ep done - 9680.\n",
      "Ep done - 9690.\n",
      "Ep done - 9700.\n",
      "Eval num_timesteps=970000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.24 +/- 0.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 970000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03893646 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.276     |\n",
      "|    explained_variance   | 0.378      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0102     |\n",
      "|    n_updates            | 4730       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 86\n",
      "Ep done - 32160.\n",
      "Ep done - 32170.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 5776     |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Ep done - 32180.\n",
      "Ep done - 32190.\n",
      "Ep done - 32200.\n",
      "Ep done - 32210.\n",
      "Ep done - 32220.\n",
      "Ep done - 32230.\n",
      "Ep done - 32240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 475        |\n",
      "|    time_elapsed         | 5785       |\n",
      "|    total_timesteps      | 972800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05214984 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.322      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Ep done - 32250.\n",
      "Ep done - 32260.\n",
      "Ep done - 32270.\n",
      "Ep done - 32280.\n",
      "Ep done - 32290.\n",
      "Ep done - 32300.\n",
      "Ep done - 32310.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.5        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 476        |\n",
      "|    time_elapsed         | 5794       |\n",
      "|    total_timesteps      | 974848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03271644 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.245     |\n",
      "|    explained_variance   | 0.348      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Ep done - 32320.\n",
      "Ep done - 32330.\n",
      "Ep done - 32340.\n",
      "Ep done - 32350.\n",
      "Ep done - 32360.\n",
      "Ep done - 32370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 477         |\n",
      "|    time_elapsed         | 5804        |\n",
      "|    total_timesteps      | 976896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028175043 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00116    |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 32380.\n",
      "Ep done - 32390.\n",
      "Ep done - 32400.\n",
      "Ep done - 32410.\n",
      "Ep done - 32420.\n",
      "Ep done - 32430.\n",
      "Ep done - 32440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 478         |\n",
      "|    time_elapsed         | 5813        |\n",
      "|    total_timesteps      | 978944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041862298 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0289      |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 32450.\n",
      "Ep done - 32460.\n",
      "Ep done - 32470.\n",
      "Ep done - 32480.\n",
      "Ep done - 9710.\n",
      "Ep done - 9720.\n",
      "Ep done - 9730.\n",
      "Ep done - 9740.\n",
      "Ep done - 9750.\n",
      "Ep done - 9760.\n",
      "Ep done - 9770.\n",
      "Ep done - 9780.\n",
      "Ep done - 9790.\n",
      "Ep done - 9800.\n",
      "Eval num_timesteps=980000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.17 +/- 0.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045215447 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.241      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.58\n",
      "SELFPLAY: new best model, bumping up generation to 87\n",
      "Ep done - 32490.\n",
      "Ep done - 32500.\n",
      "Ep done - 32510.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 5831     |\n",
      "|    total_timesteps | 980992   |\n",
      "---------------------------------\n",
      "Ep done - 32520.\n",
      "Ep done - 32530.\n",
      "Ep done - 32540.\n",
      "Ep done - 32550.\n",
      "Ep done - 32560.\n",
      "Ep done - 32570.\n",
      "Ep done - 32580.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.53       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 480        |\n",
      "|    time_elapsed         | 5841       |\n",
      "|    total_timesteps      | 983040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03914181 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.255     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 4790       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 32590.\n",
      "Ep done - 32600.\n",
      "Ep done - 32610.\n",
      "Ep done - 32620.\n",
      "Ep done - 32630.\n",
      "Ep done - 32640.\n",
      "Ep done - 32650.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 481        |\n",
      "|    time_elapsed         | 5850       |\n",
      "|    total_timesteps      | 985088     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03628048 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.25      |\n",
      "|    explained_variance   | 0.359      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0302     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 32660.\n",
      "Ep done - 32670.\n",
      "Ep done - 32680.\n",
      "Ep done - 32690.\n",
      "Ep done - 32700.\n",
      "Ep done - 32710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 482        |\n",
      "|    time_elapsed         | 5859       |\n",
      "|    total_timesteps      | 987136     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04125423 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 4810       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "Ep done - 32720.\n",
      "Ep done - 32730.\n",
      "Ep done - 32740.\n",
      "Ep done - 32750.\n",
      "Ep done - 32760.\n",
      "Ep done - 32770.\n",
      "Ep done - 32780.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 483         |\n",
      "|    time_elapsed         | 5868        |\n",
      "|    total_timesteps      | 989184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039338924 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0224      |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.0283     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Ep done - 32790.\n",
      "Ep done - 32800.\n",
      "Ep done - 32810.\n",
      "Ep done - 9810.\n",
      "Ep done - 9820.\n",
      "Ep done - 9830.\n",
      "Ep done - 9840.\n",
      "Ep done - 9850.\n",
      "Ep done - 9860.\n",
      "Ep done - 9870.\n",
      "Ep done - 9880.\n",
      "Ep done - 9890.\n",
      "Ep done - 9900.\n",
      "Eval num_timesteps=990000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 30.30 +/- 0.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.3       |\n",
      "|    mean_reward          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 990000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04882084 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 4830       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.59\n",
      "SELFPLAY: new best model, bumping up generation to 88\n",
      "Ep done - 32820.\n",
      "Ep done - 32830.\n",
      "Ep done - 32840.\n",
      "Ep done - 32850.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.56     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 5887     |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Ep done - 32860.\n",
      "Ep done - 32870.\n",
      "Ep done - 32880.\n",
      "Ep done - 32890.\n",
      "Ep done - 32900.\n",
      "Ep done - 32910.\n",
      "Ep done - 32920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.39        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 485         |\n",
      "|    time_elapsed         | 5896        |\n",
      "|    total_timesteps      | 993280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037961617 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0207      |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 32930.\n",
      "Ep done - 32940.\n",
      "Ep done - 32950.\n",
      "Ep done - 32960.\n",
      "Ep done - 32970.\n",
      "Ep done - 32980.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 486        |\n",
      "|    time_elapsed         | 5905       |\n",
      "|    total_timesteps      | 995328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04075996 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.267     |\n",
      "|    explained_variance   | 0.261      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0638     |\n",
      "|    n_updates            | 4850       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.188      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 32990.\n",
      "Ep done - 33000.\n",
      "Ep done - 33010.\n",
      "Ep done - 33020.\n",
      "Ep done - 33030.\n",
      "Ep done - 33040.\n",
      "Ep done - 33050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 487         |\n",
      "|    time_elapsed         | 5915        |\n",
      "|    total_timesteps      | 997376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039672844 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Ep done - 33060.\n",
      "Ep done - 33070.\n",
      "Ep done - 33080.\n",
      "Ep done - 33090.\n",
      "Ep done - 33100.\n",
      "Ep done - 33110.\n",
      "Ep done - 33120.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 5924        |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046096794 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0344      |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 0.186       |\n",
      "-----------------------------------------\n",
      "Ep done - 33130.\n",
      "Ep done - 33140.\n",
      "Ep done - 9910.\n",
      "Ep done - 9920.\n",
      "Ep done - 9930.\n",
      "Ep done - 9940.\n",
      "Ep done - 9950.\n",
      "Ep done - 9960.\n",
      "Ep done - 9970.\n",
      "Ep done - 9980.\n",
      "Ep done - 9990.\n",
      "Ep done - 10000.\n",
      "Eval num_timesteps=1000000, episode_reward=0.48 +/- 0.87\n",
      "Episode length: 30.10 +/- 0.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.48        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034865946 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.48\n",
      "SELFPLAY: new best model, bumping up generation to 89\n",
      "Ep done - 33150.\n",
      "Ep done - 33160.\n",
      "Ep done - 33170.\n",
      "Ep done - 33180.\n",
      "Ep done - 33190.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 5942     |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n",
      "Ep done - 33200.\n",
      "Ep done - 33210.\n",
      "Ep done - 33220.\n",
      "Ep done - 33230.\n",
      "Ep done - 33240.\n",
      "Ep done - 33250.\n",
      "Ep done - 33260.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 490         |\n",
      "|    time_elapsed         | 5952        |\n",
      "|    total_timesteps      | 1003520     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052442417 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 4890        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.149       |\n",
      "-----------------------------------------\n",
      "Ep done - 33270.\n",
      "Ep done - 33280.\n",
      "Ep done - 33290.\n",
      "Ep done - 33300.\n",
      "Ep done - 33310.\n",
      "Ep done - 33320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 491         |\n",
      "|    time_elapsed         | 5961        |\n",
      "|    total_timesteps      | 1005568     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037912257 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 4900        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 33330.\n",
      "Ep done - 33340.\n",
      "Ep done - 33350.\n",
      "Ep done - 33360.\n",
      "Ep done - 33370.\n",
      "Ep done - 33380.\n",
      "Ep done - 33390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 492         |\n",
      "|    time_elapsed         | 5970        |\n",
      "|    total_timesteps      | 1007616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051644348 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.438       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00829     |\n",
      "|    n_updates            | 4910        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 33400.\n",
      "Ep done - 33410.\n",
      "Ep done - 33420.\n",
      "Ep done - 33430.\n",
      "Ep done - 33440.\n",
      "Ep done - 33450.\n",
      "Ep done - 33460.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.3       |\n",
      "|    ep_rew_mean          | 0.48       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 493        |\n",
      "|    time_elapsed         | 5979       |\n",
      "|    total_timesteps      | 1009664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04323329 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.271     |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 33470.\n",
      "Ep done - 10010.\n",
      "Ep done - 10020.\n",
      "Ep done - 10030.\n",
      "Ep done - 10040.\n",
      "Ep done - 10050.\n",
      "Ep done - 10060.\n",
      "Ep done - 10070.\n",
      "Ep done - 10080.\n",
      "Ep done - 10090.\n",
      "Ep done - 10100.\n",
      "Eval num_timesteps=1010000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.17 +/- 0.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1010000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05148315 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.282     |\n",
      "|    explained_variance   | 0.353      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.015      |\n",
      "|    n_updates            | 4930       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 90\n",
      "Ep done - 33480.\n",
      "Ep done - 33490.\n",
      "Ep done - 33500.\n",
      "Ep done - 33510.\n",
      "Ep done - 33520.\n",
      "Ep done - 33530.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 494      |\n",
      "|    time_elapsed    | 5998     |\n",
      "|    total_timesteps | 1011712  |\n",
      "---------------------------------\n",
      "Ep done - 33540.\n",
      "Ep done - 33550.\n",
      "Ep done - 33560.\n",
      "Ep done - 33570.\n",
      "Ep done - 33580.\n",
      "Ep done - 33590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 495         |\n",
      "|    time_elapsed         | 6007        |\n",
      "|    total_timesteps      | 1013760     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042811718 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0408      |\n",
      "|    n_updates            | 4940        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 33600.\n",
      "Ep done - 33610.\n",
      "Ep done - 33620.\n",
      "Ep done - 33630.\n",
      "Ep done - 33640.\n",
      "Ep done - 33650.\n",
      "Ep done - 33660.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.34      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 168       |\n",
      "|    iterations           | 496       |\n",
      "|    time_elapsed         | 6017      |\n",
      "|    total_timesteps      | 1015808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0505374 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.263    |\n",
      "|    explained_variance   | 0.284     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0216    |\n",
      "|    n_updates            | 4950      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "Ep done - 33670.\n",
      "Ep done - 33680.\n",
      "Ep done - 33690.\n",
      "Ep done - 33700.\n",
      "Ep done - 33710.\n",
      "Ep done - 33720.\n",
      "Ep done - 33730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.4         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 497         |\n",
      "|    time_elapsed         | 6026        |\n",
      "|    total_timesteps      | 1017856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040055912 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0656      |\n",
      "|    n_updates            | 4960        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "Ep done - 33740.\n",
      "Ep done - 33750.\n",
      "Ep done - 33760.\n",
      "Ep done - 33770.\n",
      "Ep done - 33780.\n",
      "Ep done - 33790.\n",
      "Ep done - 33800.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 498        |\n",
      "|    time_elapsed         | 6035       |\n",
      "|    total_timesteps      | 1019904    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04490103 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0364     |\n",
      "|    n_updates            | 4970       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "Ep done - 10110.\n",
      "Ep done - 10120.\n",
      "Ep done - 10130.\n",
      "Ep done - 10140.\n",
      "Ep done - 10150.\n",
      "Ep done - 10160.\n",
      "Ep done - 10170.\n",
      "Ep done - 10180.\n",
      "Ep done - 10190.\n",
      "Ep done - 10200.\n",
      "Eval num_timesteps=1020000, episode_reward=0.63 +/- 0.77\n",
      "Episode length: 30.21 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045532696 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0526      |\n",
      "|    n_updates            | 4980        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.63\n",
      "SELFPLAY: new best model, bumping up generation to 91\n",
      "Ep done - 33810.\n",
      "Ep done - 33820.\n",
      "Ep done - 33830.\n",
      "Ep done - 33840.\n",
      "Ep done - 33850.\n",
      "Ep done - 33860.\n",
      "Ep done - 33870.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.53     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 499      |\n",
      "|    time_elapsed    | 6054     |\n",
      "|    total_timesteps | 1021952  |\n",
      "---------------------------------\n",
      "Ep done - 33880.\n",
      "Ep done - 33890.\n",
      "Ep done - 33900.\n",
      "Ep done - 33910.\n",
      "Ep done - 33920.\n",
      "Ep done - 33930.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 500         |\n",
      "|    time_elapsed         | 6063        |\n",
      "|    total_timesteps      | 1024000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050190985 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0355      |\n",
      "|    n_updates            | 4990        |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 33940.\n",
      "Ep done - 33950.\n",
      "Ep done - 33960.\n",
      "Ep done - 33970.\n",
      "Ep done - 33980.\n",
      "Ep done - 33990.\n",
      "Ep done - 34000.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 168         |\n",
      "|    iterations           | 501         |\n",
      "|    time_elapsed         | 6073        |\n",
      "|    total_timesteps      | 1026048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049194686 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 5000        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Ep done - 34010.\n",
      "Ep done - 34020.\n",
      "Ep done - 34030.\n",
      "Ep done - 34040.\n",
      "Ep done - 34050.\n",
      "Ep done - 34060.\n",
      "Ep done - 34070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 502         |\n",
      "|    time_elapsed         | 6082        |\n",
      "|    total_timesteps      | 1028096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030771509 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.228      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 5010        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 34080.\n",
      "Ep done - 34090.\n",
      "Ep done - 34100.\n",
      "Ep done - 34110.\n",
      "Ep done - 34120.\n",
      "Ep done - 34130.\n",
      "Ep done - 10210.\n",
      "Ep done - 10220.\n",
      "Ep done - 10230.\n",
      "Ep done - 10240.\n",
      "Ep done - 10250.\n",
      "Ep done - 10260.\n",
      "Ep done - 10270.\n",
      "Ep done - 10280.\n",
      "Ep done - 10290.\n",
      "Ep done - 10300.\n",
      "Eval num_timesteps=1030000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.23 +/- 0.49\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1030000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03874775 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.252     |\n",
      "|    explained_variance   | 0.176      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00963    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.68\n",
      "SELFPLAY: new best model, bumping up generation to 92\n",
      "Ep done - 34140.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 503      |\n",
      "|    time_elapsed    | 6100     |\n",
      "|    total_timesteps | 1030144  |\n",
      "---------------------------------\n",
      "Ep done - 34150.\n",
      "Ep done - 34160.\n",
      "Ep done - 34170.\n",
      "Ep done - 34180.\n",
      "Ep done - 34190.\n",
      "Ep done - 34200.\n",
      "Ep done - 34210.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.65       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 504        |\n",
      "|    time_elapsed         | 6110       |\n",
      "|    total_timesteps      | 1032192    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03897801 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.244     |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0481     |\n",
      "|    n_updates            | 5030       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 34220.\n",
      "Ep done - 34230.\n",
      "Ep done - 34240.\n",
      "Ep done - 34250.\n",
      "Ep done - 34260.\n",
      "Ep done - 34270.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 505        |\n",
      "|    time_elapsed         | 6119       |\n",
      "|    total_timesteps      | 1034240    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04053717 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.257     |\n",
      "|    explained_variance   | 0.35       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 34280.\n",
      "Ep done - 34290.\n",
      "Ep done - 34300.\n",
      "Ep done - 34310.\n",
      "Ep done - 34320.\n",
      "Ep done - 34330.\n",
      "Ep done - 34340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 506         |\n",
      "|    time_elapsed         | 6128        |\n",
      "|    total_timesteps      | 1036288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039621897 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.222      |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 5050        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Ep done - 34350.\n",
      "Ep done - 34360.\n",
      "Ep done - 34370.\n",
      "Ep done - 34380.\n",
      "Ep done - 34390.\n",
      "Ep done - 34400.\n",
      "Ep done - 34410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 507         |\n",
      "|    time_elapsed         | 6138        |\n",
      "|    total_timesteps      | 1038336     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030603718 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.0832      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 5060        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Ep done - 34420.\n",
      "Ep done - 34430.\n",
      "Ep done - 34440.\n",
      "Ep done - 34450.\n",
      "Ep done - 34460.\n",
      "Ep done - 10310.\n",
      "Ep done - 10320.\n",
      "Ep done - 10330.\n",
      "Ep done - 10340.\n",
      "Ep done - 10350.\n",
      "Ep done - 10360.\n",
      "Ep done - 10370.\n",
      "Ep done - 10380.\n",
      "Ep done - 10390.\n",
      "Ep done - 10400.\n",
      "Eval num_timesteps=1040000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.23 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042947207 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 5070        |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 93\n",
      "Ep done - 34470.\n",
      "Ep done - 34480.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 168      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 6156     |\n",
      "|    total_timesteps | 1040384  |\n",
      "---------------------------------\n",
      "Ep done - 34490.\n",
      "Ep done - 34500.\n",
      "Ep done - 34510.\n",
      "Ep done - 34520.\n",
      "Ep done - 34530.\n",
      "Ep done - 34540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 509         |\n",
      "|    time_elapsed         | 6166        |\n",
      "|    total_timesteps      | 1042432     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043246284 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.258      |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.019       |\n",
      "|    n_updates            | 5080        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 34550.\n",
      "Ep done - 34560.\n",
      "Ep done - 34570.\n",
      "Ep done - 34580.\n",
      "Ep done - 34590.\n",
      "Ep done - 34600.\n",
      "Ep done - 34610.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 510         |\n",
      "|    time_elapsed         | 6175        |\n",
      "|    total_timesteps      | 1044480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045740172 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.238      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00204    |\n",
      "|    n_updates            | 5090        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Ep done - 34620.\n",
      "Ep done - 34630.\n",
      "Ep done - 34640.\n",
      "Ep done - 34650.\n",
      "Ep done - 34660.\n",
      "Ep done - 34670.\n",
      "Ep done - 34680.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 511        |\n",
      "|    time_elapsed         | 6184       |\n",
      "|    total_timesteps      | 1046528    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05266195 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00648    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Ep done - 34690.\n",
      "Ep done - 34700.\n",
      "Ep done - 34710.\n",
      "Ep done - 34720.\n",
      "Ep done - 34730.\n",
      "Ep done - 34740.\n",
      "Ep done - 34750.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 512        |\n",
      "|    time_elapsed         | 6194       |\n",
      "|    total_timesteps      | 1048576    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04196565 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | 0.427      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00427    |\n",
      "|    n_updates            | 5110       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Ep done - 34760.\n",
      "Ep done - 34770.\n",
      "Ep done - 34780.\n",
      "Ep done - 34790.\n",
      "Ep done - 34800.\n",
      "Ep done - 10410.\n",
      "Ep done - 10420.\n",
      "Ep done - 10430.\n",
      "Ep done - 10440.\n",
      "Ep done - 10450.\n",
      "Ep done - 10460.\n",
      "Ep done - 10470.\n",
      "Ep done - 10480.\n",
      "Ep done - 10490.\n",
      "Ep done - 10500.\n",
      "Eval num_timesteps=1050000, episode_reward=0.47 +/- 0.85\n",
      "Episode length: 30.13 +/- 0.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040842168 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 5120        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.47\n",
      "SELFPLAY: new best model, bumping up generation to 94\n",
      "Ep done - 34810.\n",
      "Ep done - 34820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.5      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 513      |\n",
      "|    time_elapsed    | 6212     |\n",
      "|    total_timesteps | 1050624  |\n",
      "---------------------------------\n",
      "Ep done - 34830.\n",
      "Ep done - 34840.\n",
      "Ep done - 34850.\n",
      "Ep done - 34860.\n",
      "Ep done - 34870.\n",
      "Ep done - 34880.\n",
      "Ep done - 34890.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 514        |\n",
      "|    time_elapsed         | 6221       |\n",
      "|    total_timesteps      | 1052672    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04509411 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.289      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0525     |\n",
      "|    n_updates            | 5130       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Ep done - 34900.\n",
      "Ep done - 34910.\n",
      "Ep done - 34920.\n",
      "Ep done - 34930.\n",
      "Ep done - 34940.\n",
      "Ep done - 34950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 29.9       |\n",
      "|    ep_rew_mean          | 0.56       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 515        |\n",
      "|    time_elapsed         | 6231       |\n",
      "|    total_timesteps      | 1054720    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04821443 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.248      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 34960.\n",
      "Ep done - 34970.\n",
      "Ep done - 34980.\n",
      "Ep done - 34990.\n",
      "Ep done - 35000.\n",
      "Ep done - 35010.\n",
      "Ep done - 35020.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 516         |\n",
      "|    time_elapsed         | 6240        |\n",
      "|    total_timesteps      | 1056768     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044384364 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.034       |\n",
      "|    n_updates            | 5150        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Ep done - 35030.\n",
      "Ep done - 35040.\n",
      "Ep done - 35050.\n",
      "Ep done - 35060.\n",
      "Ep done - 35070.\n",
      "Ep done - 35080.\n",
      "Ep done - 35090.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 517        |\n",
      "|    time_elapsed         | 6249       |\n",
      "|    total_timesteps      | 1058816    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03632034 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.234     |\n",
      "|    explained_variance   | 0.372      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0414     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 35100.\n",
      "Ep done - 35110.\n",
      "Ep done - 35120.\n",
      "Ep done - 35130.\n",
      "Ep done - 10510.\n",
      "Ep done - 10520.\n",
      "Ep done - 10530.\n",
      "Ep done - 10540.\n",
      "Ep done - 10550.\n",
      "Ep done - 10560.\n",
      "Ep done - 10570.\n",
      "Ep done - 10580.\n",
      "Ep done - 10590.\n",
      "Ep done - 10600.\n",
      "Eval num_timesteps=1060000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.11 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.5         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060327254 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0271      |\n",
      "|    n_updates            | 5170        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.5\n",
      "SELFPLAY: new best model, bumping up generation to 95\n",
      "Ep done - 35140.\n",
      "Ep done - 35150.\n",
      "Ep done - 35160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.58     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 6268     |\n",
      "|    total_timesteps | 1060864  |\n",
      "---------------------------------\n",
      "Ep done - 35170.\n",
      "Ep done - 35180.\n",
      "Ep done - 35190.\n",
      "Ep done - 35200.\n",
      "Ep done - 35210.\n",
      "Ep done - 35220.\n",
      "Ep done - 35230.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 519         |\n",
      "|    time_elapsed         | 6278        |\n",
      "|    total_timesteps      | 1062912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034459308 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0128      |\n",
      "|    n_updates            | 5180        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.133       |\n",
      "-----------------------------------------\n",
      "Ep done - 35240.\n",
      "Ep done - 35250.\n",
      "Ep done - 35260.\n",
      "Ep done - 35270.\n",
      "Ep done - 35280.\n",
      "Ep done - 35290.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 520         |\n",
      "|    time_elapsed         | 6287        |\n",
      "|    total_timesteps      | 1064960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042276263 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0297      |\n",
      "|    n_updates            | 5190        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 35300.\n",
      "Ep done - 35310.\n",
      "Ep done - 35320.\n",
      "Ep done - 35330.\n",
      "Ep done - 35340.\n",
      "Ep done - 35350.\n",
      "Ep done - 35360.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30         |\n",
      "|    ep_rew_mean          | 0.43       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 521        |\n",
      "|    time_elapsed         | 6296       |\n",
      "|    total_timesteps      | 1067008    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05296762 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.242     |\n",
      "|    explained_variance   | 0.244      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0443     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 35370.\n",
      "Ep done - 35380.\n",
      "Ep done - 35390.\n",
      "Ep done - 35400.\n",
      "Ep done - 35410.\n",
      "Ep done - 35420.\n",
      "Ep done - 35430.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 522         |\n",
      "|    time_elapsed         | 6305        |\n",
      "|    total_timesteps      | 1069056     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042529184 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.247      |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 5210        |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    value_loss           | 0.173       |\n",
      "-----------------------------------------\n",
      "Ep done - 35440.\n",
      "Ep done - 35450.\n",
      "Ep done - 35460.\n",
      "Ep done - 10610.\n",
      "Ep done - 10620.\n",
      "Ep done - 10630.\n",
      "Ep done - 10640.\n",
      "Ep done - 10650.\n",
      "Ep done - 10660.\n",
      "Ep done - 10670.\n",
      "Ep done - 10680.\n",
      "Ep done - 10690.\n",
      "Ep done - 10700.\n",
      "Eval num_timesteps=1070000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.16 +/- 0.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1070000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03710662 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.239     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 96\n",
      "Ep done - 35470.\n",
      "Ep done - 35480.\n",
      "Ep done - 35490.\n",
      "Ep done - 35500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 523      |\n",
      "|    time_elapsed    | 6324     |\n",
      "|    total_timesteps | 1071104  |\n",
      "---------------------------------\n",
      "Ep done - 35510.\n",
      "Ep done - 35520.\n",
      "Ep done - 35530.\n",
      "Ep done - 35540.\n",
      "Ep done - 35550.\n",
      "Ep done - 35560.\n",
      "Ep done - 35570.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.65        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 524         |\n",
      "|    time_elapsed         | 6333        |\n",
      "|    total_timesteps      | 1073152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039950848 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 5230        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "Ep done - 35580.\n",
      "Ep done - 35590.\n",
      "Ep done - 35600.\n",
      "Ep done - 35610.\n",
      "Ep done - 35620.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 35630.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 525         |\n",
      "|    time_elapsed         | 6343        |\n",
      "|    total_timesteps      | 1075200     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050280638 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.224      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 5240        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 35640.\n",
      "Ep done - 35650.\n",
      "Ep done - 35660.\n",
      "Ep done - 35670.\n",
      "Ep done - 35680.\n",
      "Ep done - 35690.\n",
      "Ep done - 35700.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30          |\n",
      "|    ep_rew_mean          | 0.34        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 526         |\n",
      "|    time_elapsed         | 6352        |\n",
      "|    total_timesteps      | 1077248     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039189495 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0548      |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "Ep done - 35710.\n",
      "Ep done - 35720.\n",
      "Ep done - 35730.\n",
      "Ep done - 35740.\n",
      "Ep done - 35750.\n",
      "Ep done - 35760.\n",
      "Ep done - 35770.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.46        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 527         |\n",
      "|    time_elapsed         | 6361        |\n",
      "|    total_timesteps      | 1079296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041514885 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.325       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 5260        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    value_loss           | 0.17        |\n",
      "-----------------------------------------\n",
      "Ep done - 35780.\n",
      "Ep done - 35790.\n",
      "Ep done - 10710.\n",
      "Ep done - 10720.\n",
      "Ep done - 10730.\n",
      "Ep done - 10740.\n",
      "Ep done - 10750.\n",
      "Ep done - 10760.\n",
      "Ep done - 10770.\n",
      "Ep done - 10780.\n",
      "Ep done - 10790.\n",
      "Ep done - 10800.\n",
      "Eval num_timesteps=1080000, episode_reward=0.78 +/- 0.58\n",
      "Episode length: 30.16 +/- 0.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03333773 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.216     |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 5270       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 97\n",
      "Ep done - 35800.\n",
      "Ep done - 35810.\n",
      "Ep done - 35820.\n",
      "Ep done - 35830.\n",
      "Ep done - 35840.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 6380     |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "Ep done - 35850.\n",
      "Ep done - 35860.\n",
      "Ep done - 35870.\n",
      "Ep done - 35880.\n",
      "Ep done - 35890.\n",
      "Ep done - 35900.\n",
      "Ep done - 35910.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.61       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 529        |\n",
      "|    time_elapsed         | 6390       |\n",
      "|    total_timesteps      | 1083392    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04976645 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.214     |\n",
      "|    explained_variance   | 0.256      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Ep done - 35920.\n",
      "Ep done - 35930.\n",
      "Ep done - 35940.\n",
      "Ep done - 35950.\n",
      "Ep done - 35960.\n",
      "Ep done - 35970.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 530         |\n",
      "|    time_elapsed         | 6399        |\n",
      "|    total_timesteps      | 1085440     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036731668 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 5290        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "Ep done - 35980.\n",
      "Ep done - 35990.\n",
      "Ep done - 36000.\n",
      "Ep done - 36010.\n",
      "Ep done - 36020.\n",
      "Ep done - 36030.\n",
      "Ep done - 36040.\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 30.1     |\n",
      "|    ep_rew_mean          | 0.66     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 169      |\n",
      "|    iterations           | 531      |\n",
      "|    time_elapsed         | 6408     |\n",
      "|    total_timesteps      | 1087488  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.040906 |\n",
      "|    clip_fraction        | 0.134    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.19    |\n",
      "|    explained_variance   | 0.43     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.0388   |\n",
      "|    n_updates            | 5300     |\n",
      "|    policy_gradient_loss | -0.0203  |\n",
      "|    value_loss           | 0.147    |\n",
      "--------------------------------------\n",
      "Ep done - 36050.\n",
      "Ep done - 36060.\n",
      "Ep done - 36070.\n",
      "Ep done - 36080.\n",
      "Ep done - 36090.\n",
      "Ep done - 36100.\n",
      "Ep done - 36110.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 532         |\n",
      "|    time_elapsed         | 6418        |\n",
      "|    total_timesteps      | 1089536     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035248257 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0277      |\n",
      "|    n_updates            | 5310        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.09        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 36120.\n",
      "Ep done - 10810.\n",
      "Ep done - 10820.\n",
      "Ep done - 10830.\n",
      "Ep done - 10840.\n",
      "Ep done - 10850.\n",
      "Ep done - 10860.\n",
      "Ep done - 10870.\n",
      "Ep done - 10880.\n",
      "Ep done - 10890.\n",
      "Ep done - 10900.\n",
      "Eval num_timesteps=1090000, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050621454 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 5320        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.8\n",
      "SELFPLAY: new best model, bumping up generation to 98\n",
      "Ep done - 36130.\n",
      "Ep done - 36140.\n",
      "Ep done - 36150.\n",
      "Ep done - 36160.\n",
      "Ep done - 36170.\n",
      "Ep done - 36180.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 533      |\n",
      "|    time_elapsed    | 6437     |\n",
      "|    total_timesteps | 1091584  |\n",
      "---------------------------------\n",
      "Ep done - 36190.\n",
      "Ep done - 36200.\n",
      "Ep done - 36210.\n",
      "Ep done - 36220.\n",
      "Ep done - 36230.\n",
      "Ep done - 36240.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 534        |\n",
      "|    time_elapsed         | 6446       |\n",
      "|    total_timesteps      | 1093632    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04758173 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0651     |\n",
      "|    n_updates            | 5330       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Ep done - 36250.\n",
      "Ep done - 36260.\n",
      "Ep done - 36270.\n",
      "Ep done - 36280.\n",
      "Ep done - 36290.\n",
      "Ep done - 36300.\n",
      "Ep done - 36310.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 535         |\n",
      "|    time_elapsed         | 6455        |\n",
      "|    total_timesteps      | 1095680     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047200173 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.329       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 5340        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "Ep done - 36320.\n",
      "Ep done - 36330.\n",
      "Ep done - 36340.\n",
      "Ep done - 36350.\n",
      "Ep done - 36360.\n",
      "Ep done - 36370.\n",
      "Ep done - 36380.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.59        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 536         |\n",
      "|    time_elapsed         | 6465        |\n",
      "|    total_timesteps      | 1097728     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040729634 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0261      |\n",
      "|    n_updates            | 5350        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 36390.\n",
      "Ep done - 36400.\n",
      "Ep done - 36410.\n",
      "Ep done - 36420.\n",
      "Ep done - 36430.\n",
      "Ep done - 36440.\n",
      "Ep done - 36450.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 537         |\n",
      "|    time_elapsed         | 6474        |\n",
      "|    total_timesteps      | 1099776     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040101714 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0299      |\n",
      "|    n_updates            | 5360        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Ep done - 36460.\n",
      "Ep done - 10910.\n",
      "Ep done - 10920.\n",
      "Ep done - 10930.\n",
      "Ep done - 10940.\n",
      "Ep done - 10950.\n",
      "Ep done - 10960.\n",
      "Ep done - 10970.\n",
      "Ep done - 10980.\n",
      "Ep done - 10990.\n",
      "Ep done - 11000.\n",
      "Eval num_timesteps=1100000, episode_reward=0.67 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.67        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043311797 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 99\n",
      "Ep done - 36470.\n",
      "Ep done - 36480.\n",
      "Ep done - 36490.\n",
      "Ep done - 36500.\n",
      "Ep done - 36510.\n",
      "Ep done - 36520.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 6493     |\n",
      "|    total_timesteps | 1101824  |\n",
      "---------------------------------\n",
      "Ep done - 36530.\n",
      "Ep done - 36540.\n",
      "Ep done - 36550.\n",
      "Ep done - 36560.\n",
      "Ep done - 36570.\n",
      "Ep done - 36580.\n",
      "Ep done - 36590.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.58       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 539        |\n",
      "|    time_elapsed         | 6502       |\n",
      "|    total_timesteps      | 1103872    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04396238 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.181     |\n",
      "|    explained_variance   | 0.0744     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0533     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Ep done - 36600.\n",
      "Ep done - 36610.\n",
      "Ep done - 36620.\n",
      "Ep done - 36630.\n",
      "Ep done - 36640.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 36650.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 540         |\n",
      "|    time_elapsed         | 6512        |\n",
      "|    total_timesteps      | 1105920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052182376 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0805      |\n",
      "|    n_updates            | 5390        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 36660.\n",
      "Ep done - 36670.\n",
      "Ep done - 36680.\n",
      "Ep done - 36690.\n",
      "Ep done - 36700.\n",
      "Ep done - 36710.\n",
      "Ep done - 36720.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 541         |\n",
      "|    time_elapsed         | 6521        |\n",
      "|    total_timesteps      | 1107968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036159642 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0853      |\n",
      "|    n_updates            | 5400        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "Ep done - 36730.\n",
      "Ep done - 36740.\n",
      "Ep done - 36750.\n",
      "Ep done - 36760.\n",
      "Ep done - 36770.\n",
      "Ep done - 36780.\n",
      "Ep done - 36790.\n",
      "Ep done - 11010.\n",
      "Ep done - 11020.\n",
      "Ep done - 11030.\n",
      "Ep done - 11040.\n",
      "Ep done - 11050.\n",
      "Ep done - 11060.\n",
      "Ep done - 11070.\n",
      "Ep done - 11080.\n",
      "Ep done - 11090.\n",
      "Ep done - 11100.\n",
      "Eval num_timesteps=1110000, episode_reward=0.56 +/- 0.83\n",
      "Episode length: 30.10 +/- 0.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042721942 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0559      |\n",
      "|    n_updates            | 5410        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.188       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.56\n",
      "SELFPLAY: new best model, bumping up generation to 100\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 6539     |\n",
      "|    total_timesteps | 1110016  |\n",
      "---------------------------------\n",
      "Ep done - 36800.\n",
      "Ep done - 36810.\n",
      "Ep done - 36820.\n",
      "Ep done - 36830.\n",
      "Ep done - 36840.\n",
      "Ep done - 36850.\n",
      "Ep done - 36860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 543         |\n",
      "|    time_elapsed         | 6549        |\n",
      "|    total_timesteps      | 1112064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040159956 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00411     |\n",
      "|    n_updates            | 5420        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 36870.\n",
      "Ep done - 36880.\n",
      "Ep done - 36890.\n",
      "Ep done - 36900.\n",
      "Ep done - 36910.\n",
      "Ep done - 36920.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 544         |\n",
      "|    time_elapsed         | 6558        |\n",
      "|    total_timesteps      | 1114112     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039068706 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.2        |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0634      |\n",
      "|    n_updates            | 5430        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 36930.\n",
      "Ep done - 36940.\n",
      "Ep done - 36950.\n",
      "Ep done - 36960.\n",
      "Ep done - 36970.\n",
      "Ep done - 36980.\n",
      "Ep done - 36990.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.65      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 169       |\n",
      "|    iterations           | 545       |\n",
      "|    time_elapsed         | 6567      |\n",
      "|    total_timesteps      | 1116160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0333879 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.181    |\n",
      "|    explained_variance   | 0.449     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0491    |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -0.02     |\n",
      "|    value_loss           | 0.0786    |\n",
      "---------------------------------------\n",
      "Ep done - 37000.\n",
      "Ep done - 37010.\n",
      "Ep done - 37020.\n",
      "Ep done - 37030.\n",
      "Ep done - 37040.\n",
      "Ep done - 37050.\n",
      "Ep done - 37060.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 546         |\n",
      "|    time_elapsed         | 6577        |\n",
      "|    total_timesteps      | 1118208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049022995 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 5450        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Ep done - 37070.\n",
      "Ep done - 37080.\n",
      "Ep done - 37090.\n",
      "Ep done - 37100.\n",
      "Ep done - 37110.\n",
      "Ep done - 37120.\n",
      "Ep done - 11110.\n",
      "Ep done - 11120.\n",
      "Ep done - 11130.\n",
      "Ep done - 11140.\n",
      "Ep done - 11150.\n",
      "Ep done - 11160.\n",
      "Ep done - 11170.\n",
      "Ep done - 11180.\n",
      "Ep done - 11190.\n",
      "Ep done - 11200.\n",
      "Eval num_timesteps=1120000, episode_reward=0.78 +/- 0.63\n",
      "Episode length: 30.20 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.78        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047805544 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00341     |\n",
      "|    n_updates            | 5460        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.0569      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.78\n",
      "SELFPLAY: new best model, bumping up generation to 101\n",
      "Ep done - 37130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 6595     |\n",
      "|    total_timesteps | 1120256  |\n",
      "---------------------------------\n",
      "Ep done - 37140.\n",
      "Ep done - 37150.\n",
      "Ep done - 37160.\n",
      "Ep done - 37170.\n",
      "Ep done - 37180.\n",
      "Ep done - 37190.\n",
      "Ep done - 37200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 169         |\n",
      "|    iterations           | 548         |\n",
      "|    time_elapsed         | 6605        |\n",
      "|    total_timesteps      | 1122304     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044635125 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.144      |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0575      |\n",
      "|    n_updates            | 5470        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.136       |\n",
      "-----------------------------------------\n",
      "Ep done - 37210.\n",
      "Ep done - 37220.\n",
      "Ep done - 37230.\n",
      "Ep done - 37240.\n",
      "Ep done - 37250.\n",
      "Ep done - 37260.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 169        |\n",
      "|    iterations           | 549        |\n",
      "|    time_elapsed         | 6614       |\n",
      "|    total_timesteps      | 1124352    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03388484 |\n",
      "|    clip_fraction        | 0.0861     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.0948     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0204     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 0.0933     |\n",
      "----------------------------------------\n",
      "Ep done - 37270.\n",
      "Ep done - 37280.\n",
      "Ep done - 37290.\n",
      "Ep done - 37300.\n",
      "Ep done - 37310.\n",
      "Ep done - 37320.\n",
      "Ep done - 37330.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 550         |\n",
      "|    time_elapsed         | 6624        |\n",
      "|    total_timesteps      | 1126400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041098367 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.142      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0527      |\n",
      "|    n_updates            | 5490        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "Ep done - 37340.\n",
      "Ep done - 37350.\n",
      "Ep done - 37360.\n",
      "Ep done - 37370.\n",
      "Ep done - 37380.\n",
      "Ep done - 37390.\n",
      "Ep done - 37400.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 551        |\n",
      "|    time_elapsed         | 6633       |\n",
      "|    total_timesteps      | 1128448    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03404434 |\n",
      "|    clip_fraction        | 0.0829     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.133     |\n",
      "|    explained_variance   | 0.122      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0483     |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 37410.\n",
      "Ep done - 37420.\n",
      "Ep done - 37430.\n",
      "Ep done - 37440.\n",
      "Ep done - 37450.\n",
      "Ep done - 11210.\n",
      "Ep done - 11220.\n",
      "Ep done - 11230.\n",
      "Ep done - 11240.\n",
      "Ep done - 11250.\n",
      "Ep done - 11260.\n",
      "Ep done - 11270.\n",
      "Ep done - 11280.\n",
      "Ep done - 11290.\n",
      "Ep done - 11300.\n",
      "Eval num_timesteps=1130000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.14 +/- 0.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047207117 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.134      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 5510        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.7\n",
      "SELFPLAY: new best model, bumping up generation to 102\n",
      "Ep done - 37460.\n",
      "Ep done - 37470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.62     |\n",
      "| time/              |          |\n",
      "|    fps             | 169      |\n",
      "|    iterations      | 552      |\n",
      "|    time_elapsed    | 6652     |\n",
      "|    total_timesteps | 1130496  |\n",
      "---------------------------------\n",
      "Ep done - 37480.\n",
      "Ep done - 37490.\n",
      "Ep done - 37500.\n",
      "Ep done - 37510.\n",
      "Ep done - 37520.\n",
      "Ep done - 37530.\n",
      "Ep done - 37540.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 553        |\n",
      "|    time_elapsed         | 6661       |\n",
      "|    total_timesteps      | 1132544    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03947856 |\n",
      "|    clip_fraction        | 0.0856     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0202     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "Ep done - 37550.\n",
      "Ep done - 37560.\n",
      "Ep done - 37570.\n",
      "Ep done - 37580.\n",
      "Ep done - 37590.\n",
      "Ep done - 37600.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.1      |\n",
      "|    ep_rew_mean          | 0.68      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 170       |\n",
      "|    iterations           | 554       |\n",
      "|    time_elapsed         | 6670      |\n",
      "|    total_timesteps      | 1134592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0364751 |\n",
      "|    clip_fraction        | 0.0935    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.132    |\n",
      "|    explained_variance   | 0.26      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00571   |\n",
      "|    n_updates            | 5530      |\n",
      "|    policy_gradient_loss | -0.0185   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "Ep done - 37610.\n",
      "Ep done - 37620.\n",
      "Ep done - 37630.\n",
      "Ep done - 37640.\n",
      "Ep done - 37650.\n",
      "Ep done - 37660.\n",
      "Ep done - 37670.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 555         |\n",
      "|    time_elapsed         | 6680        |\n",
      "|    total_timesteps      | 1136640     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043442197 |\n",
      "|    clip_fraction        | 0.09        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | -0.0275     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0627      |\n",
      "|    n_updates            | 5540        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 37680.\n",
      "Ep done - 37690.\n",
      "Ep done - 37700.\n",
      "Ep done - 37710.\n",
      "Ep done - 37720.\n",
      "Ep done - 37730.\n",
      "Ep done - 37740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.54       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 556        |\n",
      "|    time_elapsed         | 6689       |\n",
      "|    total_timesteps      | 1138688    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02916764 |\n",
      "|    clip_fraction        | 0.0876     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 5550       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    value_loss           | 0.096      |\n",
      "----------------------------------------\n",
      "Ep done - 37750.\n",
      "Ep done - 37760.\n",
      "Ep done - 37770.\n",
      "Ep done - 37780.\n",
      "Ep done - 11310.\n",
      "Ep done - 11320.\n",
      "Ep done - 11330.\n",
      "Ep done - 11340.\n",
      "Ep done - 11350.\n",
      "Ep done - 11360.\n",
      "Ep done - 11370.\n",
      "Ep done - 11380.\n",
      "Ep done - 11390.\n",
      "Ep done - 11400.\n",
      "Eval num_timesteps=1140000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.18 +/- 0.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035643794 |\n",
      "|    clip_fraction        | 0.0912      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0867      |\n",
      "|    n_updates            | 5560        |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.54\n",
      "SELFPLAY: new best model, bumping up generation to 103\n",
      "Ep done - 37790.\n",
      "Ep done - 37800.\n",
      "Ep done - 37810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.51     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 557      |\n",
      "|    time_elapsed    | 6708     |\n",
      "|    total_timesteps | 1140736  |\n",
      "---------------------------------\n",
      "Ep done - 37820.\n",
      "Ep done - 37830.\n",
      "Ep done - 37840.\n",
      "Ep done - 37850.\n",
      "Ep done - 37860.\n",
      "Ep done - 37870.\n",
      "Ep done - 37880.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.52       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 558        |\n",
      "|    time_elapsed         | 6717       |\n",
      "|    total_timesteps      | 1142784    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03481961 |\n",
      "|    clip_fraction        | 0.0858     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.358      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.059      |\n",
      "|    n_updates            | 5570       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "Ep done - 37890.\n",
      "Ep done - 37900.\n",
      "Ep done - 37910.\n",
      "Ep done - 37920.\n",
      "Ep done - 37930.\n",
      "Ep done - 37940.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.59       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 559        |\n",
      "|    time_elapsed         | 6726       |\n",
      "|    total_timesteps      | 1144832    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05693128 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0294     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Ep done - 37950.\n",
      "Ep done - 37960.\n",
      "Ep done - 37970.\n",
      "Ep done - 37980.\n",
      "Ep done - 37990.\n",
      "Ep done - 38000.\n",
      "Ep done - 38010.\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 30.2      |\n",
      "|    ep_rew_mean          | 0.61      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 170       |\n",
      "|    iterations           | 560       |\n",
      "|    time_elapsed         | 6736      |\n",
      "|    total_timesteps      | 1146880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0394685 |\n",
      "|    clip_fraction        | 0.0976    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.139    |\n",
      "|    explained_variance   | 0.199     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0493    |\n",
      "|    n_updates            | 5590      |\n",
      "|    policy_gradient_loss | -0.0164   |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "Ep done - 38020.\n",
      "Ep done - 38030.\n",
      "Ep done - 38040.\n",
      "Ep done - 38050.\n",
      "Ep done - 38060.\n",
      "Ep done - 38070.\n",
      "Ep done - 38080.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 561         |\n",
      "|    time_elapsed         | 6745        |\n",
      "|    total_timesteps      | 1148928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058157794 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.129      |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 5600        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.12        |\n",
      "-----------------------------------------\n",
      "Ep done - 38090.\n",
      "Ep done - 38100.\n",
      "Ep done - 38110.\n",
      "Ep done - 11410.\n",
      "Ep done - 11420.\n",
      "Ep done - 11430.\n",
      "Ep done - 11440.\n",
      "Ep done - 11450.\n",
      "Ep done - 11460.\n",
      "Ep done - 11470.\n",
      "Ep done - 11480.\n",
      "Ep done - 11490.\n",
      "Ep done - 11500.\n",
      "Eval num_timesteps=1150000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.19 +/- 0.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1150000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05622049 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.134     |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0135     |\n",
      "|    n_updates            | 5610       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    value_loss           | 0.0984     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.67\n",
      "SELFPLAY: new best model, bumping up generation to 104\n",
      "Ep done - 38120.\n",
      "Ep done - 38130.\n",
      "Ep done - 38140.\n",
      "Ep done - 38150.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.64     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 562      |\n",
      "|    time_elapsed    | 6764     |\n",
      "|    total_timesteps | 1150976  |\n",
      "---------------------------------\n",
      "Ep done - 38160.\n",
      "Ep done - 38170.\n",
      "Ep done - 38180.\n",
      "Ep done - 38190.\n",
      "Ep done - 38200.\n",
      "Ep done - 38210.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.58        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 563         |\n",
      "|    time_elapsed         | 6773        |\n",
      "|    total_timesteps      | 1153024     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038103797 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0762      |\n",
      "|    n_updates            | 5620        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 38220.\n",
      "Ep done - 38230.\n",
      "Ep done - 38240.\n",
      "Ep done - 38250.\n",
      "Ep done - 38260.\n",
      "Ep done - 38270.\n",
      "Ep done - 38280.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 564        |\n",
      "|    time_elapsed         | 6782       |\n",
      "|    total_timesteps      | 1155072    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03573143 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.151     |\n",
      "|    explained_variance   | -0.0294    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0723     |\n",
      "|    n_updates            | 5630       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 38290.\n",
      "Ep done - 38300.\n",
      "Ep done - 38310.\n",
      "Ep done - 38320.\n",
      "Ep done - 38330.\n",
      "Ep done - 38340.\n",
      "Ep done - 38350.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.3        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 565         |\n",
      "|    time_elapsed         | 6792        |\n",
      "|    total_timesteps      | 1157120     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050151974 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.151      |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0313      |\n",
      "|    n_updates            | 5640        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Ep done - 38360.\n",
      "Ep done - 38370.\n",
      "Ep done - 38380.\n",
      "Ep done - 38390.\n",
      "Ep done - 38400.\n",
      "Ep done - 38410.\n",
      "Ep done - 38420.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 566         |\n",
      "|    time_elapsed         | 6801        |\n",
      "|    total_timesteps      | 1159168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049421526 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00811    |\n",
      "|    n_updates            | 5650        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 38430.\n",
      "Ep done - 38440.\n",
      "Ep done - 38450.\n",
      "Ep done - 11510.\n",
      "Ep done - 11520.\n",
      "Ep done - 11530.\n",
      "Ep done - 11540.\n",
      "Ep done - 11550.\n",
      "Ep done - 11560.\n",
      "Ep done - 11570.\n",
      "Ep done - 11580.\n",
      "Ep done - 11590.\n",
      "Ep done - 11600.\n",
      "Eval num_timesteps=1160000, episode_reward=0.71 +/- 0.67\n",
      "Episode length: 30.21 +/- 0.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033828907 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0457      |\n",
      "|    n_updates            | 5660        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.131       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.71\n",
      "SELFPLAY: new best model, bumping up generation to 105\n",
      "Ep done - 38460.\n",
      "Ep done - 38470.\n",
      "Ep done - 38480.\n",
      "Ep done - 38490.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 567      |\n",
      "|    time_elapsed    | 6819     |\n",
      "|    total_timesteps | 1161216  |\n",
      "---------------------------------\n",
      "Ep done - 38500.\n",
      "Ep done - 38510.\n",
      "Ep done - 38520.\n",
      "Ep done - 38530.\n",
      "Ep done - 38540.\n",
      "Ep done - 38550.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.27        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 568         |\n",
      "|    time_elapsed         | 6829        |\n",
      "|    total_timesteps      | 1163264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046934225 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.188      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 5670        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.161       |\n",
      "-----------------------------------------\n",
      "Ep done - 38560.\n",
      "Ep done - 38570.\n",
      "Ep done - 38580.\n",
      "Ep done - 38590.\n",
      "Ep done - 38600.\n",
      "Ep done - 38610.\n",
      "Ep done - 38620.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.47        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 569         |\n",
      "|    time_elapsed         | 6838        |\n",
      "|    total_timesteps      | 1165312     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053857513 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0433      |\n",
      "|    n_updates            | 5680        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.181       |\n",
      "-----------------------------------------\n",
      "Ep done - 38630.\n",
      "Ep done - 38640.\n",
      "Ep done - 38650.\n",
      "Ep done - 38660.\n",
      "Ep done - 38670.\n",
      "Ep done - 38680.\n",
      "Ep done - 38690.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.49        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 570         |\n",
      "|    time_elapsed         | 6847        |\n",
      "|    total_timesteps      | 1167360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050546944 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0362      |\n",
      "|    n_updates            | 5690        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Ep done - 38700.\n",
      "Ep done - 38710.\n",
      "Ep done - 38720.\n",
      "Ep done - 38730.\n",
      "Ep done - 38740.\n",
      "Ep done - 38750.\n",
      "Ep done - 38760.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.42        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 571         |\n",
      "|    time_elapsed         | 6857        |\n",
      "|    total_timesteps      | 1169408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048384253 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.219      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 5700        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 38770.\n",
      "Ep done - 38780.\n",
      "Ep done - 11610.\n",
      "Ep done - 11620.\n",
      "Ep done - 11630.\n",
      "Ep done - 11640.\n",
      "Ep done - 11650.\n",
      "Ep done - 11660.\n",
      "Ep done - 11670.\n",
      "Ep done - 11680.\n",
      "Ep done - 11690.\n",
      "Ep done - 11700.\n",
      "Eval num_timesteps=1170000, episode_reward=0.44 +/- 0.89\n",
      "Episode length: 30.13 +/- 0.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047705427 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0237      |\n",
      "|    n_updates            | 5710        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.44\n",
      "SELFPLAY: new best model, bumping up generation to 106\n",
      "Ep done - 38790.\n",
      "Ep done - 38800.\n",
      "Ep done - 38810.\n",
      "Ep done - 38820.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.55     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 572      |\n",
      "|    time_elapsed    | 6876     |\n",
      "|    total_timesteps | 1171456  |\n",
      "---------------------------------\n",
      "Ep done - 38830.\n",
      "Ep done - 38840.\n",
      "Ep done - 38850.\n",
      "Ep done - 38860.\n",
      "Ep done - 38870.\n",
      "Ep done - 38880.\n",
      "Ep done - 38890.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.46       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 573        |\n",
      "|    time_elapsed         | 6886       |\n",
      "|    total_timesteps      | 1173504    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05263876 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.208     |\n",
      "|    explained_variance   | 0.264      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Ep done - 38900.\n",
      "Ep done - 38910.\n",
      "Ep done - 38920.\n",
      "Ep done - 38930.\n",
      "Ep done - 38940.\n",
      "Ep done - 38950.\n",
      "Ep done - 38960.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.44        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 574         |\n",
      "|    time_elapsed         | 6895        |\n",
      "|    total_timesteps      | 1175552     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037859928 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 5730        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 0.204       |\n",
      "-----------------------------------------\n",
      "Ep done - 38970.\n",
      "Ep done - 38980.\n",
      "Ep done - 38990.\n",
      "Ep done - 39000.\n",
      "Ep done - 39010.\n",
      "Ep done - 39020.\n",
      "Ep done - 39030.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 575         |\n",
      "|    time_elapsed         | 6904        |\n",
      "|    total_timesteps      | 1177600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042589046 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 5740        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "Ep done - 39040.\n",
      "Ep done - 39050.\n",
      "Ep done - 39060.\n",
      "Ep done - 39070.\n",
      "Ep done - 39080.\n",
      "Ep done - 39090.\n",
      "Ep done - 39100.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.56        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 576         |\n",
      "|    time_elapsed         | 6914        |\n",
      "|    total_timesteps      | 1179648     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045043357 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 5750        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 39110.\n",
      "Ep done - 11710.\n",
      "Ep done - 11720.\n",
      "Ep done - 11730.\n",
      "Ep done - 11740.\n",
      "Ep done - 11750.\n",
      "Ep done - 11760.\n",
      "Ep done - 11770.\n",
      "Ep done - 11780.\n",
      "Ep done - 11790.\n",
      "Ep done - 11800.\n",
      "Eval num_timesteps=1180000, episode_reward=0.51 +/- 0.83\n",
      "Episode length: 30.08 +/- 0.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.1       |\n",
      "|    mean_reward          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05135953 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.256      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.51\n",
      "SELFPLAY: new best model, bumping up generation to 107\n",
      "Ep done - 39120.\n",
      "Ep done - 39130.\n",
      "Ep done - 39140.\n",
      "Ep done - 39150.\n",
      "Ep done - 39160.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.57     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 577      |\n",
      "|    time_elapsed    | 6932     |\n",
      "|    total_timesteps | 1181696  |\n",
      "---------------------------------\n",
      "Ep done - 39170.\n",
      "Ep done - 39180.\n",
      "Ep done - 39190.\n",
      "Ep done - 39200.\n",
      "Ep done - 39210.\n",
      "Ep done - 39220.\n",
      "Ep done - 39230.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.66       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 578        |\n",
      "|    time_elapsed         | 6941       |\n",
      "|    total_timesteps      | 1183744    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04353184 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0643     |\n",
      "|    n_updates            | 5770       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Ep done - 39240.\n",
      "Ep done - 39250.\n",
      "Ep done - 39260.\n",
      "Ep done - 39270.\n",
      "Ep done - 39280.\n",
      "Ep done - 39290.\n",
      "Ep done - 39300.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 579        |\n",
      "|    time_elapsed         | 6951       |\n",
      "|    total_timesteps      | 1185792    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03711496 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.176     |\n",
      "|    explained_variance   | 0.318      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "Ep done - 39310.\n",
      "Ep done - 39320.\n",
      "Ep done - 39330.\n",
      "Ep done - 39340.\n",
      "Ep done - 39350.\n",
      "Ep done - 39360.\n",
      "Ep done - 39370.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 580         |\n",
      "|    time_elapsed         | 6960        |\n",
      "|    total_timesteps      | 1187840     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041211233 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00545     |\n",
      "|    n_updates            | 5790        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.128       |\n",
      "-----------------------------------------\n",
      "Ep done - 39380.\n",
      "Ep done - 39390.\n",
      "Ep done - 39400.\n",
      "Ep done - 39410.\n",
      "Ep done - 39420.\n",
      "Ep done - 39430.\n",
      "Ep done - 39440.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 581         |\n",
      "|    time_elapsed         | 6969        |\n",
      "|    total_timesteps      | 1189888     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034201536 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.0928      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 5800        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Ep done - 11810.\n",
      "Ep done - 11820.\n",
      "Ep done - 11830.\n",
      "Ep done - 11840.\n",
      "Ep done - 11850.\n",
      "Ep done - 11860.\n",
      "Ep done - 11870.\n",
      "Ep done - 11880.\n",
      "Ep done - 11890.\n",
      "Ep done - 11900.\n",
      "Eval num_timesteps=1190000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 30.25 +/- 0.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.6        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1190000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03688402 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | 0.17       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 5810       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.6\n",
      "SELFPLAY: new best model, bumping up generation to 108\n",
      "Ep done - 39450.\n",
      "Ep done - 39460.\n",
      "Ep done - 39470.\n",
      "Ep done - 39480.\n",
      "Ep done - 39490.\n",
      "Ep done - 39500.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.71     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 582      |\n",
      "|    time_elapsed    | 6988     |\n",
      "|    total_timesteps | 1191936  |\n",
      "---------------------------------\n",
      "Ep done - 39510.\n",
      "Ep done - 39520.\n",
      "Ep done - 39530.\n",
      "Ep done - 39540.\n",
      "Ep done - 39550.\n",
      "Ep done - 39560.\n",
      "Ep done - 39570.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.64        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 583         |\n",
      "|    time_elapsed         | 6998        |\n",
      "|    total_timesteps      | 1193984     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029212948 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00189    |\n",
      "|    n_updates            | 5820        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n",
      "Ep done - 39580.\n",
      "Ep done - 39590.\n",
      "Ep done - 39600.\n",
      "Ep done - 39610.\n",
      "Ep done - 39620.\n",
      "Ep done - 39630.\n",
      "Ep done - 39640.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 584         |\n",
      "|    time_elapsed         | 7007        |\n",
      "|    total_timesteps      | 1196032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043815713 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0209      |\n",
      "|    n_updates            | 5830        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Ep done - 39650.\n",
      "Ep done - 39660.\n",
      "Ep done - 39670.\n",
      "Ep done - 39680.\n",
      "Ep done - 39690.\n",
      "Ep done - 39700.\n",
      "Ep done - 39710.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 585        |\n",
      "|    time_elapsed         | 7016       |\n",
      "|    total_timesteps      | 1198080    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03365908 |\n",
      "|    clip_fraction        | 0.0928     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.252      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00878    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 0.0737     |\n",
      "----------------------------------------\n",
      "Ep done - 39720.\n",
      "Ep done - 39730.\n",
      "Ep done - 39740.\n",
      "Ep done - 39750.\n",
      "Ep done - 39760.\n",
      "Ep done - 39770.\n",
      "Ep done - 11910.\n",
      "Ep done - 11920.\n",
      "Ep done - 11930.\n",
      "Ep done - 11940.\n",
      "Ep done - 11950.\n",
      "Ep done - 11960.\n",
      "Ep done - 11970.\n",
      "Ep done - 11980.\n",
      "Ep done - 11990.\n",
      "Ep done - 12000.\n",
      "Eval num_timesteps=1200000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.14 +/- 0.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030612636 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00335    |\n",
      "|    n_updates            | 5850        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 0.0743      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 109\n",
      "Ep done - 39780.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.78     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 7035     |\n",
      "|    total_timesteps | 1200128  |\n",
      "---------------------------------\n",
      "Ep done - 39790.\n",
      "Ep done - 39800.\n",
      "Ep done - 39810.\n",
      "Ep done - 39820.\n",
      "Ep done - 39830.\n",
      "Ep done - 39840.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.87        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 587         |\n",
      "|    time_elapsed         | 7044        |\n",
      "|    total_timesteps      | 1202176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031899735 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.0195      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 5860        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "Ep done - 39850.\n",
      "Ep done - 39860.\n",
      "Ep done - 39870.\n",
      "Ep done - 39880.\n",
      "Ep done - 39890.\n",
      "Ep done - 39900.\n",
      "Ep done - 39910.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 588         |\n",
      "|    time_elapsed         | 7053        |\n",
      "|    total_timesteps      | 1204224     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038533993 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.173      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 5870        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.0527      |\n",
      "-----------------------------------------\n",
      "Ep done - 39920.\n",
      "Ep done - 39930.\n",
      "Ep done - 39940.\n",
      "Ep done - 39950.\n",
      "Ep done - 39960.\n",
      "Ep done - 39970.\n",
      "Ep done - 39980.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 589         |\n",
      "|    time_elapsed         | 7063        |\n",
      "|    total_timesteps      | 1206272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042959347 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0518      |\n",
      "|    n_updates            | 5880        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 0.122       |\n",
      "-----------------------------------------\n",
      "Ep done - 39990.\n",
      "Ep done - 40000.\n",
      "Ep done - 40010.\n",
      "Ep done - 40020.\n",
      "Ep done - 40030.\n",
      "Ep done - 40040.\n",
      "Ep done - 40050.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 590         |\n",
      "|    time_elapsed         | 7072        |\n",
      "|    total_timesteps      | 1208320     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048923016 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00525    |\n",
      "|    n_updates            | 5890        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 0.072       |\n",
      "-----------------------------------------\n",
      "Ep done - 40060.\n",
      "Ep done - 40070.\n",
      "Ep done - 40080.\n",
      "Ep done - 40090.\n",
      "Ep done - 40100.\n",
      "Ep done - 12010.\n",
      "Ep done - 12020.\n",
      "Ep done - 12030.\n",
      "Ep done - 12040.\n",
      "Ep done - 12050.\n",
      "Ep done - 12060.\n",
      "Ep done - 12070.\n",
      "Ep done - 12080.\n",
      "Ep done - 12090.\n",
      "Ep done - 12100.\n",
      "Eval num_timesteps=1210000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.12 +/- 0.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034235865 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 5900        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 0.0688      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 110\n",
      "Ep done - 40110.\n",
      "Ep done - 40120.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.8      |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 591      |\n",
      "|    time_elapsed    | 7090     |\n",
      "|    total_timesteps | 1210368  |\n",
      "---------------------------------\n",
      "Ep done - 40130.\n",
      "Ep done - 40140.\n",
      "Ep done - 40150.\n",
      "Ep done - 40160.\n",
      "Ep done - 40170.\n",
      "Ep done - 40180.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.64       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 592        |\n",
      "|    time_elapsed         | 7100       |\n",
      "|    total_timesteps      | 1212416    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03693305 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.295      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 5910       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    value_loss           | 0.0596     |\n",
      "----------------------------------------\n",
      "Ep done - 40190.\n",
      "Ep done - 40200.\n",
      "Ep done - 40210.\n",
      "Ep done - 40220.\n",
      "Ep done - 40230.\n",
      "Ep done - 40240.\n",
      "Ep done - 40250.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.1       |\n",
      "|    ep_rew_mean          | 0.51       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 593        |\n",
      "|    time_elapsed         | 7109       |\n",
      "|    total_timesteps      | 1214464    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05796275 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.187     |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0325     |\n",
      "|    n_updates            | 5920       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Ep done - 40260.\n",
      "Ep done - 40270.\n",
      "Ep done - 40280.\n",
      "Ep done - 40290.\n",
      "Ep done - 40300.\n",
      "Ep done - 40310.\n",
      "Ep done - 40320.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.41        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 594         |\n",
      "|    time_elapsed         | 7119        |\n",
      "|    total_timesteps      | 1216512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042035002 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0508      |\n",
      "|    n_updates            | 5930        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 40330.\n",
      "Ep done - 40340.\n",
      "Ep done - 40350.\n",
      "Ep done - 40360.\n",
      "Ep done - 40370.\n",
      "Ep done - 40380.\n",
      "Ep done - 40390.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.55        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 595         |\n",
      "|    time_elapsed         | 7128        |\n",
      "|    total_timesteps      | 1218560     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075213075 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.178      |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0644      |\n",
      "|    n_updates            | 5940        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "Ep done - 40400.\n",
      "Ep done - 40410.\n",
      "Ep done - 40420.\n",
      "Ep done - 40430.\n",
      "Ep done - 40440.\n",
      "Ep done - 12110.\n",
      "Ep done - 12120.\n",
      "Ep done - 12130.\n",
      "Ep done - 12140.\n",
      "Ep done - 12150.\n",
      "Ep done - 12160.\n",
      "Ep done - 12170.\n",
      "Ep done - 12180.\n",
      "Ep done - 12190.\n",
      "Ep done - 12200.\n",
      "Eval num_timesteps=1220000, episode_reward=0.61 +/- 0.76\n",
      "Episode length: 30.08 +/- 0.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041512527 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 5950        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.13        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.61\n",
      "SELFPLAY: new best model, bumping up generation to 111\n",
      "Ep done - 40450.\n",
      "Ep done - 40460.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.6      |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 7146     |\n",
      "|    total_timesteps | 1220608  |\n",
      "---------------------------------\n",
      "Ep done - 40470.\n",
      "Ep done - 40480.\n",
      "Ep done - 40490.\n",
      "Ep done - 40500.\n",
      "Ep done - 40510.\n",
      "Ep done - 40520.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 597         |\n",
      "|    time_elapsed         | 7156        |\n",
      "|    total_timesteps      | 1222656     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038927305 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 5960        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 0.132       |\n",
      "-----------------------------------------\n",
      "Ep done - 40530.\n",
      "Ep done - 40540.\n",
      "Ep done - 40550.\n",
      "Ep done - 40560.\n",
      "Ep done - 40570.\n",
      "Ep done - 40580.\n",
      "Ep done - 40590.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 598         |\n",
      "|    time_elapsed         | 7165        |\n",
      "|    total_timesteps      | 1224704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037362866 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.172      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0264      |\n",
      "|    n_updates            | 5970        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "Ep done - 40600.\n",
      "Ep done - 40610.\n",
      "Ep done - 40620.\n",
      "Ep done - 40630.\n",
      "Ep done - 40640.\n",
      "Ep done - 40650.\n",
      "Ep done - 40660.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.62       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 170        |\n",
      "|    iterations           | 599        |\n",
      "|    time_elapsed         | 7174       |\n",
      "|    total_timesteps      | 1226752    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04768973 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0339     |\n",
      "|    n_updates            | 5980       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Ep done - 40670.\n",
      "Ep done - 40680.\n",
      "Ep done - 40690.\n",
      "Ep done - 40700.\n",
      "Ep done - 40710.\n",
      "Ep done - 40720.\n",
      "Ep done - 40730.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.63        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 600         |\n",
      "|    time_elapsed         | 7184        |\n",
      "|    total_timesteps      | 1228800     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038282152 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00663     |\n",
      "|    n_updates            | 5990        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "Ep done - 40740.\n",
      "Ep done - 40750.\n",
      "Ep done - 40760.\n",
      "Ep done - 40770.\n",
      "Ep done - 12210.\n",
      "Ep done - 12220.\n",
      "Ep done - 12230.\n",
      "Ep done - 12240.\n",
      "Ep done - 12250.\n",
      "Ep done - 12260.\n",
      "Ep done - 12270.\n",
      "Ep done - 12280.\n",
      "Ep done - 12290.\n",
      "Ep done - 12300.\n",
      "Eval num_timesteps=1230000, episode_reward=0.76 +/- 0.65\n",
      "Episode length: 30.18 +/- 0.83\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1230000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03869886 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 6000       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.76\n",
      "SELFPLAY: new best model, bumping up generation to 112\n",
      "Ep done - 40780.\n",
      "Ep done - 40790.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 601      |\n",
      "|    time_elapsed    | 7202     |\n",
      "|    total_timesteps | 1230848  |\n",
      "---------------------------------\n",
      "Ep done - 40800.\n",
      "Ep done - 40810.\n",
      "Ep done - 40820.\n",
      "Ep done - 40830.\n",
      "Ep done - 40840.\n",
      "Ep done - 40850.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 40860.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 170         |\n",
      "|    iterations           | 602         |\n",
      "|    time_elapsed         | 7212        |\n",
      "|    total_timesteps      | 1232896     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041216135 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 6010        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Ep done - 40870.\n",
      "Ep done - 40880.\n",
      "Ep done - 40890.\n",
      "Ep done - 40900.\n",
      "Ep done - 40910.\n",
      "Ep done - 40920.\n",
      "Ep done - 40930.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 603        |\n",
      "|    time_elapsed         | 7221       |\n",
      "|    total_timesteps      | 1234944    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03418038 |\n",
      "|    clip_fraction        | 0.0946     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00591    |\n",
      "|    n_updates            | 6020       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Ep done - 40940.\n",
      "Ep done - 40950.\n",
      "Ep done - 40960.\n",
      "Ep done - 40970.\n",
      "Ep done - 40980.\n",
      "Ep done - 40990.\n",
      "Ep done - 41000.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 604        |\n",
      "|    time_elapsed         | 7230       |\n",
      "|    total_timesteps      | 1236992    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04248387 |\n",
      "|    clip_fraction        | 0.0978     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 6030       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 0.0947     |\n",
      "----------------------------------------\n",
      "Ep done - 41010.\n",
      "Ep done - 41020.\n",
      "Ep done - 41030.\n",
      "Ep done - 41040.\n",
      "Ep done - 41050.\n",
      "Ep done - 41060.\n",
      "Ep done - 41070.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.1        |\n",
      "|    ep_rew_mean          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 605         |\n",
      "|    time_elapsed         | 7240        |\n",
      "|    total_timesteps      | 1239040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050052077 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00289     |\n",
      "|    n_updates            | 6040        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Ep done - 41080.\n",
      "Ep done - 41090.\n",
      "Ep done - 41100.\n",
      "Ep done - 12310.\n",
      "Ep done - 12320.\n",
      "Ep done - 12330.\n",
      "Ep done - 12340.\n",
      "Ep done - 12350.\n",
      "Ep done - 12360.\n",
      "Ep done - 12370.\n",
      "Ep done - 12380.\n",
      "Ep done - 12390.\n",
      "Ep done - 12400.\n",
      "Eval num_timesteps=1240000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.15 +/- 0.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049197793 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.157      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 6050        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 0.0886      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.79\n",
      "SELFPLAY: new best model, bumping up generation to 113\n",
      "Ep done - 41110.\n",
      "Ep done - 41120.\n",
      "Ep done - 41130.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.66     |\n",
      "| time/              |          |\n",
      "|    fps             | 170      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 7258     |\n",
      "|    total_timesteps | 1241088  |\n",
      "---------------------------------\n",
      "Ep done - 41140.\n",
      "Ep done - 41150.\n",
      "Ep done - 41160.\n",
      "Ep done - 41170.\n",
      "Ep done - 41180.\n",
      "Ep done - 41190.\n",
      "Ep done - 41200.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 607         |\n",
      "|    time_elapsed         | 7268        |\n",
      "|    total_timesteps      | 1243136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056756575 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.177      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 6060        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.119       |\n",
      "-----------------------------------------\n",
      "Ep done - 41210.\n",
      "Ep done - 41220.\n",
      "Ep done - 41230.\n",
      "Ep done - 41240.\n",
      "Ep done - 41250.\n",
      "Ep done - 41260.\n",
      "Ep done - 41270.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.6         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 608         |\n",
      "|    time_elapsed         | 7277        |\n",
      "|    total_timesteps      | 1245184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047603093 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 6070        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Ep done - 41280.\n",
      "Ep done - 41290.\n",
      "Ep done - 41300.\n",
      "Ep done - 41310.\n",
      "Ep done - 41320.\n",
      "Ep done - 41330.\n",
      "Ep done - 41340.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 609         |\n",
      "|    time_elapsed         | 7286        |\n",
      "|    total_timesteps      | 1247232     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041817576 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.475       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 6080        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 41350.\n",
      "Ep done - 41360.\n",
      "Ep done - 41370.\n",
      "Ep done - 41380.\n",
      "Ep done - 41390.\n",
      "Ep done - 41400.\n",
      "Ep done - 41410.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 610         |\n",
      "|    time_elapsed         | 7296        |\n",
      "|    total_timesteps      | 1249280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040091306 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0225      |\n",
      "|    n_updates            | 6090        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Ep done - 41420.\n",
      "Ep done - 41430.\n",
      "Ep done - 12410.\n",
      "Ep done - 12420.\n",
      "Ep done - 12430.\n",
      "Ep done - 12440.\n",
      "Ep done - 12450.\n",
      "Ep done - 12460.\n",
      "Ep done - 12470.\n",
      "Ep done - 12480.\n",
      "Ep done - 12490.\n",
      "Ep done - 12500.\n",
      "Eval num_timesteps=1250000, episode_reward=0.77 +/- 0.63\n",
      "Episode length: 30.16 +/- 0.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30.2       |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1250000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05249697 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 6100       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 0.0869     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.77\n",
      "SELFPLAY: new best model, bumping up generation to 114\n",
      "Ep done - 41440.\n",
      "Ep done - 41450.\n",
      "Ep done - 41460.\n",
      "Ep done - 41470.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 0.61     |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 611      |\n",
      "|    time_elapsed    | 7314     |\n",
      "|    total_timesteps | 1251328  |\n",
      "---------------------------------\n",
      "Ep done - 41480.\n",
      "Ep done - 41490.\n",
      "Ep done - 41500.\n",
      "Ep done - 41510.\n",
      "Ep done - 41520.\n",
      "Ep done - 41530.\n",
      "Ep done - 41540.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.62        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 612         |\n",
      "|    time_elapsed         | 7324        |\n",
      "|    total_timesteps      | 1253376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052148137 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.025       |\n",
      "|    n_updates            | 6110        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Ep done - 41550.\n",
      "Ep done - 41560.\n",
      "Ep done - 41570.\n",
      "Ep done - 41580.\n",
      "Ep done - 41590.\n",
      "Ep done - 41600.\n",
      "Ep done - 41610.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 613         |\n",
      "|    time_elapsed         | 7333        |\n",
      "|    total_timesteps      | 1255424     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046819847 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0648      |\n",
      "|    n_updates            | 6120        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Ep done - 41620.\n",
      "Ep done - 41630.\n",
      "Ep done - 41640.\n",
      "Ep done - 41650.\n",
      "Ep done - 41660.\n",
      "Ep done - 41670.\n",
      "Ep done - 41680.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.55       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 614        |\n",
      "|    time_elapsed         | 7343       |\n",
      "|    total_timesteps      | 1257472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06483002 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.201     |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 6130       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Ep done - 41690.\n",
      "Ep done - 41700.\n",
      "Ep done - 41710.\n",
      "Ep done - 41720.\n",
      "Ep done - 41730.\n",
      "Ep done - 41740.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.67       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 615        |\n",
      "|    time_elapsed         | 7352       |\n",
      "|    total_timesteps      | 1259520    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05261697 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.201     |\n",
      "|    explained_variance   | 0.224      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0522     |\n",
      "|    n_updates            | 6140       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "Ep done - 41750.\n",
      "Ep done - 41760.\n",
      "Ep done - 12510.\n",
      "Ep done - 12520.\n",
      "Ep done - 12530.\n",
      "Ep done - 12540.\n",
      "Ep done - 12550.\n",
      "Ep done - 12560.\n",
      "Ep done - 12570.\n",
      "Ep done - 12580.\n",
      "Ep done - 12590.\n",
      "Ep done - 12600.\n",
      "Eval num_timesteps=1260000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.15 +/- 0.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.1        |\n",
      "|    mean_reward          | 0.66        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049838714 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.198      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 6150        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.0931      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.66\n",
      "SELFPLAY: new best model, bumping up generation to 115\n",
      "Ep done - 41770.\n",
      "Ep done - 41780.\n",
      "Ep done - 41790.\n",
      "Ep done - 41800.\n",
      "Ep done - 41810.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 0.76     |\n",
      "| time/              |          |\n",
      "|    fps             | 171      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 7371     |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "Ep done - 41820.\n",
      "Ep done - 41830.\n",
      "Ep done - 41840.\n",
      "Ep done - 41850.\n",
      "Ep done - 41860.\n",
      "Ep done - 41870.\n",
      "Ep done - 41880.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 617        |\n",
      "|    time_elapsed         | 7380       |\n",
      "|    total_timesteps      | 1263616    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04153005 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.225     |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0283     |\n",
      "|    n_updates            | 6160       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Ep done - 41890.\n",
      "Ep done - 41900.\n",
      "Ep done - 41910.\n",
      "Ep done - 41920.\n",
      "Ep done - 41930.\n",
      "Ep done - 41940.\n",
      "Ep done - 41950.\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.2       |\n",
      "|    ep_rew_mean          | 0.63       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 171        |\n",
      "|    iterations           | 618        |\n",
      "|    time_elapsed         | 7390       |\n",
      "|    total_timesteps      | 1265664    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04852904 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.211     |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 6170       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_callback = SelfPlayCallback(    \n",
    "    env_eval,\n",
    "    best_model_save_path=LOGDIR,\n",
    "    log_path=LOGDIR,\n",
    "    eval_freq=EVAL_FREQ,\n",
    "    n_eval_episodes=EVAL_EPISODES,\n",
    "    deterministic=False \n",
    "    )\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6f4d6-e4af-40f6-8dc5-bb4a93a76e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fb305-2d50-439b-8ea9-721ed92a9809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b754f-325b-4cd6-889a-684b154e3b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2cc1ebe0-55da-483a-98b8-480abe6723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_model_copy.predict(env.reset()[0], action_masks=env.action_masks())\n",
    "# model.predict(env.reset()[0], action_masks=env.action_masks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af50891d-79a1-4474-96de-fe47e069f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 1, 2, 0, 0, 0],\n",
       "                     [0, 0, 0, 2, 1, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [0, 0, 0, 0, 0, 0, 0, 0]])),\n",
       "             ('player', 1)])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48957583-55ae-4d37-a91a-4d3a0d09a152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076a2e9-f6ec-4ace-a5c7-6d6863935c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.unwrapped.get_obs()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19b6d5cf-09f8-463b-a95d-3a9add321149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs = spaces.flatten(env.unwrapped.observation_space, obs)\n",
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8ed10-d6d0-4363-9516-89dda14fd7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f7da2c24-2262-4c6c-8464-ae1cb2390730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(22), None)\n",
      "(array(29), None)\n",
      "(array(41), None)\n",
      "(array(28), None)\n",
      "(array(5), None)\n",
      "(array(46), None)\n",
      "(array(24), None)\n",
      "(array(13), None)\n",
      "(array(40), None)\n",
      "(array(59), None)\n",
      "(array(13), None)\n",
      "(array(45), None)\n",
      "(array(18), None)\n",
      "(array(62), None)\n",
      "(array(51), None)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(model.predict(new_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ede4294c-f6cf-4fff-b4ae-83f2a1c14ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2,\n",
       "       1, 2, 1, 0, 2, 2, 1, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 0, 0, 1, 0,\n",
       "       2, 2, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc4e49-8b25-4a20-9d99-6e9b04d61b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0969e-6711-4e9a-9ceb-3e6ea3b9d304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5ec1f886-e37f-496d-b6e5-dd33ccd191fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval = OthelloEnv()\n",
    "env_eval = Monitor(env=env_eval)\n",
    "env_eval = FlattenObservation(env_eval)\n",
    "\n",
    "env_eval = DummyVecEnv(env_fns=[lambda: env_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2b4b5155-d19f-4bc2-91da-bbc09b488fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MaskablePPO.load('ppo_masked_selfplay/history_00000385.zip')\n",
    "model_random = MaskablePPO.load('ppo_masked_selfplay/history_00000170.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9b6d8668-3386-434d-97b8-a8ed3ba6ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_eval.envs[0].unwrapped.change_to_latest_agent(model_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d252cb9d-8f37-4209-9da1-80252d48d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep done - 310.\n",
      "Ep done - 320.\n",
      "Ep done - 330.\n",
      "Ep done - 340.\n",
      "Ep done - 350.\n",
      "Ep done - 360.\n",
      "Ep done - 370.\n",
      "Ep done - 380.\n",
      "Ep done - 390.\n",
      "Ep done - 400.\n"
     ]
    }
   ],
   "source": [
    "episode_rewards, episode_lengths = evaluate_policy(\n",
    "                model1,\n",
    "                env_eval,\n",
    "                n_eval_episodes=100,                \n",
    "                deterministic=True,\n",
    "                return_episode_rewards=True,\n",
    "                warn=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3527ceef-0eb6-4e5c-bf97-1e83b354a502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed67b8-6289-4c32-970e-362e1f44165d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7ebfd-6326-45c0-be8e-0bc89976894e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
